{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # To simplify implementation, please initialize both self.lin_dst\n",
    "        # and self.lin_src out_features to out_channels\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "\n",
    "        self.lin_dst = nn.Linear(self.in_channels_src, self.out_channels)\n",
    "        self.lin_src = nn.Linear(self.in_channels_dst, self.out_channels)\n",
    "        self.lin_update = nn.Linear(2*self.out_channels, self.out_channels)\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "    ):\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "        ##########################################\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "        ## Note:\n",
    "        ## 1. Different from what we implemented in Colab 3, we use message_and_aggregate\n",
    "        ## to replace the message and aggregate. The benefit is that we can avoid\n",
    "        ## materializing x_i and x_j, and make the implementation more efficient.\n",
    "        ## 2. To implement efficiently, following PyG documentation is helpful:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html\n",
    "        ## 3. Here edge_index is torch_sparse SparseTensor.\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "        ##########################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~4 lines of code)\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        # print(aggr_out.shape, dst_out.shape)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        # print(aggr_out.shape, )\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "        ##########################################\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "            ############# Your code here #############\n",
    "            ## (~1 line of code)\n",
    "            ## Note:\n",
    "            ## 1. Initialize self.attn_proj here.\n",
    "            ## 2. You should use nn.Sequential for self.attn_proj\n",
    "            ## 3. nn.Linear and nn.Tanh are useful.\n",
    "            ## 4. You can create a vector parameter by using:\n",
    "            ## nn.Linear(some_size, 1, bias=False)\n",
    "            ## 5. The first linear layer should have out_features as args['attn_size']\n",
    "            ## 6. You can assume we only have one \"head\" for the attention.\n",
    "            ## 7. We recommend you to implement the mean aggregation first. After \n",
    "            ## the mean aggregation works well in the training, then you can \n",
    "            ## implement this part.\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "            #########################################\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        # TODO: Implement this function that aggregates all message type results.\n",
    "        # Here, xs is a list of tensors (embeddings) with respect to message \n",
    "        # type aggregation results.\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~2 lines of code)\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "            ##########################################\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~10 lines of code)\n",
    "            ## Note:\n",
    "            ## 1. Store the value of attention alpha (as a numpy array) to self.alpha,\n",
    "            ## which has the shape (len(xs), ) self.alpha will be not be used \n",
    "            ## to backpropagate etc. in the model. We will use it to see how much \n",
    "            ## attention the layer pays on different message types.\n",
    "            ## 2. torch.softmax and torch.cat are useful.\n",
    "            ## 3. You might need to reshape the tensors by using the \n",
    "            ## `view()` function https://pytorch.org/docs/stable/tensor_view.html\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out\n",
    "            ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    # TODO: Implement this function that returns a dictionary of `HeteroGNNConv` \n",
    "    # layers where the keys are message types. `hetero_graph` is deepsnap `HeteroGraph`\n",
    "    # object and the `conv` is the `HeteroGNNConv`.\n",
    "\n",
    "    convs = {}\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## (~9 lines of code)\n",
    "\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    ##########################################\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        self.convs1 = None\n",
    "        self.convs2 = None\n",
    "\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~10 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For self.convs1 and self.convs2, call generate_convs at first and then\n",
    "        ## pass the returned dictionary of `HeteroGNNConv` to `HeteroGNNWrapperConv`.\n",
    "        ## 2. For self.bns, self.relus and self.post_mps, the keys are node_types.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.node_types` will be helpful.\n",
    "        ## 3. Initialize all batchnorms to torch.nn.BatchNorm1d(hidden_size, eps=1.0).\n",
    "        ## 4. Initialize all relus to nn.LeakyReLU().\n",
    "        ## 5. For self.post_mps, each value in the ModuleDict is a linear layer \n",
    "        ## where the `out_features` is the number of classes for that node type.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.num_node_labels(node_type)` will be\n",
    "        ## useful.\n",
    "\n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            self.post_mps[node_type] = nn.Linear(self.hidden_size, hetero_graph.num_node_labels(node_type))\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(self, node_feature, edge_index):\n",
    "        # TODO: Implement the forward function. Notice that `node_feature` is \n",
    "        # a dictionary of tensors where keys are node types and values are \n",
    "        # corresponding feature tensors. The `edge_index` is a dictionary of \n",
    "        # tensors where keys are message types and values are corresponding\n",
    "        # edge index tensors (with respect to each message type).\n",
    "\n",
    "        x = node_feature\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~7 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. `deepsnap.hetero_gnn.forward_op` can be helpful.\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        # TODO: remove this for regression tasks\n",
    "        x = forward_op(x, self.post_mps)\n",
    "\n",
    "        ##########################################\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        \n",
    "        loss = 0\n",
    "        loss_func = F.cross_entropy\n",
    "\n",
    "        print(\"Preds\", preds)\n",
    "        print(\"Y\", y)\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For each node type in preds, accumulate computed loss to `loss`\n",
    "        ## 2. Loss need to be computed with respect to the given index\n",
    "\n",
    "        for node_type in preds:\n",
    "            idx = indices[node_type]\n",
    "            loss += loss_func(preds[node_type][idx], y[node_type][idx])\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    loss = None\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## Note:\n",
    "    ## 1. `deepsnap.hetero_graph.HeteroGraph.node_label` is useful\n",
    "    ## 2. Compute the loss here\n",
    "    \n",
    "    loss = model.loss(preds, hetero_graph.node_label, train_idx)\n",
    "    ##########################################\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(model, graph, indices, best_model=None, best_val=0):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        num_node_types = 0\n",
    "        micro = 0\n",
    "        macro = 0\n",
    "        for node_type in preds:\n",
    "            idx = index[node_type]\n",
    "            pred = preds[node_type][idx]\n",
    "            pred = pred.max(1)[1]\n",
    "            label_np = graph.node_label[node_type][idx].cpu().numpy()\n",
    "            pred_np = pred.cpu().numpy()\n",
    "            micro = f1_score(label_np, pred_np, average='micro')\n",
    "            macro = f1_score(label_np, pred_np, average='macro')\n",
    "            num_node_types += 1\n",
    "        # Averaging f1 score might not make sense, but in our example we only\n",
    "        # have one node type\n",
    "        micro /= num_node_types\n",
    "        macro /= num_node_types\n",
    "        accs.append((micro, macro))\n",
    "    if accs[1][0] > best_val:\n",
    "        best_val = accs[1][0]\n",
    "        best_model = copy.deepcopy(model)\n",
    "    return accs, best_model, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change the following parameters\n",
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 64,\n",
    "    'epochs': 100,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.003,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [0, 0, 0],   # event 0\n",
    "                [1, 1, 1]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_node_label = {\n",
    "    \"event\": torch.tensor([0, 1]), # Class 0, Class 1\n",
    "    \"concept\": torch.tensor([0, 1])  # Class 0, Class 1\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 3\n",
      "\t Feature 3\n",
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 3\n",
      "\t Feature 3\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 3\n",
      "\t Feature 3\n",
      "KEY ('event', 'similar', 'event')\n",
      "KEY NUMS ('event', 'similar', 'event') 2 2\n",
      "MAX EDGES tensor(1) tensor(1)\n",
      "KEY ('event', 'related', 'concept')\n",
      "KEY NUMS ('event', 'related', 'concept') 2 2\n",
      "MAX EDGES tensor(1) tensor(1)\n",
      "KEY ('concept', 'related', 'event')\n",
      "KEY NUMS ('concept', 'related', 'event') 2 2\n",
      "MAX EDGES tensor(1) tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# with open(\"./1_concepts_similar.pkl\", \"rb\") as f:\n",
    "#     G = pickle.load(f)\n",
    "#hetero_graph = HeteroGraph(G, netlib=nx, directed=False)\n",
    "\n",
    "hetero_graph = HeteroGraph(\n",
    "    node_feature=S_node_feature,\n",
    "    node_label=S_node_label,\n",
    "    edge_index=S_edge_index\n",
    ")\n",
    "\n",
    "# # Construct a deepsnap tensor backend HeteroGraph\n",
    "# # hetero_graph = HeteroGraph(\n",
    "# #     node_feature=node_feature,\n",
    "# #     node_label=node_label,\n",
    "# #     edge_index=edge_index,\n",
    "# #     directed=True\n",
    "# # )\n",
    "\n",
    "# hetero_graph = HeteroGraph(G)\n",
    "\n",
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "\n",
    "# Node feature and node label to device\n",
    "\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "\n",
    "# Edge_index to sparse tensor and to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key)\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max())\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds {'event': tensor([[0.0507, 0.0147],\n",
      "        [0.0484, 0.0041]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 0.1034, -0.0853],\n",
      "        [ 0.0825, -0.0624]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 1.38116, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 0.0891, -0.0391],\n",
      "        [ 0.0197,  0.0518]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 0.1669, -0.1208],\n",
      "        [ 0.0246, -0.0098]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 2: loss 1.28922, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 0.1490, -0.1130],\n",
      "        [-0.0286,  0.1223]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 0.2578, -0.1902],\n",
      "        [-0.0679,  0.0739]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 3: loss 1.15504, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 0.2545, -0.2320],\n",
      "        [-0.1142,  0.2399]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 0.4031, -0.3162],\n",
      "        [-0.2142,  0.2087]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 4: loss 0.9558, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 0.4195, -0.4199],\n",
      "        [-0.2500,  0.4213]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 0.6248, -0.5063],\n",
      "        [-0.4263,  0.4027]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 5: loss 0.70686, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 0.6494, -0.6846],\n",
      "        [-0.4435,  0.6690]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 0.9152, -0.7570],\n",
      "        [-0.7027,  0.6515]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 6: loss 0.45992, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 0.9258, -1.0045],\n",
      "        [-0.6838,  0.9635]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 1.2438, -1.0423],\n",
      "        [-1.0123,  0.9252]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 7: loss 0.2715, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 1.2055, -1.3289],\n",
      "        [-0.9408,  1.2573]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 1.5627, -1.3148],\n",
      "        [-1.3093,  1.1828]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 8: loss 0.15792, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 1.4508, -1.6131],\n",
      "        [-1.1855,  1.5070]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 1.8393, -1.5440],\n",
      "        [-1.5651,  1.4007]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 9: loss 0.09739, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 1.6524, -1.8440],\n",
      "        [-1.4035,  1.7040]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 2.0659, -1.7298],\n",
      "        [-1.7746,  1.5787]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 10: loss 0.06509, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 1.8179, -2.0293],\n",
      "        [-1.5918,  1.8621]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 2.2498, -1.8837],\n",
      "        [-1.9457,  1.7265]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 11: loss 0.04662, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 1.9572, -2.1815],\n",
      "        [-1.7531,  1.9966]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 2.4022, -2.0156],\n",
      "        [-2.0885,  1.8533]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 12: loss 0.03514, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 2.0778, -2.3106],\n",
      "        [-1.8924,  2.1166]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 2.5321, -2.1318],\n",
      "        [-2.2114,  1.9653]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 13: loss 0.02748, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 2.1848, -2.4235],\n",
      "        [-2.0149,  2.2264]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 2.6459, -2.2362],\n",
      "        [-2.3198,  2.0662]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 14: loss 0.02207, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 2.2812, -2.5242],\n",
      "        [-2.1245,  2.3281]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 2.7477, -2.3312],\n",
      "        [-2.4174,  2.1586]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 15: loss 0.01809, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 2.3692, -2.6154],\n",
      "        [-2.2242,  2.4226]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 2.8400, -2.4184],\n",
      "        [-2.5065,  2.2438]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 16: loss 0.01508, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 2.4501, -2.6989],\n",
      "        [-2.3158,  2.5108]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 2.9245, -2.4988],\n",
      "        [-2.5885,  2.3229]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 17: loss 0.01275, train micro 25.0%, train macro 16.67%, valid micro 25.0%, valid macro 16.67%, test micro 25.0%, test macro 16.67%\n",
      "Preds {'event': tensor([[ 2.5249, -2.7757],\n",
      "        [-2.4005,  2.5930]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.0023, -2.5734],\n",
      "        [-2.6645,  2.3966]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 18: loss 0.01092, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 2.5943, -2.8468],\n",
      "        [-2.4791,  2.6699]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.0743, -2.6426],\n",
      "        [-2.7352,  2.4654]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 19: loss 0.00945, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 2.6588, -2.9128],\n",
      "        [-2.5524,  2.7417]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.1411, -2.7070],\n",
      "        [-2.8010,  2.5298]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 20: loss 0.00826, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 2.7189, -2.9741],\n",
      "        [-2.6207,  2.8089]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.2032, -2.7671],\n",
      "        [-2.8625,  2.5900]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 21: loss 0.00728, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 2.7749, -3.0312],\n",
      "        [-2.6845,  2.8716]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.2610, -2.8230],\n",
      "        [-2.9200,  2.6465]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 22: loss 0.00648, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 2.8271, -3.0845],\n",
      "        [-2.7442,  2.9303]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.3149, -2.8753],\n",
      "        [-2.9738,  2.6994]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 23: loss 0.0058, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 2.8759, -3.1342],\n",
      "        [-2.8000,  2.9852]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.3652, -2.9240],\n",
      "        [-3.0242,  2.7489]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 24: loss 0.00524, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 2.9215, -3.1806],\n",
      "        [-2.8521,  3.0364]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.4121, -2.9695],\n",
      "        [-3.0714,  2.7954]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 25: loss 0.00476, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 2.9641, -3.2239],\n",
      "        [-2.9009,  3.0844]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.4559, -3.0120],\n",
      "        [-3.1156,  2.8389]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 26: loss 0.00435, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.0039, -3.2644],\n",
      "        [-2.9466,  3.1292]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.4969, -3.0518],\n",
      "        [-3.1570,  2.8798]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 27: loss 0.004, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.0411, -3.3023],\n",
      "        [-2.9893,  3.1711]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.5351, -3.0889],\n",
      "        [-3.1958,  2.9181]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 28: loss 0.0037, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.0760, -3.3378],\n",
      "        [-3.0293,  3.2102]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.5709, -3.1237],\n",
      "        [-3.2323,  2.9541]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 29: loss 0.00344, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.1086, -3.3710],\n",
      "        [-3.0668,  3.2469]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.6044, -3.1562],\n",
      "        [-3.2664,  2.9878]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 30: loss 0.00321, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.1392, -3.4021],\n",
      "        [-3.1018,  3.2811]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.6358, -3.1866],\n",
      "        [-3.2985,  3.0195]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 31: loss 0.00301, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.1678, -3.4312],\n",
      "        [-3.1347,  3.3132]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.6652, -3.2151],\n",
      "        [-3.3287,  3.0493]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 32: loss 0.00283, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.1947, -3.4585],\n",
      "        [-3.1655,  3.3432]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.6927, -3.2418],\n",
      "        [-3.3570,  3.0773]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 33: loss 0.00268, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.2199, -3.4841],\n",
      "        [-3.1944,  3.3713]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.7185, -3.2668],\n",
      "        [-3.3836,  3.1036]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 34: loss 0.00254, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.2435, -3.5082],\n",
      "        [-3.2215,  3.3976]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.7428, -3.2903],\n",
      "        [-3.4086,  3.1284]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 35: loss 0.00242, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.2657, -3.5308],\n",
      "        [-3.2470,  3.4223]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.7655, -3.3123],\n",
      "        [-3.4322,  3.1517]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 36: loss 0.0023, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.2866, -3.5521],\n",
      "        [-3.2709,  3.4455]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.7869, -3.3330],\n",
      "        [-3.4544,  3.1737]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 37: loss 0.00221, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.3063, -3.5721],\n",
      "        [-3.2934,  3.4672]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.8071, -3.3525],\n",
      "        [-3.4754,  3.1944]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 38: loss 0.00212, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.3249, -3.5910],\n",
      "        [-3.3146,  3.4876]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.8261, -3.3709],\n",
      "        [-3.4952,  3.2140]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 39: loss 0.00203, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.3423, -3.6088],\n",
      "        [-3.3346,  3.5068]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.8440, -3.3882],\n",
      "        [-3.5139,  3.2325]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 40: loss 0.00196, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.3589, -3.6256],\n",
      "        [-3.3534,  3.5249]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.8608, -3.4045],\n",
      "        [-3.5316,  3.2500]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 41: loss 0.00189, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.3745, -3.6415],\n",
      "        [-3.3712,  3.5419]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.8768, -3.4199],\n",
      "        [-3.5483,  3.2666]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 42: loss 0.00183, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.3892, -3.6566],\n",
      "        [-3.3879,  3.5580]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.8919, -3.4344],\n",
      "        [-3.5642,  3.2823]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 43: loss 0.00178, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.4032, -3.6708],\n",
      "        [-3.4038,  3.5732]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.9062, -3.4482],\n",
      "        [-3.5792,  3.2972]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 44: loss 0.00172, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.4165, -3.6844],\n",
      "        [-3.4189,  3.5875]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.9198, -3.4613],\n",
      "        [-3.5936,  3.3114]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 45: loss 0.00168, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.4292, -3.6972],\n",
      "        [-3.4331,  3.6011]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.9327, -3.4737],\n",
      "        [-3.6072,  3.3248]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 46: loss 0.00163, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.4412, -3.7095],\n",
      "        [-3.4467,  3.6140]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.9450, -3.4855],\n",
      "        [-3.6202,  3.3377]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 47: loss 0.00159, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.4526, -3.7212],\n",
      "        [-3.4596,  3.6262]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.9567, -3.4968],\n",
      "        [-3.6326,  3.3500]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 48: loss 0.00155, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.4636, -3.7323],\n",
      "        [-3.4719,  3.6378]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.9678, -3.5075],\n",
      "        [-3.6444,  3.3617]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 49: loss 0.00152, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.4740, -3.7430],\n",
      "        [-3.4836,  3.6489]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.9785, -3.5178],\n",
      "        [-3.6557,  3.3729]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 50: loss 0.00149, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.4840, -3.7532],\n",
      "        [-3.4948,  3.6594]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.9887, -3.5276],\n",
      "        [-3.6666,  3.3837]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 51: loss 0.00146, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.4936, -3.7630],\n",
      "        [-3.5056,  3.6695]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 3.9985, -3.5370],\n",
      "        [-3.6770,  3.3940]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 52: loss 0.00143, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.5028, -3.7724],\n",
      "        [-3.5158,  3.6791]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.0079, -3.5460],\n",
      "        [-3.6871,  3.4039]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 53: loss 0.0014, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.5117, -3.7815],\n",
      "        [-3.5257,  3.6884]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.0170, -3.5546],\n",
      "        [-3.6967,  3.4135]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 54: loss 0.00137, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.5202, -3.7902],\n",
      "        [-3.5352,  3.6972]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.0257, -3.5630],\n",
      "        [-3.7060,  3.4227]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 55: loss 0.00135, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.5285, -3.7986],\n",
      "        [-3.5444,  3.7058]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.0341, -3.5710],\n",
      "        [-3.7150,  3.4316]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 56: loss 0.00133, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.5365, -3.8067],\n",
      "        [-3.5532,  3.7140]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.0423, -3.5788],\n",
      "        [-3.7237,  3.4402]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 57: loss 0.0013, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.5442, -3.8146],\n",
      "        [-3.5617,  3.7219]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.0501, -3.5863],\n",
      "        [-3.7321,  3.4486]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 58: loss 0.00128, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.5517, -3.8223],\n",
      "        [-3.5700,  3.7295]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.0578, -3.5936],\n",
      "        [-3.7403,  3.4566]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 59: loss 0.00126, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.5589, -3.8297],\n",
      "        [-3.5780,  3.7369]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.0652, -3.6007],\n",
      "        [-3.7482,  3.4645]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 60: loss 0.00124, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.5660, -3.8369],\n",
      "        [-3.5857,  3.7441]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.0724, -3.6076],\n",
      "        [-3.7560,  3.4721]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 61: loss 0.00123, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.5729, -3.8440],\n",
      "        [-3.5933,  3.7510]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.0794, -3.6143],\n",
      "        [-3.7635,  3.4796]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 62: loss 0.00121, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.5796, -3.8508],\n",
      "        [-3.6006,  3.7578]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.0863, -3.6208],\n",
      "        [-3.7708,  3.4869]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 63: loss 0.00119, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.5861, -3.8575],\n",
      "        [-3.6078,  3.7644]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.0929, -3.6272],\n",
      "        [-3.7780,  3.4939]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 64: loss 0.00118, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.5925, -3.8641],\n",
      "        [-3.6148,  3.7708]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.0995, -3.6334],\n",
      "        [-3.7850,  3.5009]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 65: loss 0.00116, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.5988, -3.8705],\n",
      "        [-3.6216,  3.7771]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1059, -3.6395],\n",
      "        [-3.7918,  3.5077]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 66: loss 0.00115, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6050, -3.8768],\n",
      "        [-3.6283,  3.7832]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1122, -3.6454],\n",
      "        [-3.7985,  3.5143]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 67: loss 0.00113, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6110, -3.8830],\n",
      "        [-3.6349,  3.7892]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1183, -3.6513],\n",
      "        [-3.8051,  3.5208]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 68: loss 0.00112, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6169, -3.8890],\n",
      "        [-3.6413,  3.7951]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1243, -3.6570],\n",
      "        [-3.8116,  3.5272]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 69: loss 0.0011, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6227, -3.8950],\n",
      "        [-3.6476,  3.8008]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1303, -3.6627],\n",
      "        [-3.8179,  3.5335]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 70: loss 0.00109, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6284, -3.9008],\n",
      "        [-3.6538,  3.8065]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1361, -3.6682],\n",
      "        [-3.8242,  3.5397]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 71: loss 0.00108, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6341, -3.9066],\n",
      "        [-3.6599,  3.8120]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1419, -3.6737],\n",
      "        [-3.8304,  3.5458]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 72: loss 0.00106, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6396, -3.9123],\n",
      "        [-3.6659,  3.8175]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1476, -3.6791],\n",
      "        [-3.8364,  3.5518]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 73: loss 0.00105, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6451, -3.9179],\n",
      "        [-3.6719,  3.8229]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1532, -3.6844],\n",
      "        [-3.8424,  3.5577]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 74: loss 0.00104, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6505, -3.9235],\n",
      "        [-3.6777,  3.8282]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1587, -3.6896],\n",
      "        [-3.8483,  3.5636]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 75: loss 0.00103, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6559, -3.9289],\n",
      "        [-3.6835,  3.8334]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1642, -3.6948],\n",
      "        [-3.8541,  3.5694]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 76: loss 0.00102, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6612, -3.9343],\n",
      "        [-3.6892,  3.8386]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1696, -3.6999],\n",
      "        [-3.8599,  3.5751]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 77: loss 0.00101, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6664, -3.9397],\n",
      "        [-3.6949,  3.8437]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1749, -3.7050],\n",
      "        [-3.8656,  3.5807]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 78: loss 0.001, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6716, -3.9450],\n",
      "        [-3.7004,  3.8487]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1802, -3.7100],\n",
      "        [-3.8712,  3.5863]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 79: loss 0.00098, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6767, -3.9503],\n",
      "        [-3.7060,  3.8537]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1854, -3.7150],\n",
      "        [-3.8768,  3.5918]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 80: loss 0.00097, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6818, -3.9555],\n",
      "        [-3.7114,  3.8587]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1906, -3.7199],\n",
      "        [-3.8823,  3.5973]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 81: loss 0.00096, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6868, -3.9606],\n",
      "        [-3.7169,  3.8636]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.1958, -3.7248],\n",
      "        [-3.8878,  3.6027]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 82: loss 0.00095, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6918, -3.9657],\n",
      "        [-3.7222,  3.8684]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2009, -3.7296],\n",
      "        [-3.8932,  3.6080]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 83: loss 0.00094, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.6967, -3.9708],\n",
      "        [-3.7276,  3.8732]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2059, -3.7344],\n",
      "        [-3.8986,  3.6134]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 84: loss 0.00093, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7016, -3.9758],\n",
      "        [-3.7328,  3.8780]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2109, -3.7391],\n",
      "        [-3.9039,  3.6186]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 85: loss 0.00093, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7065, -3.9808],\n",
      "        [-3.7381,  3.8827]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2159, -3.7439],\n",
      "        [-3.9092,  3.6239]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 86: loss 0.00092, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7113, -3.9858],\n",
      "        [-3.7433,  3.8874]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2209, -3.7485],\n",
      "        [-3.9145,  3.6291]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 87: loss 0.00091, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7162, -3.9907],\n",
      "        [-3.7485,  3.8921]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2258, -3.7532],\n",
      "        [-3.9197,  3.6342]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 88: loss 0.0009, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7209, -3.9956],\n",
      "        [-3.7536,  3.8967]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2307, -3.7578],\n",
      "        [-3.9248,  3.6394]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 89: loss 0.00089, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7257, -4.0004],\n",
      "        [-3.7587,  3.9013]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2355, -3.7624],\n",
      "        [-3.9300,  3.6444]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 90: loss 0.00088, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7304, -4.0053],\n",
      "        [-3.7638,  3.9058]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2403, -3.7670],\n",
      "        [-3.9351,  3.6495]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 91: loss 0.00087, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7351, -4.0101],\n",
      "        [-3.7688,  3.9104]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2451, -3.7715],\n",
      "        [-3.9402,  3.6545]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 92: loss 0.00086, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7397, -4.0148],\n",
      "        [-3.7738,  3.9149]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2499, -3.7760],\n",
      "        [-3.9452,  3.6595]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 93: loss 0.00086, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7443, -4.0196],\n",
      "        [-3.7788,  3.9194]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2546, -3.7805],\n",
      "        [-3.9502,  3.6644]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 94: loss 0.00085, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7490, -4.0243],\n",
      "        [-3.7838,  3.9238]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2594, -3.7850],\n",
      "        [-3.9552,  3.6694]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 95: loss 0.00084, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7535, -4.0290],\n",
      "        [-3.7887,  3.9283]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2640, -3.7894],\n",
      "        [-3.9601,  3.6743]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 96: loss 0.00083, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7581, -4.0337],\n",
      "        [-3.7936,  3.9327]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2687, -3.7938],\n",
      "        [-3.9651,  3.6791]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 97: loss 0.00082, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7626, -4.0383],\n",
      "        [-3.7985,  3.9371]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2734, -3.7982],\n",
      "        [-3.9700,  3.6840]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 98: loss 0.00082, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7671, -4.0429],\n",
      "        [-3.8034,  3.9414]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2780, -3.8026],\n",
      "        [-3.9748,  3.6888]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 99: loss 0.00081, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Preds {'event': tensor([[ 3.7716, -4.0475],\n",
      "        [-3.8082,  3.9458]], grad_fn=<AddmmBackward0>), 'concept': tensor([[ 4.2826, -3.8069],\n",
      "        [-3.9797,  3.6936]], grad_fn=<AddmmBackward0>)}\n",
      "Y {'event': tensor([0, 1]), 'concept': tensor([0, 1])}\n",
      "Epoch 100: loss 0.0008, train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n",
      "Best model: train micro 50.0%, train macro 50.0%, valid micro 50.0%, valid macro 50.0%, test micro 50.0%, test macro 50.0%\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_val = 0\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}: loss {round(loss, 5)}, \"\n",
    "        f\"train micro {round(accs[0][0] * 100, 2)}%, train macro {round(accs[0][1] * 100, 2)}%, \"\n",
    "        f\"valid micro {round(accs[1][0] * 100, 2)}%, valid macro {round(accs[1][1] * 100, 2)}%, \"\n",
    "        f\"test micro {round(accs[2][0] * 100, 2)}%, test macro {round(accs[2][1] * 100, 2)}%\"\n",
    "    )\n",
    "best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx])\n",
    "print(\n",
    "    f\"Best model: \"\n",
    "    f\"train micro {round(best_accs[0][0] * 100, 2)}%, train macro {round(best_accs[0][1] * 100, 2)}%, \"\n",
    "    f\"valid micro {round(best_accs[1][0] * 100, 2)}%, valid macro {round(best_accs[1][1] * 100, 2)}%, \"\n",
    "    f\"test micro {round(best_accs[2][0] * 100, 2)}%, test macro {round(best_accs[2][1] * 100, 2)}%\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
