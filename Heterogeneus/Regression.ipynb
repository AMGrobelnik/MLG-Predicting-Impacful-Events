{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # To simplify implementation, please initialize both self.lin_dst\n",
    "        # and self.lin_src out_features to out_channels\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "        \n",
    "        # Old code from colab, didn't work:\n",
    "        # self.lin_dst = nn.Linear(self.in_channels_src, self.out_channels)\n",
    "        # self.lin_src = nn.Linear(self.in_channels_dst, self.out_channels)\n",
    "        # self.lin_update = nn.Linear(2*self.out_channels, self.out_channels)\n",
    "        \n",
    "        self.lin_dst = nn.Linear(in_channels_dst, out_channels)\n",
    "        self.lin_src = nn.Linear(in_channels_src, out_channels)\n",
    "        self.lin_update = nn.Linear(2 * out_channels, out_channels)\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "    ):\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "        ##########################################\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "        ## Note:\n",
    "        ## 1. Different from what we implemented in Colab 3, we use message_and_aggregate\n",
    "        ## to replace the message and aggregate. The benefit is that we can avoid\n",
    "        ## materializing x_i and x_j, and make the implementation more efficient.\n",
    "        ## 2. To implement efficiently, following PyG documentation is helpful:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html\n",
    "        ## 3. Here edge_index is torch_sparse SparseTensor.\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "        ##########################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~4 lines of code)\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        # print(aggr_out.shape, dst_out.shape)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        # print(aggr_out.shape, )\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "        ##########################################\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "            ############# Your code here #############\n",
    "            ## (~1 line of code)\n",
    "            ## Note:\n",
    "            ## 1. Initialize self.attn_proj here.\n",
    "            ## 2. You should use nn.Sequential for self.attn_proj\n",
    "            ## 3. nn.Linear and nn.Tanh are useful.\n",
    "            ## 4. You can create a vector parameter by using:\n",
    "            ## nn.Linear(some_size, 1, bias=False)\n",
    "            ## 5. The first linear layer should have out_features as args['attn_size']\n",
    "            ## 6. You can assume we only have one \"head\" for the attention.\n",
    "            ## 7. We recommend you to implement the mean aggregation first. After \n",
    "            ## the mean aggregation works well in the training, then you can \n",
    "            ## implement this part.\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "            #########################################\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        # TODO: Implement this function that aggregates all message type results.\n",
    "        # Here, xs is a list of tensors (embeddings) with respect to message \n",
    "        # type aggregation results.\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~2 lines of code)\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "            ##########################################\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~10 lines of code)\n",
    "            ## Note:\n",
    "            ## 1. Store the value of attention alpha (as a numpy array) to self.alpha,\n",
    "            ## which has the shape (len(xs), ) self.alpha will be not be used \n",
    "            ## to backpropagate etc. in the model. We will use it to see how much \n",
    "            ## attention the layer pays on different message types.\n",
    "            ## 2. torch.softmax and torch.cat are useful.\n",
    "            ## 3. You might need to reshape the tensors by using the \n",
    "            ## `view()` function https://pytorch.org/docs/stable/tensor_view.html\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out\n",
    "            ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    # TODO: Implement this function that returns a dictionary of `HeteroGNNConv` \n",
    "    # layers where the keys are message types. `hetero_graph` is deepsnap `HeteroGraph`\n",
    "    # object and the `conv` is the `HeteroGNNConv`.\n",
    "\n",
    "    convs = {}\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## (~9 lines of code)\n",
    "\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    ##########################################\n",
    "    \n",
    "    return convs\n",
    "\n",
    "# def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "#     convs = {}\n",
    "\n",
    "#     # Retrieve the number of features for all node types\n",
    "#     node_feature_dims = {\n",
    "#         node_type: hetero_graph.num_node_features(node_type)\n",
    "#         for node_type in hetero_graph.node_types\n",
    "#     }\n",
    "\n",
    "#     for message_type in hetero_graph.message_types:\n",
    "#         src_type, _, dst_type = message_type\n",
    "#         if first_layer:\n",
    "#             # Initialize input channels based on the number of features for each node type\n",
    "#             in_channels_src = node_feature_dims[src_type]\n",
    "#             in_channels_dst = node_feature_dims[dst_type]\n",
    "#         else:\n",
    "#             # For subsequent layers, use the hidden_size as the dimension\n",
    "#             in_channels_src = hidden_size\n",
    "#             in_channels_dst = hidden_size\n",
    "#         out_channels = hidden_size\n",
    "#         convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    \n",
    "#     return convs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        self.convs1 = None\n",
    "        self.convs2 = None\n",
    "\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~10 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For self.convs1 and self.convs2, call generate_convs at first and then\n",
    "        ## pass the returned dictionary of `HeteroGNNConv` to `HeteroGNNWrapperConv`.\n",
    "        ## 2. For self.bns, self.relus and self.post_mps, the keys are node_types.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.node_types` will be helpful.\n",
    "        ## 3. Initialize all batchnorms to torch.nn.BatchNorm1d(hidden_size, eps=1.0).\n",
    "        ## 4. Initialize all relus to nn.LeakyReLU().\n",
    "        ## 5. For self.post_mps, each value in the ModuleDict is a linear layer \n",
    "        ## where the `out_features` is the number of classes for that node type.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.num_node_labels(node_type)` will be\n",
    "        ## useful.\n",
    "\n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            self.post_mps[node_type] = nn.Linear(self.hidden_size, hetero_graph.num_node_labels(node_type))\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "            \n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(self, node_feature, edge_index):\n",
    "        # TODO: Implement the forward function. Notice that `node_feature` is \n",
    "        # a dictionary of tensors where keys are node types and values are \n",
    "        # corresponding feature tensors. The `edge_index` is a dictionary of \n",
    "        # tensors where keys are message types and values are corresponding\n",
    "        # edge index tensors (with respect to each message type).\n",
    "        \n",
    "        \n",
    "        x = node_feature\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~7 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. `deepsnap.hetero_gnn.forward_op` can be helpful.\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        # TODO: remove this for regression tasks\n",
    "        # x = forward_op(x, self.post_mps)\n",
    "        x = forward_op(x, self.fc)\n",
    "        #print(\"X\", x)\n",
    "\n",
    "        ##########################################\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        loss = 0\n",
    "        loss_func = torch.nn.L1Loss()\n",
    "\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For each node type in preds, accumulate computed loss to `loss`\n",
    "        ## 2. Loss need to be computed with respect to the given \n",
    "        mask = y['event'] > 0 # only select nodes with non-negative targets\n",
    "\n",
    "        idx = torch.masked_select(indices['event'], mask)\n",
    "        loss += loss_func(preds['event'][idx], y['event'][idx])\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    loss = None\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## Note:\n",
    "    ## 1. `deepsnap.hetero_graph.HeteroGraph.node_label` is useful\n",
    "    ## 2. Compute the loss here\n",
    "    \n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "    ##########################################\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(model, graph, indices, best_model=None, best_val=0):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        \n",
    "        #print(\"Index\", index)\n",
    "        #print(\"Preds\", preds['event'])\n",
    "\n",
    "        idx = index['event']\n",
    "\n",
    "        L1 = torch.sum(torch.abs(preds['event'][idx] - graph.node_target['event'][idx]))\n",
    "        \n",
    "        accs.append(L1)\n",
    "        #print(\"ACC\", s)\n",
    "\n",
    "        #pred = preds['event'][idx]\n",
    "        \n",
    "        # num_node_types = 0\n",
    "        # micro = 0\n",
    "        # macro = 0\n",
    "        \n",
    "    \n",
    "\n",
    "        # for node_type in preds:\n",
    "        #     idx = index[node_type]\n",
    "        #     pred = preds[node_type][idx]\n",
    "        #     pred = pred.max(1)[1]\n",
    "        #     label_np = graph.node_label[node_type][idx].cpu().numpy()\n",
    "        #     pred_np = pred.cpu().numpy()\n",
    "        #     micro = f1_score(label_np, pred_np, average='micro')\n",
    "        #     macro = f1_score(label_np, pred_np, average='macro')\n",
    "        #     num_node_types += 1\n",
    "        # Averaging f1 score might not make sense, but in our example we only\n",
    "        # have one node type\n",
    "        # micro /= num_node_types\n",
    "        # macro /= num_node_types\n",
    "        #accs.append((micro, macro))\n",
    "    if accs[1] < best_val:\n",
    "        best_val = accs[1]\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    return accs, best_model, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change the following parameters\n",
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 64,\n",
    "    'epochs': 100,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.303,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [1, 1, 1],   # event 0\n",
    "                [2, 2, 2]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([1000, 20], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([0, 0], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVF 2\n",
      "CF 2\n",
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 2\n",
      "\t Feature 2\n",
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 2\n",
      "\t Feature 2\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 2\n",
      "\t Feature 2\n",
      "KEY ('event', 'related', 'concept') <class 'tuple'>\n",
      "KEY NUMS ('event', 'related', 'concept') 8487 8729\n",
      "MAX EDGES tensor(8448) tensor(8728) 8487 8729\n",
      "KEY ('event', 'similar', 'event') <class 'tuple'>\n",
      "KEY NUMS ('event', 'similar', 'event') 8487 8487\n",
      "MAX EDGES tensor(8486) tensor(8486) 8487 8487\n",
      "KEY ('concept', 'related', 'event') <class 'tuple'>\n",
      "KEY NUMS ('concept', 'related', 'event') 8729 8487\n",
      "MAX EDGES tensor(8728) tensor(8448) 8729 8487\n"
     ]
    }
   ],
   "source": [
    "with open(\"./1_concepts_similar.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "    # Convert to directed graph for compatibility with Deepsnap\n",
    "    G = G.to_directed()\n",
    "    \n",
    "hetero_graph = HeteroGraph(G, netlib=nx, directed=True)\n",
    "\n",
    "# Testing\n",
    "# hetero_graph = HeteroGraph(\n",
    "#     node_feature=S_node_feature,\n",
    "#     node_target=S_node_targets,\n",
    "#     edge_index=S_edge_index\n",
    "# )\n",
    "\n",
    "print(\"EVF\", hetero_graph.num_node_features(\"event\"))\n",
    "print(\"CF\", hetero_graph.num_node_features(\"concept\"))\n",
    "\n",
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "\n",
    "# Node feature and node label to device\n",
    "\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "\n",
    "# edge_index1 = hetero_graph.edge_index[(\"concept\", \"related\", \"event\")]\n",
    "# edge_index1 = hetero_graph.edge_index[(\"event\", \"related\", \"concept\")]\n",
    "\n",
    "# Edge_index to sparse tensor and to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key, type(key))\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    \n",
    "    \n",
    "    # TODO: remove quick fix\n",
    "    # if key == ('event', 'related', 'concept'):\n",
    "    #     edge_index = hetero_graph.edge_index[(\"concept\", \"related\", \"event\")]\n",
    "    # else:\n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max(), hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
    "    \n",
    "\n",
    "\n",
    "for key in hetero_graph.node_target:\n",
    "    hetero_graph.node_target[key] = hetero_graph.node_target[key].to(args['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5940])\n",
      "torch.Size([1698])\n",
      "torch.Size([849])\n"
     ]
    }
   ],
   "source": [
    "# train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "# val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "# test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "\n",
    "nEvents = hetero_graph.num_nodes(\"event\")\n",
    "nConcepts = hetero_graph.num_nodes(\"concept\")\n",
    "\n",
    "s1 = 0.7\n",
    "s2 = 0.8\n",
    "\n",
    "train_idx = {   \"event\": torch.tensor(range(0, int(nEvents * s1))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(0, int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "val_idx = {   \"event\": torch.tensor(range(int(nEvents * s1), int(nEvents * s2))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s1), int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "test_idx = {   \"event\": torch.tensor(range(int(nEvents * s2), nEvents)).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s2), nConcepts)).to(args[\"device\"])\n",
    "            }\n",
    "\n",
    "print(train_idx[\"event\"].shape)\n",
    "print(test_idx[\"event\"].shape)\n",
    "print(val_idx[\"event\"].shape)\n",
    "\n",
    "# dataset = deepsnap.dataset.GraphDataset([hetero_graph], task='node')\n",
    "\n",
    "# dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
    "# datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2.126741409301758 Accs [tensor(3.8218e+09, grad_fn=<SumBackward0>), tensor(5.4959e+08, grad_fn=<SumBackward0>), tensor(1.0610e+09, grad_fn=<SumBackward0>)]\n",
      "Epoch 1 Loss 1.6435120105743408 Accs [tensor(2.7394e+08, grad_fn=<SumBackward0>), tensor(39408736., grad_fn=<SumBackward0>), tensor(76872544., grad_fn=<SumBackward0>)]\n",
      "Epoch 2 Loss 5.474625110626221 Accs [tensor(38731448., grad_fn=<SumBackward0>), tensor(5582238.5000, grad_fn=<SumBackward0>), tensor(10841666., grad_fn=<SumBackward0>)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb Cell 11\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m], weight_decay\u001b[39m=\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     loss \u001b[39m=\u001b[39m train(model, optimizer, hetero_graph, train_idx)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     accs, best_model, best_val \u001b[39m=\u001b[39m test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# print(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m#     f\"Epoch {epoch + 1}: loss {round(loss, 5)}, \"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m#     f\"train micro {round(accs[0][0] * 100, 2)}%, train macro {round(accs[0][1] * 100, 2)}%, \"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m#     f\"valid micro {round(accs[1][0] * 100, 2)}%, valid macro {round(accs[1][1] * 100, 2)}%, \"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m#     f\"test micro {round(accs[2][0] * 100, 2)}%, test macro {round(accs[2][1] * 100, 2)}%\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# )\u001b[39;00m\n",
      "\u001b[1;32m/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m############# Your code here #############\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m## Note:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m## 1. `deepsnap.hetero_graph.HeteroGraph.node_label` is useful\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m## 2. Compute the loss here\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mloss(preds, hetero_graph\u001b[39m.\u001b[39;49mnode_target, train_idx)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m##########################################\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32m/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb Cell 11\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m############# Your code here #############\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39m## (~3 lines of code)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39m## Note:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m#for node_type in preds:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m mask \u001b[39m=\u001b[39m y[\u001b[39m'\u001b[39m\u001b[39mevent\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39m# only select nodes with non-negative targets\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m idx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmasked_select(indices[\u001b[39m'\u001b[39;49m\u001b[39mevent\u001b[39;49m\u001b[39m'\u001b[39;49m], mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_func(preds[\u001b[39m'\u001b[39m\u001b[39mevent\u001b[39m\u001b[39m'\u001b[39m][idx], y[\u001b[39m'\u001b[39m\u001b[39mevent\u001b[39m\u001b[39m'\u001b[39m][idx])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/galgantar/Documents/Code/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m##########################################\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n",
    "    # print(\n",
    "    #     f\"Epoch {epoch + 1}: loss {round(loss, 5)}, \"\n",
    "    #     f\"train micro {round(accs[0][0] * 100, 2)}%, train macro {round(accs[0][1] * 100, 2)}%, \"\n",
    "    #     f\"valid micro {round(accs[1][0] * 100, 2)}%, valid macro {round(accs[1][1] * 100, 2)}%, \"\n",
    "    #     f\"test micro {round(accs[2][0] * 100, 2)}%, test macro {round(accs[2][1] * 100, 2)}%\"\n",
    "    # )\n",
    "    print(f\"Epoch {epoch} Loss {loss} Accs {accs}\")\n",
    "\n",
    "best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx])\n",
    "\n",
    "print(\"Best accs\", best_accs)\n",
    "\n",
    "\n",
    "\n",
    "# print(\n",
    "#     f\"Best model: \"\n",
    "#     f\"train micro {round(best_accs[0][0] * 100, 2)}%, train macro {round(best_accs[0][1] * 100, 2)}%, \"\n",
    "#     f\"valid micro {round(best_accs[1][0] * 100, 2)}%, valid macro {round(best_accs[1][1] * 100, 2)}%, \"\n",
    "#     f\"test micro {round(best_accs[2][0] * 100, 2)}%, test macro {round(best_accs[2][1] * 100, 2)}%\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best_model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "# mask = preds['event'] > 0\n",
    "# preds['event'][preds['event'] > 0].shape\n",
    "\n",
    "\n",
    "for i in range(3000):\n",
    "    if preds['event'][i] != -1:\n",
    "        print(preds['event'][i], hetero_graph.node_target['event'][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
