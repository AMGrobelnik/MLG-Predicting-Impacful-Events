{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # To simplify implementation, please initialize both self.lin_dst\n",
    "        # and self.lin_src out_features to out_channels\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "\n",
    "        self.lin_dst = nn.Linear(self.in_channels_src, self.out_channels)\n",
    "        self.lin_src = nn.Linear(self.in_channels_dst, self.out_channels)\n",
    "        self.lin_update = nn.Linear(2*self.out_channels, self.out_channels)\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "    ):\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "        ##########################################\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "        ## Note:\n",
    "        ## 1. Different from what we implemented in Colab 3, we use message_and_aggregate\n",
    "        ## to replace the message and aggregate. The benefit is that we can avoid\n",
    "        ## materializing x_i and x_j, and make the implementation more efficient.\n",
    "        ## 2. To implement efficiently, following PyG documentation is helpful:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html\n",
    "        ## 3. Here edge_index is torch_sparse SparseTensor.\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "        ##########################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~4 lines of code)\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        # print(aggr_out.shape, dst_out.shape)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        # print(aggr_out.shape, )\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "        ##########################################\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "            ############# Your code here #############\n",
    "            ## (~1 line of code)\n",
    "            ## Note:\n",
    "            ## 1. Initialize self.attn_proj here.\n",
    "            ## 2. You should use nn.Sequential for self.attn_proj\n",
    "            ## 3. nn.Linear and nn.Tanh are useful.\n",
    "            ## 4. You can create a vector parameter by using:\n",
    "            ## nn.Linear(some_size, 1, bias=False)\n",
    "            ## 5. The first linear layer should have out_features as args['attn_size']\n",
    "            ## 6. You can assume we only have one \"head\" for the attention.\n",
    "            ## 7. We recommend you to implement the mean aggregation first. After \n",
    "            ## the mean aggregation works well in the training, then you can \n",
    "            ## implement this part.\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "            #########################################\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        # TODO: Implement this function that aggregates all message type results.\n",
    "        # Here, xs is a list of tensors (embeddings) with respect to message \n",
    "        # type aggregation results.\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~2 lines of code)\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "            ##########################################\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~10 lines of code)\n",
    "            ## Note:\n",
    "            ## 1. Store the value of attention alpha (as a numpy array) to self.alpha,\n",
    "            ## which has the shape (len(xs), ) self.alpha will be not be used \n",
    "            ## to backpropagate etc. in the model. We will use it to see how much \n",
    "            ## attention the layer pays on different message types.\n",
    "            ## 2. torch.softmax and torch.cat are useful.\n",
    "            ## 3. You might need to reshape the tensors by using the \n",
    "            ## `view()` function https://pytorch.org/docs/stable/tensor_view.html\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out\n",
    "            ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    # TODO: Implement this function that returns a dictionary of `HeteroGNNConv` \n",
    "    # layers where the keys are message types. `hetero_graph` is deepsnap `HeteroGraph`\n",
    "    # object and the `conv` is the `HeteroGNNConv`.\n",
    "\n",
    "    convs = {}\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## (~9 lines of code)\n",
    "\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    ##########################################\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        self.convs1 = None\n",
    "        self.convs2 = None\n",
    "\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~10 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For self.convs1 and self.convs2, call generate_convs at first and then\n",
    "        ## pass the returned dictionary of `HeteroGNNConv` to `HeteroGNNWrapperConv`.\n",
    "        ## 2. For self.bns, self.relus and self.post_mps, the keys are node_types.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.node_types` will be helpful.\n",
    "        ## 3. Initialize all batchnorms to torch.nn.BatchNorm1d(hidden_size, eps=1.0).\n",
    "        ## 4. Initialize all relus to nn.LeakyReLU().\n",
    "        ## 5. For self.post_mps, each value in the ModuleDict is a linear layer \n",
    "        ## where the `out_features` is the number of classes for that node type.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.num_node_labels(node_type)` will be\n",
    "        ## useful.\n",
    "\n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            self.post_mps[node_type] = nn.Linear(self.hidden_size, hetero_graph.num_node_labels(node_type))\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(self, node_feature, edge_index):\n",
    "        # TODO: Implement the forward function. Notice that `node_feature` is \n",
    "        # a dictionary of tensors where keys are node types and values are \n",
    "        # corresponding feature tensors. The `edge_index` is a dictionary of \n",
    "        # tensors where keys are message types and values are corresponding\n",
    "        # edge index tensors (with respect to each message type).\n",
    "\n",
    "        x = node_feature\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~7 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. `deepsnap.hetero_gnn.forward_op` can be helpful.\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        # TODO: remove this for regression tasks\n",
    "        # x = forward_op(x, self.post_mps)\n",
    "        x = forward_op(x, self.fc)\n",
    "        #print(\"X\", x)\n",
    "\n",
    "        ##########################################\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        loss = 0\n",
    "        loss_func = torch.nn.L1Loss()\n",
    "\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For each node type in preds, accumulate computed loss to `loss`\n",
    "        ## 2. Loss need to be computed with respect to the given index\n",
    "\n",
    "        #for node_type in preds:\n",
    "        mask = y['event'] > 0\n",
    "\n",
    "        idx = torch.masked_select(indices['event'], mask)\n",
    "        loss += loss_func(preds['event'][idx], y['event'][idx])\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    loss = None\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## Note:\n",
    "    ## 1. `deepsnap.hetero_graph.HeteroGraph.node_label` is useful\n",
    "    ## 2. Compute the loss here\n",
    "    \n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "    ##########################################\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(model, graph, indices, best_model=None, best_val=0):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        \n",
    "        #print(\"Index\", index)\n",
    "        #print(\"Preds\", preds['event'])\n",
    "\n",
    "        idx = index['event']\n",
    "\n",
    "        L1 = torch.sum(torch.abs(preds['event'][idx] - graph.node_target['event'][idx]))\n",
    "        \n",
    "        accs.append(L1)\n",
    "        #print(\"ACC\", s)\n",
    "\n",
    "        #pred = preds['event'][idx]\n",
    "        \n",
    "        # num_node_types = 0\n",
    "        # micro = 0\n",
    "        # macro = 0\n",
    "        \n",
    "    \n",
    "\n",
    "        # for node_type in preds:\n",
    "        #     idx = index[node_type]\n",
    "        #     pred = preds[node_type][idx]\n",
    "        #     pred = pred.max(1)[1]\n",
    "        #     label_np = graph.node_label[node_type][idx].cpu().numpy()\n",
    "        #     pred_np = pred.cpu().numpy()\n",
    "        #     micro = f1_score(label_np, pred_np, average='micro')\n",
    "        #     macro = f1_score(label_np, pred_np, average='macro')\n",
    "        #     num_node_types += 1\n",
    "        # Averaging f1 score might not make sense, but in our example we only\n",
    "        # have one node type\n",
    "        # micro /= num_node_types\n",
    "        # macro /= num_node_types\n",
    "        #accs.append((micro, macro))\n",
    "    if accs[1] < best_val:\n",
    "        best_val = accs[1]\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    return accs, best_model, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change the following parameters\n",
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 64,\n",
    "    'epochs': 100,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.303,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [1, 1, 1],   # event 0\n",
    "                [2, 2, 2]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([1000, 20], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([0, 0], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVF 2\n",
      "CF 2\n",
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 2\n",
      "\t Feature 2\n",
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 2\n",
      "\t Feature 2\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 2\n",
      "\t Feature 2\n",
      "KEY ('event', 'related', 'concept') <class 'tuple'>\n",
      "KEY NUMS ('event', 'related', 'concept') 8487 8729\n",
      "MAX EDGES tensor(8448) tensor(8448) 8487 8729\n",
      "KEY ('event', 'similar', 'event') <class 'tuple'>\n",
      "KEY NUMS ('event', 'similar', 'event') 8487 8487\n",
      "MAX EDGES tensor(8486) tensor(8486) 8487 8487\n",
      "KEY ('concept', 'related', 'event') <class 'tuple'>\n",
      "KEY NUMS ('concept', 'related', 'event') 8729 8487\n",
      "MAX EDGES tensor(8448) tensor(8448) 8729 8487\n"
     ]
    }
   ],
   "source": [
    "with open(\"./1_concepts_similar.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "hetero_graph = HeteroGraph(G, netlib=nx, directed=False)\n",
    "\n",
    "# Testing\n",
    "# hetero_graph = HeteroGraph(\n",
    "#     node_feature=S_node_feature,\n",
    "#     node_target=S_node_targets,\n",
    "#     edge_index=S_edge_index\n",
    "# )\n",
    "\n",
    "print(\"EVF\", hetero_graph.num_node_features(\"event\"))\n",
    "print(\"CF\", hetero_graph.num_node_features(\"concept\"))\n",
    "\n",
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "\n",
    "# Node feature and node label to device\n",
    "\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "\n",
    "# edge_index1 = hetero_graph.edge_index[(\"concept\", \"related\", \"event\")]\n",
    "# edge_index1 = hetero_graph.edge_index[(\"event\", \"related\", \"concept\")]\n",
    "\n",
    "# Edge_index to sparse tensor and to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key, type(key))\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    \n",
    "    \n",
    "    # TODO: remove quick fix\n",
    "    if key == ('event', 'related', 'concept'):\n",
    "        edge_index = hetero_graph.edge_index[(\"concept\", \"related\", \"event\")]\n",
    "    else:\n",
    "        edge_index = hetero_graph.edge_index[key]\n",
    "\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max(), hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5940])\n",
      "torch.Size([1698])\n",
      "torch.Size([849])\n"
     ]
    }
   ],
   "source": [
    "# train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "# val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "# test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "\n",
    "nEvents = hetero_graph.num_nodes(\"event\")\n",
    "nConcepts = hetero_graph.num_nodes(\"concept\")\n",
    "\n",
    "s1 = 0.7\n",
    "s2 = 0.8\n",
    "\n",
    "train_idx = {   \"event\": torch.tensor(range(0, int(nEvents * s1))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(0, int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "val_idx = {   \"event\": torch.tensor(range(int(nEvents * s1), int(nEvents * s2))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s1), int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "test_idx = {   \"event\": torch.tensor(range(int(nEvents * s2), 8487)).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s2), 8729)).to(args[\"device\"])\n",
    "            }\n",
    "\n",
    "print(train_idx[\"event\"].shape)\n",
    "print(test_idx[\"event\"].shape)\n",
    "print(val_idx[\"event\"].shape)\n",
    "\n",
    "# dataset = deepsnap.dataset.GraphDataset([hetero_graph], task='node')\n",
    "\n",
    "# dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
    "# datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2.2170863151550293 Accs [tensor(4.3127e+08, grad_fn=<SumBackward0>), tensor(62473544., grad_fn=<SumBackward0>), tensor(1.2271e+08, grad_fn=<SumBackward0>)]\n",
      "Epoch 1 Loss 2.8715450763702393 Accs [tensor(14910079., grad_fn=<SumBackward0>), tensor(2166003.2500, grad_fn=<SumBackward0>), tensor(4308702., grad_fn=<SumBackward0>)]\n",
      "Epoch 2 Loss 4.462274551391602 Accs [tensor(1738281.2500, grad_fn=<SumBackward0>), tensor(243038.6562, grad_fn=<SumBackward0>), tensor(476515.1250, grad_fn=<SumBackward0>)]\n",
      "Epoch 3 Loss 3.276823043823242 Accs [tensor(253412.5938, grad_fn=<SumBackward0>), tensor(35297.2344, grad_fn=<SumBackward0>), tensor(68460.3359, grad_fn=<SumBackward0>)]\n",
      "Epoch 4 Loss 2.0613551139831543 Accs [tensor(106739.5234, grad_fn=<SumBackward0>), tensor(16377.3691, grad_fn=<SumBackward0>), tensor(32403.3945, grad_fn=<SumBackward0>)]\n",
      "Epoch 5 Loss 1.6071022748947144 Accs [tensor(64529.8828, grad_fn=<SumBackward0>), tensor(9707.7109, grad_fn=<SumBackward0>), tensor(18980.6660, grad_fn=<SumBackward0>)]\n",
      "Epoch 6 Loss 1.5382360219955444 Accs [tensor(112963.0469, grad_fn=<SumBackward0>), tensor(16563.7090, grad_fn=<SumBackward0>), tensor(32307.8184, grad_fn=<SumBackward0>)]\n",
      "Epoch 7 Loss 1.555647611618042 Accs [tensor(284028.5625, grad_fn=<SumBackward0>), tensor(40807.5312, grad_fn=<SumBackward0>), tensor(80867.6250, grad_fn=<SumBackward0>)]\n",
      "Epoch 8 Loss 1.2839016914367676 Accs [tensor(174155.5938, grad_fn=<SumBackward0>), tensor(25218.3496, grad_fn=<SumBackward0>), tensor(49246.9609, grad_fn=<SumBackward0>)]\n",
      "Epoch 9 Loss 1.2503238916397095 Accs [tensor(74671.8594, grad_fn=<SumBackward0>), tensor(11096.3164, grad_fn=<SumBackward0>), tensor(21106.9551, grad_fn=<SumBackward0>)]\n",
      "Epoch 10 Loss 1.225045919418335 Accs [tensor(38102.3008, grad_fn=<SumBackward0>), tensor(5834.6953, grad_fn=<SumBackward0>), tensor(10887.4121, grad_fn=<SumBackward0>)]\n",
      "Epoch 11 Loss 1.363286018371582 Accs [tensor(35044.3320, grad_fn=<SumBackward0>), tensor(5367.2246, grad_fn=<SumBackward0>), tensor(9845.5508, grad_fn=<SumBackward0>)]\n",
      "Epoch 12 Loss 1.2458760738372803 Accs [tensor(39313.7695, grad_fn=<SumBackward0>), tensor(5961.7886, grad_fn=<SumBackward0>), tensor(11031.9629, grad_fn=<SumBackward0>)]\n",
      "Epoch 13 Loss 1.2673094272613525 Accs [tensor(28252.9395, grad_fn=<SumBackward0>), tensor(4406.4893, grad_fn=<SumBackward0>), tensor(8049.9336, grad_fn=<SumBackward0>)]\n",
      "Epoch 14 Loss 1.3101540803909302 Accs [tensor(14591.3232, grad_fn=<SumBackward0>), tensor(2535.0693, grad_fn=<SumBackward0>), tensor(4378.1025, grad_fn=<SumBackward0>)]\n",
      "Epoch 15 Loss 1.1837881803512573 Accs [tensor(8823.6387, grad_fn=<SumBackward0>), tensor(1686.8522, grad_fn=<SumBackward0>), tensor(2732.4812, grad_fn=<SumBackward0>)]\n",
      "Epoch 16 Loss 1.2481646537780762 Accs [tensor(8084.7559, grad_fn=<SumBackward0>), tensor(1574.7052, grad_fn=<SumBackward0>), tensor(2522.6926, grad_fn=<SumBackward0>)]\n",
      "Epoch 17 Loss 1.2884316444396973 Accs [tensor(7773.2417, grad_fn=<SumBackward0>), tensor(1543.5845, grad_fn=<SumBackward0>), tensor(2391.4368, grad_fn=<SumBackward0>)]\n",
      "Epoch 18 Loss 1.1963186264038086 Accs [tensor(8209.4414, grad_fn=<SumBackward0>), tensor(1582.7014, grad_fn=<SumBackward0>), tensor(2509.3572, grad_fn=<SumBackward0>)]\n",
      "Epoch 19 Loss 1.2103182077407837 Accs [tensor(8174.1611, grad_fn=<SumBackward0>), tensor(1577.7510, grad_fn=<SumBackward0>), tensor(2502.2488, grad_fn=<SumBackward0>)]\n",
      "Epoch 20 Loss 1.2401286363601685 Accs [tensor(7700.1021, grad_fn=<SumBackward0>), tensor(1522.8784, grad_fn=<SumBackward0>), tensor(2363.3108, grad_fn=<SumBackward0>)]\n",
      "Epoch 21 Loss 1.209700584411621 Accs [tensor(7535.5957, grad_fn=<SumBackward0>), tensor(1520.0756, grad_fn=<SumBackward0>), tensor(2324.7266, grad_fn=<SumBackward0>)]\n",
      "Epoch 22 Loss 1.162442922592163 Accs [tensor(7987.6406, grad_fn=<SumBackward0>), tensor(1566.7759, grad_fn=<SumBackward0>), tensor(2479.8333, grad_fn=<SumBackward0>)]\n",
      "Epoch 23 Loss 1.273573398590088 Accs [tensor(7830.2490, grad_fn=<SumBackward0>), tensor(1566.4380, grad_fn=<SumBackward0>), tensor(2423.5144, grad_fn=<SumBackward0>)]\n",
      "Epoch 24 Loss 1.2431914806365967 Accs [tensor(7473.5762, grad_fn=<SumBackward0>), tensor(1514.4904, grad_fn=<SumBackward0>), tensor(2287.0632, grad_fn=<SumBackward0>)]\n",
      "Epoch 25 Loss 1.1894283294677734 Accs [tensor(7382.1489, grad_fn=<SumBackward0>), tensor(1473.3596, grad_fn=<SumBackward0>), tensor(2233.6887, grad_fn=<SumBackward0>)]\n",
      "Epoch 26 Loss 1.243882417678833 Accs [tensor(7273.4873, grad_fn=<SumBackward0>), tensor(1454.7167, grad_fn=<SumBackward0>), tensor(2182.0488, grad_fn=<SumBackward0>)]\n",
      "Epoch 27 Loss 1.2533314228057861 Accs [tensor(6983.1240, grad_fn=<SumBackward0>), tensor(1445.0408, grad_fn=<SumBackward0>), tensor(2126.7898, grad_fn=<SumBackward0>)]\n",
      "Epoch 28 Loss 1.1763670444488525 Accs [tensor(7335.0293, grad_fn=<SumBackward0>), tensor(1506.1615, grad_fn=<SumBackward0>), tensor(2262.9001, grad_fn=<SumBackward0>)]\n",
      "Epoch 29 Loss 1.22319495677948 Accs [tensor(7585.2910, grad_fn=<SumBackward0>), tensor(1532.8486, grad_fn=<SumBackward0>), tensor(2344.0312, grad_fn=<SumBackward0>)]\n",
      "Epoch 30 Loss 1.2665115594863892 Accs [tensor(7434.5786, grad_fn=<SumBackward0>), tensor(1518.4739, grad_fn=<SumBackward0>), tensor(2255.5479, grad_fn=<SumBackward0>)]\n",
      "Epoch 31 Loss 1.2154799699783325 Accs [tensor(7424.2964, grad_fn=<SumBackward0>), tensor(1494.9203, grad_fn=<SumBackward0>), tensor(2209.9724, grad_fn=<SumBackward0>)]\n",
      "Epoch 32 Loss 1.1600227355957031 Accs [tensor(7362.3779, grad_fn=<SumBackward0>), tensor(1462.9882, grad_fn=<SumBackward0>), tensor(2155.2471, grad_fn=<SumBackward0>)]\n",
      "Epoch 33 Loss 1.2373043298721313 Accs [tensor(7286.9653, grad_fn=<SumBackward0>), tensor(1454.1285, grad_fn=<SumBackward0>), tensor(2132.7375, grad_fn=<SumBackward0>)]\n",
      "Epoch 34 Loss 1.242023229598999 Accs [tensor(7183.2349, grad_fn=<SumBackward0>), tensor(1461.5093, grad_fn=<SumBackward0>), tensor(2137.0979, grad_fn=<SumBackward0>)]\n",
      "Epoch 35 Loss 1.1810892820358276 Accs [tensor(7503.8086, grad_fn=<SumBackward0>), tensor(1526.6855, grad_fn=<SumBackward0>), tensor(2267.3628, grad_fn=<SumBackward0>)]\n",
      "Epoch 36 Loss 1.2069451808929443 Accs [tensor(7702.1265, grad_fn=<SumBackward0>), tensor(1554.7122, grad_fn=<SumBackward0>), tensor(2353.1567, grad_fn=<SumBackward0>)]\n",
      "Epoch 37 Loss 1.2522289752960205 Accs [tensor(7647.7212, grad_fn=<SumBackward0>), tensor(1545.9536, grad_fn=<SumBackward0>), tensor(2331.7117, grad_fn=<SumBackward0>)]\n",
      "Epoch 38 Loss 1.237579345703125 Accs [tensor(7394.5430, grad_fn=<SumBackward0>), tensor(1507.1854, grad_fn=<SumBackward0>), tensor(2227.7393, grad_fn=<SumBackward0>)]\n",
      "Epoch 39 Loss 1.172136902809143 Accs [tensor(7034.5996, grad_fn=<SumBackward0>), tensor(1448.2675, grad_fn=<SumBackward0>), tensor(2106.9358, grad_fn=<SumBackward0>)]\n",
      "Epoch 40 Loss 1.2240982055664062 Accs [tensor(7055.3896, grad_fn=<SumBackward0>), tensor(1444.1835, grad_fn=<SumBackward0>), tensor(2107.5903, grad_fn=<SumBackward0>)]\n",
      "Epoch 41 Loss 1.265443205833435 Accs [tensor(7153.4414, grad_fn=<SumBackward0>), tensor(1458.7501, grad_fn=<SumBackward0>), tensor(2134.7500, grad_fn=<SumBackward0>)]\n",
      "Epoch 42 Loss 1.2499314546585083 Accs [tensor(7395.8496, grad_fn=<SumBackward0>), tensor(1504.5342, grad_fn=<SumBackward0>), tensor(2217.7615, grad_fn=<SumBackward0>)]\n",
      "Epoch 43 Loss 1.1804637908935547 Accs [tensor(7913.3667, grad_fn=<SumBackward0>), tensor(1582.6655, grad_fn=<SumBackward0>), tensor(2375.8806, grad_fn=<SumBackward0>)]\n",
      "Epoch 44 Loss 1.2188714742660522 Accs [tensor(8235.6943, grad_fn=<SumBackward0>), tensor(1629.3827, grad_fn=<SumBackward0>), tensor(2471.8672, grad_fn=<SumBackward0>)]\n",
      "Epoch 45 Loss 1.2680696249008179 Accs [tensor(8341.5635, grad_fn=<SumBackward0>), tensor(1644.1472, grad_fn=<SumBackward0>), tensor(2501.4680, grad_fn=<SumBackward0>)]\n",
      "Epoch 46 Loss 1.2709319591522217 Accs [tensor(8208.1973, grad_fn=<SumBackward0>), tensor(1626.3868, grad_fn=<SumBackward0>), tensor(2463.3911, grad_fn=<SumBackward0>)]\n",
      "Epoch 47 Loss 1.2306076288223267 Accs [tensor(7979.9209, grad_fn=<SumBackward0>), tensor(1592.2288, grad_fn=<SumBackward0>), tensor(2391.6030, grad_fn=<SumBackward0>)]\n",
      "Epoch 48 Loss 1.1535121202468872 Accs [tensor(7513.5908, grad_fn=<SumBackward0>), tensor(1522.3029, grad_fn=<SumBackward0>), tensor(2249.6042, grad_fn=<SumBackward0>)]\n",
      "Epoch 49 Loss 1.2597637176513672 Accs [tensor(7308.4312, grad_fn=<SumBackward0>), tensor(1490.2163, grad_fn=<SumBackward0>), tensor(2184.6340, grad_fn=<SumBackward0>)]\n",
      "Epoch 50 Loss 1.3171887397766113 Accs [tensor(7163.8975, grad_fn=<SumBackward0>), tensor(1463.4796, grad_fn=<SumBackward0>), tensor(2127.5894, grad_fn=<SumBackward0>)]\n",
      "Epoch 51 Loss 1.3151730298995972 Accs [tensor(7287.3394, grad_fn=<SumBackward0>), tensor(1475.3154, grad_fn=<SumBackward0>), tensor(2147.1851, grad_fn=<SumBackward0>)]\n",
      "Epoch 52 Loss 1.2665064334869385 Accs [tensor(7736.0396, grad_fn=<SumBackward0>), tensor(1539.3229, grad_fn=<SumBackward0>), tensor(2271.3669, grad_fn=<SumBackward0>)]\n",
      "Epoch 53 Loss 1.1737638711929321 Accs [tensor(8711.2900, grad_fn=<SumBackward0>), tensor(1667.6927, grad_fn=<SumBackward0>), tensor(2510.0337, grad_fn=<SumBackward0>)]\n",
      "Epoch 54 Loss 1.242037296295166 Accs [tensor(9286.7266, grad_fn=<SumBackward0>), tensor(1743.9945, grad_fn=<SumBackward0>), tensor(2652.9304, grad_fn=<SumBackward0>)]\n",
      "Epoch 55 Loss 1.3090897798538208 Accs [tensor(9486.1934, grad_fn=<SumBackward0>), tensor(1766.0101, grad_fn=<SumBackward0>), tensor(2690.0747, grad_fn=<SumBackward0>)]\n",
      "Epoch 56 Loss 1.329306721687317 Accs [tensor(9631.4150, grad_fn=<SumBackward0>), tensor(1758.2448, grad_fn=<SumBackward0>), tensor(2649.1055, grad_fn=<SumBackward0>)]\n",
      "Epoch 57 Loss 1.3076040744781494 Accs [tensor(9086.5967, grad_fn=<SumBackward0>), tensor(1690.7936, grad_fn=<SumBackward0>), tensor(2527.6296, grad_fn=<SumBackward0>)]\n",
      "Epoch 58 Loss 1.2005215883255005 Accs [tensor(15368.7900, grad_fn=<SumBackward0>), tensor(2103.2771, grad_fn=<SumBackward0>), tensor(2921.7063, grad_fn=<SumBackward0>)]\n",
      "Epoch 59 Loss 1.3967393636703491 Accs [tensor(10539.8076, grad_fn=<SumBackward0>), tensor(1956.1074, grad_fn=<SumBackward0>), tensor(3110.0090, grad_fn=<SumBackward0>)]\n",
      "Epoch 60 Loss 1.2701116800308228 Accs [tensor(8797.3291, grad_fn=<SumBackward0>), tensor(1604.0602, grad_fn=<SumBackward0>), tensor(2321.1582, grad_fn=<SumBackward0>)]\n",
      "Epoch 61 Loss 1.2452943325042725 Accs [tensor(8827.9424, grad_fn=<SumBackward0>), tensor(1623.0698, grad_fn=<SumBackward0>), tensor(2405.0132, grad_fn=<SumBackward0>)]\n",
      "Epoch 62 Loss 1.2272613048553467 Accs [tensor(9858.4922, grad_fn=<SumBackward0>), tensor(1828.6390, grad_fn=<SumBackward0>), tensor(2868.7263, grad_fn=<SumBackward0>)]\n",
      "Epoch 63 Loss 1.1771267652511597 Accs [tensor(9891.8271, grad_fn=<SumBackward0>), tensor(1853.3452, grad_fn=<SumBackward0>), tensor(2915.4290, grad_fn=<SumBackward0>)]\n",
      "Epoch 64 Loss 1.1862765550613403 Accs [tensor(9010.8350, grad_fn=<SumBackward0>), tensor(1742.1602, grad_fn=<SumBackward0>), tensor(2689.2854, grad_fn=<SumBackward0>)]\n",
      "Epoch 65 Loss 1.1616263389587402 Accs [tensor(7914.7710, grad_fn=<SumBackward0>), tensor(1590.4250, grad_fn=<SumBackward0>), tensor(2392.7747, grad_fn=<SumBackward0>)]\n",
      "Epoch 66 Loss 1.20326828956604 Accs [tensor(7310.0586, grad_fn=<SumBackward0>), tensor(1503.7307, grad_fn=<SumBackward0>), tensor(2227.8425, grad_fn=<SumBackward0>)]\n",
      "Epoch 67 Loss 1.21942937374115 Accs [tensor(7269.8145, grad_fn=<SumBackward0>), tensor(1495.2083, grad_fn=<SumBackward0>), tensor(2204.9363, grad_fn=<SumBackward0>)]\n",
      "Epoch 68 Loss 1.1775633096694946 Accs [tensor(7893.1113, grad_fn=<SumBackward0>), tensor(1579.0083, grad_fn=<SumBackward0>), tensor(2363.5945, grad_fn=<SumBackward0>)]\n",
      "Epoch 69 Loss 1.2049129009246826 Accs [tensor(8452.0371, grad_fn=<SumBackward0>), tensor(1649.3036, grad_fn=<SumBackward0>), tensor(2504.0229, grad_fn=<SumBackward0>)]\n",
      "Epoch 70 Loss 1.2311874628067017 Accs [tensor(8746.4629, grad_fn=<SumBackward0>), tensor(1674.1490, grad_fn=<SumBackward0>), tensor(2559.0215, grad_fn=<SumBackward0>)]\n",
      "Epoch 71 Loss 1.2218953371047974 Accs [tensor(8204.8359, grad_fn=<SumBackward0>), tensor(1591.3492, grad_fn=<SumBackward0>), tensor(2398.1060, grad_fn=<SumBackward0>)]\n",
      "Epoch 72 Loss 1.1737897396087646 Accs [tensor(7235.4795, grad_fn=<SumBackward0>), tensor(1469.6855, grad_fn=<SumBackward0>), tensor(2167.1646, grad_fn=<SumBackward0>)]\n",
      "Epoch 73 Loss 1.228432536125183 Accs [tensor(7390.4507, grad_fn=<SumBackward0>), tensor(1507.0092, grad_fn=<SumBackward0>), tensor(2234.6558, grad_fn=<SumBackward0>)]\n",
      "Epoch 74 Loss 1.2600994110107422 Accs [tensor(7276.5405, grad_fn=<SumBackward0>), tensor(1500.0115, grad_fn=<SumBackward0>), tensor(2220.2896, grad_fn=<SumBackward0>)]\n",
      "Epoch 75 Loss 1.2464115619659424 Accs [tensor(6919.6313, grad_fn=<SumBackward0>), tensor(1450.7594, grad_fn=<SumBackward0>), tensor(2135.3247, grad_fn=<SumBackward0>)]\n",
      "Epoch 76 Loss 1.1909607648849487 Accs [tensor(7358.7607, grad_fn=<SumBackward0>), tensor(1509.2909, grad_fn=<SumBackward0>), tensor(2250.5696, grad_fn=<SumBackward0>)]\n",
      "Epoch 77 Loss 1.2144712209701538 Accs [tensor(7694.0303, grad_fn=<SumBackward0>), tensor(1557.9393, grad_fn=<SumBackward0>), tensor(2333.5010, grad_fn=<SumBackward0>)]\n",
      "Epoch 78 Loss 1.250585913658142 Accs [tensor(7949.4023, grad_fn=<SumBackward0>), tensor(1587.9823, grad_fn=<SumBackward0>), tensor(2382.4678, grad_fn=<SumBackward0>)]\n",
      "Epoch 79 Loss 1.248390555381775 Accs [tensor(7985.7656, grad_fn=<SumBackward0>), tensor(1589.3096, grad_fn=<SumBackward0>), tensor(2391.6497, grad_fn=<SumBackward0>)]\n",
      "Epoch 80 Loss 1.2122317552566528 Accs [tensor(7452.7300, grad_fn=<SumBackward0>), tensor(1506.4987, grad_fn=<SumBackward0>), tensor(2231.3840, grad_fn=<SumBackward0>)]\n",
      "Epoch 81 Loss 1.1652252674102783 Accs [tensor(6965.7993, grad_fn=<SumBackward0>), tensor(1440.1041, grad_fn=<SumBackward0>), tensor(2098.3528, grad_fn=<SumBackward0>)]\n",
      "Epoch 82 Loss 1.1853781938552856 Accs [tensor(6976.3779, grad_fn=<SumBackward0>), tensor(1452.1602, grad_fn=<SumBackward0>), tensor(2119.3760, grad_fn=<SumBackward0>)]\n",
      "Epoch 83 Loss 1.1574324369430542 Accs [tensor(7323.7461, grad_fn=<SumBackward0>), tensor(1504.7385, grad_fn=<SumBackward0>), tensor(2233.7275, grad_fn=<SumBackward0>)]\n",
      "Epoch 84 Loss 1.2047228813171387 Accs [tensor(7438.2070, grad_fn=<SumBackward0>), tensor(1520.8234, grad_fn=<SumBackward0>), tensor(2274.9961, grad_fn=<SumBackward0>)]\n",
      "Epoch 85 Loss 1.227442741394043 Accs [tensor(7311.4810, grad_fn=<SumBackward0>), tensor(1502.6768, grad_fn=<SumBackward0>), tensor(2231.2639, grad_fn=<SumBackward0>)]\n",
      "Epoch 86 Loss 1.2015362977981567 Accs [tensor(6967.3457, grad_fn=<SumBackward0>), tensor(1452.0884, grad_fn=<SumBackward0>), tensor(2122.5945, grad_fn=<SumBackward0>)]\n",
      "Epoch 87 Loss 1.1590189933776855 Accs [tensor(6942.5874, grad_fn=<SumBackward0>), tensor(1447.5695, grad_fn=<SumBackward0>), tensor(2113.8010, grad_fn=<SumBackward0>)]\n",
      "Epoch 88 Loss 1.2025998830795288 Accs [tensor(6921.2739, grad_fn=<SumBackward0>), tensor(1439.1854, grad_fn=<SumBackward0>), tensor(2097.5210, grad_fn=<SumBackward0>)]\n",
      "Epoch 89 Loss 1.2008650302886963 Accs [tensor(7113.0093, grad_fn=<SumBackward0>), tensor(1466.5486, grad_fn=<SumBackward0>), tensor(2148.1860, grad_fn=<SumBackward0>)]\n",
      "Epoch 90 Loss 1.1572997570037842 Accs [tensor(7416.7803, grad_fn=<SumBackward0>), tensor(1513.9532, grad_fn=<SumBackward0>), tensor(2239.4521, grad_fn=<SumBackward0>)]\n",
      "Epoch 91 Loss 1.201816201210022 Accs [tensor(7437.3462, grad_fn=<SumBackward0>), tensor(1520.5187, grad_fn=<SumBackward0>), tensor(2254.6423, grad_fn=<SumBackward0>)]\n",
      "Epoch 92 Loss 1.2105611562728882 Accs [tensor(7222.1221, grad_fn=<SumBackward0>), tensor(1491.5015, grad_fn=<SumBackward0>), tensor(2204.2446, grad_fn=<SumBackward0>)]\n",
      "Epoch 93 Loss 1.1827055215835571 Accs [tensor(6849.4468, grad_fn=<SumBackward0>), tensor(1438.0126, grad_fn=<SumBackward0>), tensor(2101.7119, grad_fn=<SumBackward0>)]\n",
      "Epoch 94 Loss 1.1832095384597778 Accs [tensor(7062.7612, grad_fn=<SumBackward0>), tensor(1457.9602, grad_fn=<SumBackward0>), tensor(2129.1987, grad_fn=<SumBackward0>)]\n",
      "Epoch 95 Loss 1.2007181644439697 Accs [tensor(6904.8154, grad_fn=<SumBackward0>), tensor(1445.6136, grad_fn=<SumBackward0>), tensor(2112.4238, grad_fn=<SumBackward0>)]\n",
      "Epoch 96 Loss 1.1704542636871338 Accs [tensor(7421.1812, grad_fn=<SumBackward0>), tensor(1514.1890, grad_fn=<SumBackward0>), tensor(2236.7378, grad_fn=<SumBackward0>)]\n",
      "Epoch 97 Loss 1.194980263710022 Accs [tensor(7543.0669, grad_fn=<SumBackward0>), tensor(1534.5894, grad_fn=<SumBackward0>), tensor(2280.6240, grad_fn=<SumBackward0>)]\n",
      "Epoch 98 Loss 1.2168779373168945 Accs [tensor(7439.2441, grad_fn=<SumBackward0>), tensor(1520.1315, grad_fn=<SumBackward0>), tensor(2253.6001, grad_fn=<SumBackward0>)]\n",
      "Epoch 99 Loss 1.1994727849960327 Accs [tensor(7111.3843, grad_fn=<SumBackward0>), tensor(1472.4136, grad_fn=<SumBackward0>), tensor(2159.4702, grad_fn=<SumBackward0>)]\n",
      "Best accs [tensor(6849.4468, grad_fn=<SumBackward0>), tensor(1438.0126, grad_fn=<SumBackward0>), tensor(2101.7119, grad_fn=<SumBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n",
    "    # print(\n",
    "    #     f\"Epoch {epoch + 1}: loss {round(loss, 5)}, \"\n",
    "    #     f\"train micro {round(accs[0][0] * 100, 2)}%, train macro {round(accs[0][1] * 100, 2)}%, \"\n",
    "    #     f\"valid micro {round(accs[1][0] * 100, 2)}%, valid macro {round(accs[1][1] * 100, 2)}%, \"\n",
    "    #     f\"test micro {round(accs[2][0] * 100, 2)}%, test macro {round(accs[2][1] * 100, 2)}%\"\n",
    "    # )\n",
    "    print(f\"Epoch {epoch} Loss {loss} Accs {accs}\")\n",
    "\n",
    "best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx])\n",
    "\n",
    "print(\"Best accs\", best_accs)\n",
    "\n",
    "\n",
    "\n",
    "# print(\n",
    "#     f\"Best model: \"\n",
    "#     f\"train micro {round(best_accs[0][0] * 100, 2)}%, train macro {round(best_accs[0][1] * 100, 2)}%, \"\n",
    "#     f\"valid micro {round(best_accs[1][0] * 100, 2)}%, valid macro {round(best_accs[1][1] * 100, 2)}%, \"\n",
    "#     f\"test micro {round(best_accs[2][0] * 100, 2)}%, test macro {round(best_accs[2][1] * 100, 2)}%\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.2082, 2.9362, 7.2082, 7.2417, 2.9374, 2.9374, 7.2417, 2.9379, 2.9372,\n",
       "        2.9611, 7.2082, 2.9374, 2.9611, 2.9374, 7.2416, 2.9611, 7.2082, 2.9560,\n",
       "        2.9570, 2.9582, 2.9362, 7.2082, 7.2082, 2.9606, 2.9340, 2.9577, 7.2082,\n",
       "        7.2082, 7.2082, 2.9374, 2.9374, 2.9338, 2.9335, 2.9340, 7.2415, 2.9589,\n",
       "        2.9562, 2.9343, 2.9599, 2.9555, 2.9352, 2.9292, 2.9345, 2.9050, 2.9560,\n",
       "        2.9309, 7.2082, 2.9038, 2.9367, 2.9350, 2.9302, 2.9316, 2.9335, 2.9316,\n",
       "        2.9592, 2.9376, 2.9289, 7.2082, 2.9531, 2.9566, 2.9326, 2.9591, 2.9478,\n",
       "        2.9367, 2.9367, 2.9318, 7.2082, 2.9163, 2.9372, 2.9369, 2.9326, 2.9335,\n",
       "        2.9323, 7.2082, 2.9113, 2.9050, 2.9313, 2.9360, 2.9309, 2.9355, 2.9348,\n",
       "        7.2082, 2.9338, 2.9331, 2.9306, 2.9372, 2.9374, 2.9185, 2.9592, 2.9345,\n",
       "        2.9338, 2.9355, 0.0365, 2.9360, 2.9360, 7.2082, 2.9316, 2.9318, 2.9345,\n",
       "        2.9575, 2.9608, 2.9553, 2.9372, 7.2082, 2.9374, 2.9345, 2.9582, 2.9343,\n",
       "        2.9372, 2.9611, 7.2082, 7.2082, 2.9577, 2.9323, 2.9589, 2.9611, 2.9360,\n",
       "        7.2082, 2.9367, 2.9565, 2.9548, 7.2082, 2.9338, 2.9340, 7.2082, 2.9323,\n",
       "        2.9563, 2.9374, 2.9323, 2.9338, 2.9338, 2.9084, 2.9565, 2.9333, 2.9253,\n",
       "        2.9594, 7.2416, 2.9343, 2.9360, 7.2082, 2.9321, 7.2082, 2.9275, 2.9369,\n",
       "        0.0613, 2.9275, 2.9316, 2.9338, 2.9582, 2.9608, 7.2082, 2.9374, 2.9374,\n",
       "        2.9338, 2.9333, 2.9306, 2.9093, 2.9299, 2.9323, 2.9587, 2.9553, 2.9328,\n",
       "        2.9156, 7.2417, 2.9515, 2.9297, 2.9326, 2.9275, 2.9374, 2.9343, 2.9154,\n",
       "        2.9546, 2.9374, 2.9360, 2.9352, 7.2417, 7.2082, 2.9374, 2.9593, 7.2082,\n",
       "        2.9340, 2.9248, 7.2082, 7.2082, 7.2082, 2.9154, 2.9598, 7.2082, 2.9311,\n",
       "        7.2082, 2.9326, 2.9374, 2.9365, 2.9280, 2.9236, 2.9316, 2.9304, 2.9345,\n",
       "        2.9360, 7.2082, 2.9355, 2.9345, 2.9173, 2.9335, 7.2082],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = best_model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "mask = preds['event'] > 0\n",
    "preds['event'][preds['event'] > 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
