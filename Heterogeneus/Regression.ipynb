{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        self.lin_dst = nn.Linear(in_channels_dst, out_channels)\n",
    "        self.lin_src = nn.Linear(in_channels_src, out_channels)\n",
    "        self.lin_update = nn.Linear(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "        ):\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        \"\"\"\n",
    "        Initializes the HeteroGNNWrapperConv instance.\n",
    "\n",
    "        :param convs: Dictionary of convolution layers for each message type.\n",
    "        :param args: Arguments dictionary containing hyperparameters like hidden_size and attn_size.\n",
    "        :param aggr: Aggregation method, defaults to 'mean'.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_features: Dictionary of node features for each node type.\n",
    "        :param edge_indices: Dictionary of edge indices for each message type.\n",
    "        :return: Aggregated node embeddings for each node type.\n",
    "        \"\"\"\n",
    "        \n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        \n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        \n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "                \n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        \"\"\"\n",
    "        Aggregates node embeddings using the specified aggregation method.\n",
    "\n",
    "        :param xs: List of node embeddings to aggregate.\n",
    "        :return: Aggregated node embeddings as a torch.Tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates convolutional layers for each message type in a heterogeneous graph.\n",
    "\n",
    "    :param hetero_graph: The heterogeneous graph for which convolutions are to be created.\n",
    "    :param conv: The convolutional layer class or constructor.\n",
    "    :param hidden_size: The number of features in the hidden layer.\n",
    "    :param first_layer: Boolean indicating if this is the first layer in the network.\n",
    "    \n",
    "    :return: A dictionary of convolutional layers, keyed by message type.\n",
    "    \"\"\"\n",
    "\n",
    "    convs = {}\n",
    "    \n",
    "    # Extracting all types of messages/edges in the heterogeneous graph.\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        \n",
    "        # Determine the input feature size for source and destination nodes.\n",
    "        # If it's the first layer, use the feature size of the nodes.\n",
    "        # Otherwise, use the hidden size, since from there on the size of embeddings\n",
    "        # is the same for all nodes.\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        \n",
    "        # Create a convolutional layer for this message type and add it to the dictionary.\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, num_layers, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "        \n",
    "        # Initialize the graph convolutional layers\n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        # Initialize batch normalization, ReLU, and fully connected layers for each node type\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            \n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            \n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "            \n",
    "    def forward(self, node_feature, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_feature: Dictionary of node features for each node type.\n",
    "        :param edge_index: Dictionary of edge indices for each message type.\n",
    "        :return: The output embeddings for each node type after passing through the model.\n",
    "        \"\"\"\n",
    "        x = node_feature\n",
    "\n",
    "        # Apply graph convolutional, batch normalization, and ReLU layers\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        x = forward_op(x, self.fc)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        \"\"\"\n",
    "        Computes the loss for the model.\n",
    "\n",
    "        :param preds: Predictions made by the model.\n",
    "        :param y: Ground truth target values.\n",
    "        :param indices: Indices of nodes for which loss should be calculated.\n",
    "        \n",
    "        :return: The computed loss value.\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = 0\n",
    "        loss_func = torch.nn.MSELoss() \n",
    "             \n",
    "        mask = y['event'][indices['event'], 0] != -1\n",
    "        non_zero_idx = torch.masked_select(indices['event'], mask)\n",
    "                \n",
    "        loss += loss_func(preds['event'][non_zero_idx], y['event'][non_zero_idx])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    \"\"\"\n",
    "    Trains the model on the given heterogeneous graph using the specified indices.\n",
    "\n",
    "    :param model: The graph neural network model to train.\n",
    "    :param optimizer: The optimizer used for training the model.\n",
    "    :param hetero_graph: The heterogeneous graph data.\n",
    "    :param train_idx: Indices for training nodes.\n",
    "\n",
    "    :return: The training loss as a float.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "    optimizer.zero_grad() # Zero out any existing gradients \n",
    "    \n",
    "    # Compute predictions using the model\n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    # Compute the loss using model's loss function\n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "\n",
    "    loss.backward() # Backward pass: compute gradient of the loss\n",
    "    optimizer.step() # Perform a single optimization step, updates parameters\n",
    "    \n",
    "    return loss.item() \n",
    "\n",
    "def test(model, graph, indices, best_model, best_tvt_scores):\n",
    "    \"\"\"\n",
    "    Tests the model on given indices and updates the best model based on validation loss.\n",
    "\n",
    "    :param model: The trained graph neural network model.\n",
    "    :param graph: The heterogeneous graph data.\n",
    "    :param indices: List of indices for training, validation, and testing nodes.\n",
    "    :param best_model: The current best model based on validation loss.\n",
    "    :param best_val: The current best validation loss.\n",
    "    \n",
    "    :return: A tuple containing the list of losses for each dataset, the best model, and the best validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    tvt_scores = []\n",
    "    \n",
    "    # Evaluate the model on each set of indices\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        \n",
    "        idx = index['event']\n",
    "\n",
    "        L1 = torch.sum(torch.abs(preds['event'][idx] - graph.node_target['event'][idx]))\n",
    "        \n",
    "        tvt_scores.append(L1)\n",
    "    \n",
    "    # Update the best model and validation loss if the current model performs better\n",
    "    if tvt_scores[1] < best_tvt_scores[1]:\n",
    "        best_tvt_scores = tvt_scores\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    return tvt_scores, best_tvt_scores, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 64,\n",
    "    'epochs': 200,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.201,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell creates a small heterogeneous graph, primarily for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [1, 1, 1],   # event 0\n",
    "                [2, 2, 2]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([[50], [2000]], dtype=torch.float32),\n",
    "    # \"concept\": torch.tensor([[0], [0]], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}\n",
    "\n",
    "# Testing\n",
    "hetero_graph = HeteroGraph(\n",
    "    node_feature=S_node_feature,\n",
    "    node_target=S_node_targets,\n",
    "    edge_index=S_edge_index\n",
    ")\n",
    "\n",
    "train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./1_concepts_similar_llm.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "    # Convert to directed graph for compatibility with Deepsnap\n",
    "    # G = G.to_directed()\n",
    "    \n",
    "hetero_graph = HeteroGraph(G, netlib=nx, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 770\n",
      "\t Feature 1\n",
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 770\n",
      "\t Feature 770\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 1\n",
      "\t Feature 770\n",
      "KEY ('event', 'related', 'concept') <class 'tuple'>\n",
      "KEY NUMS ('event', 'related', 'concept') 8487 8729\n",
      "MAX EDGES tensor(8448) tensor(8728) 8487 8729\n",
      "KEY ('event', 'similar', 'event') <class 'tuple'>\n",
      "KEY NUMS ('event', 'similar', 'event') 8487 8487\n",
      "MAX EDGES tensor(8486) tensor(8486) 8487 8487\n",
      "KEY ('concept', 'related', 'event') <class 'tuple'>\n",
      "KEY NUMS ('concept', 'related', 'event') 8729 8487\n",
      "MAX EDGES tensor(8728) tensor(8448) 8729 8487\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block ensures that all tensors in a heterogeneous graph are transferred to the same \n",
    "computational device, as specified in the 'args' dictionary.\n",
    "\"\"\"\n",
    "\n",
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "# Send node features to device\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "# Create a torch.SparseTensor from edge_index and send it to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key, type(key))\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    \n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max(), hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
    "    \n",
    "# Send node targets to device\n",
    "for key in hetero_graph.node_target:\n",
    "    hetero_graph.node_target[key] = hetero_graph.node_target[key].to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5940])\n",
      "torch.Size([1698])\n",
      "torch.Size([849])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block creates a basic split of a graph's nodes into training, validation, and testing sets. \n",
    "It uses predefined ratios to divide 'event' and 'concept' nodes in the heterogeneous graph for a simple \n",
    "dataset split, mainly for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "nEvents = hetero_graph.num_nodes(\"event\")\n",
    "nConcepts = hetero_graph.num_nodes(\"concept\")\n",
    "\n",
    "s1 = 0.7\n",
    "s2 = 0.8\n",
    "\n",
    "train_idx = {   \"event\": torch.tensor(range(0, int(nEvents * s1))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(0, int(nConcepts * s1))).to(args[\"device\"])\n",
    "            }\n",
    "val_idx = {   \"event\": torch.tensor(range(int(nEvents * s1), int(nEvents * s2))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s1), int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "test_idx = {   \"event\": torch.tensor(range(int(nEvents * s2), nEvents)).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s2), nConcepts)).to(args[\"device\"])\n",
    "            }\n",
    "\n",
    "print(train_idx[\"event\"].shape)\n",
    "print(test_idx[\"event\"].shape)\n",
    "print(val_idx[\"event\"].shape)\n",
    "\n",
    "\n",
    "# TODO: Add node labels to the nodes and try to make the deepsnap split work even for regression!\n",
    "\n",
    "# dataset = deepsnap.dataset.GraphDataset([hetero_graph], task='node')\n",
    "\n",
    "# dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
    "# datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 1981.8746 Current Train,Val,Test Scores [179213152.0, 25769228.0, 51044104.0]\n",
      "Epoch 1 Loss 1873.0317 Current Train,Val,Test Scores [609543808.0, 87671520.0, 172738880.0]\n",
      "Epoch 2 Loss 1295.5286 Current Train,Val,Test Scores [25305778.0, 3632997.75, 7174771.5]\n",
      "Epoch 3 Loss 1186.0565 Current Train,Val,Test Scores [3981407.5, 573750.4375, 1114697.25]\n",
      "Epoch 4 Loss 1164.7301 Current Train,Val,Test Scores [4428557.0, 637731.25, 1251712.75]\n",
      "Epoch 5 Loss 935.9242 Current Train,Val,Test Scores [3336070.0, 481086.75, 941897.25]\n",
      "Epoch 6 Loss 798.0152 Current Train,Val,Test Scores [4406925.0, 635921.875, 1230875.0]\n",
      "Epoch 7 Loss 634.5857 Current Train,Val,Test Scores [5072590.5, 731704.4375, 1411837.0]\n",
      "Epoch 8 Loss 465.5409 Current Train,Val,Test Scores [3157510.0, 456063.25, 880504.875]\n",
      "Epoch 9 Loss 361.7397 Current Train,Val,Test Scores [1393027.25, 201721.6875, 391204.5]\n",
      "Epoch 10 Loss 322.8487 Current Train,Val,Test Scores [688788.375, 100015.859375, 194704.15625]\n",
      "Epoch 11 Loss 392.1284 Current Train,Val,Test Scores [455655.40625, 65471.390625, 129075.046875]\n",
      "Epoch 12 Loss 366.9724 Current Train,Val,Test Scores [249827.34375, 36318.7265625, 70936.0078125]\n",
      "Epoch 13 Loss 347.7849 Current Train,Val,Test Scores [143196.109375, 20931.662109375, 40662.3984375]\n",
      "Epoch 14 Loss 310.1360 Current Train,Val,Test Scores [100087.4375, 14744.466796875, 28441.01953125]\n",
      "Epoch 15 Loss 305.1586 Current Train,Val,Test Scores [73045.640625, 10829.7763671875, 20796.447265625]\n",
      "Epoch 16 Loss 290.8959 Current Train,Val,Test Scores [53103.2265625, 7858.7138671875, 15165.1787109375]\n",
      "Epoch 17 Loss 281.0331 Current Train,Val,Test Scores [34585.34375, 4627.685546875, 9875.60546875]\n",
      "Epoch 18 Loss 283.5180 Current Train,Val,Test Scores [47166.375, 6859.1826171875, 12926.927734375]\n",
      "Epoch 19 Loss 282.5334 Current Train,Val,Test Scores [122480.2578125, 17882.62890625, 33653.8125]\n",
      "Epoch 20 Loss 276.7919 Current Train,Val,Test Scores [233934.53125, 34059.2578125, 64403.8359375]\n",
      "Epoch 21 Loss 273.1748 Current Train,Val,Test Scores [395832.0, 57430.73046875, 109112.25]\n",
      "Epoch 22 Loss 270.5888 Current Train,Val,Test Scores [576777.5625, 83430.1328125, 159261.25]\n",
      "Epoch 23 Loss 265.1924 Current Train,Val,Test Scores [723165.0, 104446.2265625, 199844.40625]\n",
      "Epoch 24 Loss 258.2860 Current Train,Val,Test Scores [811675.5, 117125.265625, 224393.65625]\n",
      "Epoch 25 Loss 252.2033 Current Train,Val,Test Scores [823908.625, 118832.4921875, 227807.5]\n",
      "Epoch 26 Loss 247.7318 Current Train,Val,Test Scores [768289.375, 110798.140625, 212409.140625]\n",
      "Epoch 27 Loss 244.0493 Current Train,Val,Test Scores [664303.5, 95833.40625, 183591.59375]\n",
      "Epoch 28 Loss 239.4548 Current Train,Val,Test Scores [536139.4375, 77414.0, 148066.265625]\n",
      "Epoch 29 Loss 234.7984 Current Train,Val,Test Scores [405559.0, 58649.421875, 111894.3984375]\n",
      "Epoch 30 Loss 232.0459 Current Train,Val,Test Scores [285778.28125, 41421.578125, 78733.609375]\n",
      "Epoch 31 Loss 229.6981 Current Train,Val,Test Scores [187044.03125, 27208.25390625, 51357.74609375]\n",
      "Epoch 32 Loss 225.9568 Current Train,Val,Test Scores [116939.1328125, 17109.94140625, 31956.767578125]\n",
      "Epoch 33 Loss 222.3146 Current Train,Val,Test Scores [71435.7734375, 10499.9482421875, 19447.423828125]\n",
      "Epoch 34 Loss 219.1111 Current Train,Val,Test Scores [44905.7734375, 6551.6962890625, 12329.5234375]\n",
      "Epoch 35 Loss 216.0421 Current Train,Val,Test Scores [27085.783203125, 3838.578125, 7624.626953125]\n",
      "Epoch 36 Loss 212.0248 Current Train,Val,Test Scores [19050.77734375, 2780.548583984375, 5388.51318359375]\n",
      "Epoch 37 Loss 208.4928 Current Train,Val,Test Scores [17534.763671875, 2584.359375, 5064.34765625]\n",
      "Epoch 38 Loss 205.6597 Current Train,Val,Test Scores [18359.30078125, 2782.265625, 5310.42431640625]\n",
      "Epoch 39 Loss 202.9640 Current Train,Val,Test Scores [19995.7734375, 3069.412109375, 5763.8486328125]\n",
      "Epoch 40 Loss 200.2464 Current Train,Val,Test Scores [21435.12890625, 3308.0166015625, 6165.43896484375]\n",
      "Epoch 41 Loss 197.7640 Current Train,Val,Test Scores [22573.439453125, 3487.138671875, 6485.193359375]\n",
      "Epoch 42 Loss 195.7929 Current Train,Val,Test Scores [23449.6328125, 3620.794677734375, 6727.5]\n",
      "Epoch 43 Loss 193.8530 Current Train,Val,Test Scores [24070.734375, 3719.25048828125, 6902.4921875]\n",
      "Epoch 44 Loss 191.4818 Current Train,Val,Test Scores [24524.94140625, 3792.95068359375, 7034.4921875]\n",
      "Epoch 45 Loss 188.9328 Current Train,Val,Test Scores [24902.005859375, 3848.8505859375, 7146.810546875]\n",
      "Epoch 46 Loss 186.3573 Current Train,Val,Test Scores [25161.5703125, 3873.76416015625, 7226.892578125]\n",
      "Epoch 47 Loss 184.1096 Current Train,Val,Test Scores [25269.4453125, 3856.88720703125, 7268.8173828125]\n",
      "Epoch 48 Loss 182.1277 Current Train,Val,Test Scores [25298.05859375, 3797.546875, 7278.06298828125]\n",
      "Epoch 49 Loss 180.1197 Current Train,Val,Test Scores [25102.9453125, 3675.95068359375, 7226.0048828125]\n",
      "Epoch 50 Loss 178.2342 Current Train,Val,Test Scores [24654.34375, 3525.989013671875, 7131.54296875]\n",
      "Epoch 51 Loss 176.4218 Current Train,Val,Test Scores [29042.765625, 4189.9033203125, 8226.98046875]\n",
      "Epoch 52 Loss 174.5563 Current Train,Val,Test Scores [50262.984375, 7318.029296875, 13785.833984375]\n",
      "Epoch 53 Loss 173.0269 Current Train,Val,Test Scores [84538.9453125, 12227.13671875, 23254.951171875]\n",
      "Epoch 54 Loss 171.7002 Current Train,Val,Test Scores [124995.2421875, 18049.9609375, 34503.49609375]\n",
      "Epoch 55 Loss 170.1883 Current Train,Val,Test Scores [166159.6875, 23992.8046875, 45902.59375]\n",
      "Epoch 56 Loss 168.3966 Current Train,Val,Test Scores [204623.6875, 29532.85546875, 56574.609375]\n",
      "Epoch 57 Loss 165.9355 Current Train,Val,Test Scores [239492.59375, 34551.9609375, 66254.75]\n",
      "Epoch 58 Loss 163.5450 Current Train,Val,Test Scores [267040.40625, 38482.55859375, 73927.8125]\n",
      "Epoch 59 Loss 160.8378 Current Train,Val,Test Scores [329406.15625, 47628.3828125, 91513.40625]\n",
      "Epoch 60 Loss 158.4518 Current Train,Val,Test Scores [357069.4375, 51606.09375, 99445.609375]\n",
      "Epoch 61 Loss 158.0727 Current Train,Val,Test Scores [435520.90625, 63021.4296875, 121791.1796875]\n",
      "Epoch 62 Loss 156.3846 Current Train,Val,Test Scores [406862.09375, 58854.84765625, 113930.6171875]\n",
      "Epoch 63 Loss 151.0833 Current Train,Val,Test Scores [415707.71875, 60157.3984375, 116715.9453125]\n",
      "Epoch 64 Loss 143.7314 Current Train,Val,Test Scores [374247.375, 54157.90625, 105259.4765625]\n",
      "Epoch 65 Loss 138.9502 Current Train,Val,Test Scores [187993.4375, 27160.8359375, 52642.859375]\n",
      "Epoch 66 Loss 131.8750 Current Train,Val,Test Scores [232989.9375, 33670.94140625, 65676.46875]\n",
      "Epoch 67 Loss 120.6968 Current Train,Val,Test Scores [109116.359375, 15778.8193359375, 30988.80859375]\n",
      "Epoch 68 Loss 106.8435 Current Train,Val,Test Scores [172405.6875, 25207.58203125, 49022.87890625]\n",
      "Epoch 69 Loss 78.2973 Current Train,Val,Test Scores [970180.3125, 139933.640625, 275491.6875]\n",
      "Epoch 70 Loss 168.8397 Current Train,Val,Test Scores [1115735.75, 160834.15625, 316369.90625]\n",
      "Epoch 71 Loss 98.2300 Current Train,Val,Test Scores [1232065.75, 177787.21875, 349458.5]\n",
      "Epoch 72 Loss 91.7282 Current Train,Val,Test Scores [1227333.5, 177131.171875, 347877.1875]\n",
      "Epoch 73 Loss 88.5693 Current Train,Val,Test Scores [1219766.875, 175990.15625, 345675.25]\n",
      "Epoch 74 Loss 97.8725 Current Train,Val,Test Scores [1999453.0, 288684.28125, 567091.8125]\n",
      "Epoch 75 Loss 130.6438 Current Train,Val,Test Scores [1910727.25, 275873.125, 541880.75]\n",
      "Epoch 76 Loss 78.8082 Current Train,Val,Test Scores [1472590.625, 212423.671875, 417443.375]\n",
      "Epoch 77 Loss 104.6315 Current Train,Val,Test Scores [2896758.25, 418611.75, 822222.375]\n",
      "Epoch 78 Loss 84.9402 Current Train,Val,Test Scores [2925344.0, 422741.40625, 830595.3125]\n",
      "Epoch 79 Loss 100.3394 Current Train,Val,Test Scores [1753052.0, 252934.71875, 497705.71875]\n",
      "Epoch 80 Loss 76.2897 Current Train,Val,Test Scores [888389.625, 128546.515625, 252288.65625]\n",
      "Epoch 81 Loss 78.0814 Current Train,Val,Test Scores [670615.25, 97422.1796875, 190672.25]\n",
      "Epoch 82 Loss 63.7384 Current Train,Val,Test Scores [449875.5625, 65271.8046875, 127886.1875]\n",
      "Epoch 83 Loss 77.3505 Current Train,Val,Test Scores [537461.375, 78166.328125, 152993.53125]\n",
      "Epoch 84 Loss 59.7865 Current Train,Val,Test Scores [647681.25, 94132.625, 184309.15625]\n",
      "Epoch 85 Loss 70.7831 Current Train,Val,Test Scores [691360.8125, 100316.28125, 196422.53125]\n",
      "Epoch 86 Loss 55.7840 Current Train,Val,Test Scores [776001.25, 112264.328125, 220492.03125]\n",
      "Epoch 87 Loss 59.1686 Current Train,Val,Test Scores [877501.375, 127124.6328125, 249250.078125]\n",
      "Epoch 88 Loss 53.8918 Current Train,Val,Test Scores [902345.125, 130621.78125, 256285.421875]\n",
      "Epoch 89 Loss 56.2791 Current Train,Val,Test Scores [858607.75, 124059.78125, 243955.78125]\n",
      "Epoch 90 Loss 52.4495 Current Train,Val,Test Scores [780357.75, 112795.5625, 221739.09375]\n",
      "Epoch 91 Loss 51.3543 Current Train,Val,Test Scores [684472.1875, 99253.1796875, 194365.53125]\n",
      "Epoch 92 Loss 47.7341 Current Train,Val,Test Scores [599800.125, 87129.53125, 170347.15625]\n",
      "Epoch 93 Loss 50.2171 Current Train,Val,Test Scores [550632.4375, 79922.578125, 156332.3125]\n",
      "Epoch 94 Loss 47.3263 Current Train,Val,Test Scores [523437.125, 75703.421875, 148533.921875]\n",
      "Epoch 95 Loss 47.6191 Current Train,Val,Test Scores [492980.84375, 71193.375, 139896.03125]\n",
      "Epoch 96 Loss 44.4390 Current Train,Val,Test Scores [448327.71875, 64715.60546875, 127187.796875]\n",
      "Epoch 97 Loss 44.8336 Current Train,Val,Test Scores [395104.6875, 56957.4375, 112100.140625]\n",
      "Epoch 98 Loss 45.4778 Current Train,Val,Test Scores [334748.625, 48159.5234375, 95043.984375]\n",
      "Epoch 99 Loss 42.9597 Current Train,Val,Test Scores [265462.65625, 38171.8046875, 75331.921875]\n",
      "Epoch 100 Loss 43.2402 Current Train,Val,Test Scores [191592.25, 27579.103515625, 54234.359375]\n",
      "Epoch 101 Loss 42.5563 Current Train,Val,Test Scores [131428.46875, 18955.61328125, 37111.25390625]\n",
      "Epoch 102 Loss 41.8961 Current Train,Val,Test Scores [106897.4296875, 15315.193359375, 30290.7109375]\n",
      "Epoch 103 Loss 42.1924 Current Train,Val,Test Scores [93876.40625, 13391.26171875, 26724.28515625]\n",
      "Epoch 104 Loss 40.9479 Current Train,Val,Test Scores [86203.765625, 12331.3984375, 24625.953125]\n",
      "Epoch 105 Loss 39.6241 Current Train,Val,Test Scores [75332.9375, 10856.361328125, 21475.25]\n",
      "Epoch 106 Loss 40.4825 Current Train,Val,Test Scores [55682.8359375, 8037.71533203125, 15918.7890625]\n",
      "Epoch 107 Loss 39.5637 Current Train,Val,Test Scores [41521.4453125, 6042.84423828125, 11851.2255859375]\n",
      "Epoch 108 Loss 38.4322 Current Train,Val,Test Scores [33801.7421875, 4960.7666015625, 9640.3310546875]\n",
      "Epoch 109 Loss 38.5224 Current Train,Val,Test Scores [30917.41015625, 4547.1923828125, 8851.7216796875]\n",
      "Epoch 110 Loss 38.0602 Current Train,Val,Test Scores [29887.529296875, 4400.57177734375, 8559.345703125]\n",
      "Epoch 111 Loss 37.5266 Current Train,Val,Test Scores [32240.6796875, 4793.1123046875, 9227.7265625]\n",
      "Epoch 112 Loss 36.8546 Current Train,Val,Test Scores [38759.38671875, 5770.2705078125, 11004.25390625]\n",
      "Epoch 113 Loss 36.5722 Current Train,Val,Test Scores [46252.5703125, 6845.939453125, 13076.7578125]\n",
      "Epoch 114 Loss 36.3110 Current Train,Val,Test Scores [48430.73828125, 7147.5078125, 13692.8603515625]\n",
      "Epoch 115 Loss 35.4550 Current Train,Val,Test Scores [51910.890625, 7601.2861328125, 14664.4052734375]\n",
      "Epoch 116 Loss 35.1727 Current Train,Val,Test Scores [57895.359375, 8442.794921875, 16322.9013671875]\n",
      "Epoch 117 Loss 34.9890 Current Train,Val,Test Scores [65095.2890625, 9508.06640625, 18308.43359375]\n",
      "Epoch 118 Loss 34.4588 Current Train,Val,Test Scores [74627.21875, 10931.892578125, 20946.6953125]\n",
      "Epoch 119 Loss 33.9924 Current Train,Val,Test Scores [94390.453125, 13754.734375, 26527.83203125]\n",
      "Epoch 120 Loss 33.7652 Current Train,Val,Test Scores [90272.9140625, 13163.4716796875, 25277.8046875]\n",
      "Epoch 121 Loss 33.6068 Current Train,Val,Test Scores [87463.75, 12762.2197265625, 24526.28515625]\n",
      "Epoch 122 Loss 33.2169 Current Train,Val,Test Scores [89964.59375, 13077.02734375, 25223.505859375]\n",
      "Epoch 123 Loss 33.0319 Current Train,Val,Test Scores [93794.4296875, 13626.3564453125, 26287.947265625]\n",
      "Epoch 124 Loss 32.8699 Current Train,Val,Test Scores [98734.6875, 14384.08203125, 27658.1171875]\n",
      "Epoch 125 Loss 32.4351 Current Train,Val,Test Scores [109557.09375, 15983.8984375, 30647.7734375]\n",
      "Epoch 126 Loss 32.2914 Current Train,Val,Test Scores [137396.640625, 20012.125, 38504.4453125]\n",
      "Epoch 127 Loss 32.1540 Current Train,Val,Test Scores [146208.25, 21292.326171875, 40980.14453125]\n",
      "Epoch 128 Loss 31.8226 Current Train,Val,Test Scores [131034.7109375, 19087.51171875, 36672.03515625]\n",
      "Epoch 129 Loss 31.5287 Current Train,Val,Test Scores [125295.453125, 18264.380859375, 35006.99609375]\n",
      "Epoch 130 Loss 31.4008 Current Train,Val,Test Scores [130013.0703125, 18938.21875, 36315.546875]\n",
      "Epoch 131 Loss 31.1614 Current Train,Val,Test Scores [144523.890625, 21008.841796875, 40410.94140625]\n",
      "Epoch 132 Loss 30.8828 Current Train,Val,Test Scores [159810.84375, 23202.25390625, 44723.171875]\n",
      "Epoch 133 Loss 30.8053 Current Train,Val,Test Scores [145597.375, 21124.5390625, 40706.70703125]\n",
      "Epoch 134 Loss 30.6344 Current Train,Val,Test Scores [129824.1796875, 18884.3984375, 36279.84765625]\n",
      "Epoch 135 Loss 30.4435 Current Train,Val,Test Scores [122800.21875, 17890.12890625, 34342.4375]\n",
      "Epoch 136 Loss 30.3113 Current Train,Val,Test Scores [118105.5625, 17218.72265625, 33045.35546875]\n",
      "Epoch 137 Loss 30.1992 Current Train,Val,Test Scores [114945.3984375, 16712.61328125, 32168.12109375]\n",
      "Epoch 138 Loss 30.0248 Current Train,Val,Test Scores [114541.3125, 16661.861328125, 32132.44140625]\n",
      "Epoch 139 Loss 29.9361 Current Train,Val,Test Scores [103883.375, 15137.552734375, 29170.83203125]\n",
      "Epoch 140 Loss 29.8199 Current Train,Val,Test Scores [86164.125, 12624.42578125, 24205.7578125]\n",
      "Epoch 141 Loss 29.7016 Current Train,Val,Test Scores [75255.3359375, 11043.556640625, 21173.88671875]\n",
      "Epoch 142 Loss 29.6009 Current Train,Val,Test Scores [72659.015625, 10688.8359375, 20463.4765625]\n",
      "Epoch 143 Loss 29.4945 Current Train,Val,Test Scores [82108.9296875, 12045.328125, 23167.76953125]\n",
      "Epoch 144 Loss 29.4217 Current Train,Val,Test Scores [98379.609375, 14417.810546875, 27802.94140625]\n",
      "Epoch 145 Loss 29.3564 Current Train,Val,Test Scores [95544.03125, 14018.2421875, 27034.74609375]\n",
      "Epoch 146 Loss 29.2787 Current Train,Val,Test Scores [75483.734375, 11120.8447265625, 21389.091796875]\n",
      "Epoch 147 Loss 29.1841 Current Train,Val,Test Scores [55029.5078125, 8157.4296875, 15619.859375]\n",
      "Epoch 148 Loss 29.1196 Current Train,Val,Test Scores [45636.4140625, 6784.1552734375, 12972.6953125]\n",
      "Epoch 149 Loss 29.0379 Current Train,Val,Test Scores [41053.328125, 6121.65185546875, 11695.955078125]\n",
      "Epoch 150 Loss 28.9748 Current Train,Val,Test Scores [34399.6796875, 5166.68896484375, 9871.123046875]\n",
      "Epoch 151 Loss 28.9172 Current Train,Val,Test Scores [27318.32421875, 4170.490234375, 7919.52294921875]\n",
      "Epoch 152 Loss 28.8478 Current Train,Val,Test Scores [23566.873046875, 3600.986328125, 6914.06201171875]\n",
      "Epoch 153 Loss 28.7969 Current Train,Val,Test Scores [24086.3515625, 3673.82080078125, 7045.328125]\n",
      "Epoch 154 Loss 28.7419 Current Train,Val,Test Scores [25283.90234375, 3848.41162109375, 7383.23779296875]\n",
      "Epoch 155 Loss 28.6815 Current Train,Val,Test Scores [25843.41015625, 3926.303955078125, 7537.18896484375]\n",
      "Epoch 156 Loss 28.6381 Current Train,Val,Test Scores [25213.92578125, 3845.154296875, 7358.9775390625]\n",
      "Epoch 157 Loss 28.5844 Current Train,Val,Test Scores [24476.529296875, 3725.410888671875, 7145.95849609375]\n",
      "Epoch 158 Loss 28.5430 Current Train,Val,Test Scores [24520.5234375, 3726.43798828125, 7153.646484375]\n",
      "Epoch 159 Loss 28.4963 Current Train,Val,Test Scores [25208.51171875, 3833.4140625, 7350.93017578125]\n",
      "Epoch 160 Loss 28.4538 Current Train,Val,Test Scores [25264.15234375, 3841.67333984375, 7364.9921875]\n",
      "Epoch 161 Loss 28.4187 Current Train,Val,Test Scores [24042.08203125, 3642.041748046875, 7012.556640625]\n",
      "Epoch 162 Loss 28.3764 Current Train,Val,Test Scores [22558.1484375, 3399.462890625, 6597.74609375]\n",
      "Epoch 163 Loss 28.3429 Current Train,Val,Test Scores [22268.5234375, 3349.00146484375, 6516.05712890625]\n",
      "Epoch 164 Loss 28.3047 Current Train,Val,Test Scores [22727.58203125, 3424.310791015625, 6645.3935546875]\n",
      "Epoch 165 Loss 28.2687 Current Train,Val,Test Scores [22431.3828125, 3375.220703125, 6560.4501953125]\n",
      "Epoch 166 Loss 28.2351 Current Train,Val,Test Scores [21442.63671875, 3208.12548828125, 6279.7412109375]\n",
      "Epoch 167 Loss 28.1992 Current Train,Val,Test Scores [20998.5859375, 3132.67626953125, 6152.20751953125]\n",
      "Epoch 168 Loss 28.1710 Current Train,Val,Test Scores [21437.16015625, 3206.92822265625, 6276.951171875]\n",
      "Epoch 169 Loss 28.1399 Current Train,Val,Test Scores [21840.134765625, 3272.76318359375, 6390.94921875]\n",
      "Epoch 170 Loss 28.1140 Current Train,Val,Test Scores [21415.087890625, 3196.05908203125, 6270.99609375]\n",
      "Epoch 171 Loss 28.0859 Current Train,Val,Test Scores [20666.53125, 3066.0361328125, 6058.8037109375]\n",
      "Epoch 172 Loss 28.0605 Current Train,Val,Test Scores [20685.02734375, 3066.229736328125, 6063.50048828125]\n",
      "Epoch 173 Loss 28.0348 Current Train,Val,Test Scores [21552.76953125, 3208.224365234375, 6313.64892578125]\n",
      "Epoch 174 Loss 28.0064 Current Train,Val,Test Scores [21654.36328125, 3222.4638671875, 6341.88671875]\n",
      "Epoch 175 Loss 27.9824 Current Train,Val,Test Scores [21095.376953125, 3128.62255859375, 6177.7841796875]\n",
      "Epoch 176 Loss 27.9580 Current Train,Val,Test Scores [21078.392578125, 3125.5205078125, 6172.1318359375]\n",
      "Epoch 177 Loss 27.9381 Current Train,Val,Test Scores [21749.0, 3234.39599609375, 6365.07470703125]\n",
      "Epoch 178 Loss 27.9145 Current Train,Val,Test Scores [22393.86328125, 3338.85791015625, 6550.0703125]\n",
      "Epoch 179 Loss 27.8961 Current Train,Val,Test Scores [22339.11328125, 3329.28662109375, 6532.4736328125]\n",
      "Epoch 180 Loss 27.8757 Current Train,Val,Test Scores [21897.49609375, 3255.894287109375, 6400.9970703125]\n",
      "Epoch 181 Loss 27.8569 Current Train,Val,Test Scores [22153.24609375, 3297.362060546875, 6473.947265625]\n",
      "Epoch 182 Loss 27.8377 Current Train,Val,Test Scores [22419.49609375, 3341.316650390625, 6550.79248046875]\n",
      "Epoch 183 Loss 27.8207 Current Train,Val,Test Scores [21496.662109375, 3189.98974609375, 6282.310546875]\n",
      "Epoch 184 Loss 27.8010 Current Train,Val,Test Scores [20476.0546875, 3024.353515625, 5987.8408203125]\n",
      "Epoch 185 Loss 27.7837 Current Train,Val,Test Scores [20265.759765625, 2991.002685546875, 5927.2861328125]\n",
      "Epoch 186 Loss 27.7673 Current Train,Val,Test Scores [20252.900390625, 2988.70556640625, 5923.2060546875]\n",
      "Epoch 187 Loss 27.7499 Current Train,Val,Test Scores [20196.51953125, 2979.751708984375, 5906.9326171875]\n",
      "Epoch 188 Loss 27.7329 Current Train,Val,Test Scores [20045.58984375, 2955.55126953125, 5863.703125]\n",
      "Epoch 189 Loss 27.7168 Current Train,Val,Test Scores [20073.79296875, 2959.64111328125, 5871.7138671875]\n",
      "Epoch 190 Loss 27.7005 Current Train,Val,Test Scores [20640.271484375, 3051.16064453125, 6034.5966796875]\n",
      "Epoch 191 Loss 27.6846 Current Train,Val,Test Scores [20797.26171875, 3076.229736328125, 6079.7802734375]\n",
      "Epoch 192 Loss 27.6709 Current Train,Val,Test Scores [20798.54296875, 3076.23388671875, 6080.32421875]\n",
      "Epoch 193 Loss 27.6560 Current Train,Val,Test Scores [21118.046875, 3128.6005859375, 6172.90625]\n",
      "Epoch 194 Loss 27.6427 Current Train,Val,Test Scores [21517.4375, 3194.169921875, 6288.8603515625]\n",
      "Epoch 195 Loss 27.6297 Current Train,Val,Test Scores [21255.876953125, 3150.9541015625, 6213.35546875]\n",
      "Epoch 196 Loss 27.6167 Current Train,Val,Test Scores [20724.55859375, 3064.073486328125, 6060.072265625]\n",
      "Epoch 197 Loss 27.6040 Current Train,Val,Test Scores [20374.482421875, 3006.779296875, 5959.5771484375]\n",
      "Epoch 198 Loss 27.5913 Current Train,Val,Test Scores [20279.5703125, 2990.916259765625, 5932.9033203125]\n",
      "Epoch 199 Loss 27.5788 Current Train,Val,Test Scores [19964.951171875, 2939.35205078125, 5842.6796875]\n",
      "Best Train,Val,Test Scores [17534.763671875, 2584.359375, 5064.34765625]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creates a HeteroGNN model from the previously constructed hetero graph and trains it.\n",
    "\"\"\"\n",
    "\n",
    "best_model = None\n",
    "best_tvt_scores = (float(\"inf\"),float(\"inf\"),float(\"inf\"))\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    # Train\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    # Test for the accuracy of the model\n",
    "    cur_tvt_scores, best_tvt_scores, best_model = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_tvt_scores)\n",
    "    print(f\"Epoch {epoch} Loss {loss:.4f} Current Train,Val,Test Scores {[score.item() for score in cur_tvt_scores]}\")\n",
    "\n",
    "# Get the accuracy of the best model\n",
    "_, best_tvt_scores, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_tvt_scores)\n",
    "\n",
    "print(\"Best Train,Val,Test Scores\", [score.item() for score in best_tvt_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17.8588], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([16.0365], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([26.9606], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([13.8221], device='cuda:0', grad_fn=<SelectBackward0>) tensor([12.], device='cuda:0')\n",
      "tensor([12.1186], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([24.7092], device='cuda:0', grad_fn=<SelectBackward0>) tensor([56.], device='cuda:0')\n",
      "tensor([16.0639], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([13.7088], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([23.0197], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([6.5728], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([6.5440], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.5843], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([6.5923], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([6.6183], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([6.5992], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([6.5641], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([6.5986], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([6.4209], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([33.9187], device='cuda:0', grad_fn=<SelectBackward0>) tensor([94.], device='cuda:0')\n",
      "tensor([12.1392], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([6.5936], device='cuda:0', grad_fn=<SelectBackward0>) tensor([11.], device='cuda:0')\n",
      "tensor([6.5776], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([6.5995], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([29.6449], device='cuda:0', grad_fn=<SelectBackward0>) tensor([48.], device='cuda:0')\n",
      "tensor([6.5775], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([6.5395], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.5870], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([6.5606], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([13.7975], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([47.6776], device='cuda:0', grad_fn=<SelectBackward0>) tensor([111.], device='cuda:0')\n",
      "tensor([13.8279], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([13.8295], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.5590], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([11.7875], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([19.5343], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([6.5497], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([14.2590], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([14.5746], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([20.8913], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([13.2148], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.5632], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([22.4426], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([6.5947], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([15.3722], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([12.2065], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([27.4745], device='cuda:0', grad_fn=<SelectBackward0>) tensor([58.], device='cuda:0')\n",
      "tensor([15.9938], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([10.9931], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([14.6771], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([17.0637], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([16.1168], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([11.4611], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([17.1025], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([13.2086], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([12.2763], device='cuda:0', grad_fn=<SelectBackward0>) tensor([14.], device='cuda:0')\n",
      "tensor([15.2117], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([6.6454], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([6.5237], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([12.2632], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([24.1284], device='cuda:0', grad_fn=<SelectBackward0>) tensor([32.], device='cuda:0')\n",
      "tensor([6.5659], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([22.2264], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([20.7855], device='cuda:0', grad_fn=<SelectBackward0>) tensor([19.], device='cuda:0')\n",
      "tensor([18.4725], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([71.8210], device='cuda:0', grad_fn=<SelectBackward0>) tensor([144.], device='cuda:0')\n",
      "tensor([34.2808], device='cuda:0', grad_fn=<SelectBackward0>) tensor([79.], device='cuda:0')\n",
      "tensor([15.5641], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([13.1774], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.5626], device='cuda:0', grad_fn=<SelectBackward0>) tensor([12.], device='cuda:0')\n",
      "tensor([14.3296], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([12.1299], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([14.2449], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([12.3602], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([12.1799], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([6.5770], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([12.2454], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([24.9442], device='cuda:0', grad_fn=<SelectBackward0>) tensor([51.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "preds = best_model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "# mask = preds['event'] > 0\n",
    "# preds['event'][preds['event'] > 0].shape\n",
    "\n",
    "# print(preds['event'][0], hetero_graph.node_target['event'][0]) \n",
    "\n",
    "\n",
    "#print(hetero_graph.node_feature['event'])\n",
    "\n",
    "# for i in range(3000):\n",
    "#     if hetero_graph.node_target['event'][i] != -1: # concepts have node target -1\n",
    "#         print(preds['event'][i], hetero_graph.node_target['event'][i])\n",
    "        \n",
    "    \n",
    "for i in range(1000):    \n",
    "    if hetero_graph.node_target['event'][test_idx['event']][i] != -1:\n",
    "        print(preds['event'][test_idx['event']][i], hetero_graph.node_target['event'][test_idx['event']][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
