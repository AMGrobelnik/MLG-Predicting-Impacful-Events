{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # To simplify implementation, please initialize both self.lin_dst\n",
    "        # and self.lin_src out_features to out_channels\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "        \n",
    "        # Old code from colab, didn't work:\n",
    "        # self.lin_dst = nn.Linear(self.in_channels_src, self.out_channels)\n",
    "        # self.lin_src = nn.Linear(self.in_channels_dst, self.out_channels)\n",
    "        # self.lin_update = nn.Linear(2*self.out_channels, self.out_channels)\n",
    "        \n",
    "        self.lin_dst = nn.Linear(in_channels_dst, out_channels)\n",
    "        self.lin_src = nn.Linear(in_channels_src, out_channels)\n",
    "        self.lin_update = nn.Linear(2 * out_channels, out_channels)\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "    ):\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "        ##########################################\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "        ## Note:\n",
    "        ## 1. Different from what we implemented in Colab 3, we use message_and_aggregate\n",
    "        ## to replace the message and aggregate. The benefit is that we can avoid\n",
    "        ## materializing x_i and x_j, and make the implementation more efficient.\n",
    "        ## 2. To implement efficiently, following PyG documentation is helpful:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html\n",
    "        ## 3. Here edge_index is torch_sparse SparseTensor.\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "        ##########################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~4 lines of code)\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        # print(aggr_out.shape, dst_out.shape)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        # print(aggr_out.shape, )\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "        ##########################################\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "            ############# Your code here #############\n",
    "            ## (~1 line of code)\n",
    "            ## Note:\n",
    "            ## 1. Initialize self.attn_proj here.\n",
    "            ## 2. You should use nn.Sequential for self.attn_proj\n",
    "            ## 3. nn.Linear and nn.Tanh are useful.\n",
    "            ## 4. You can create a vector parameter by using:\n",
    "            ## nn.Linear(some_size, 1, bias=False)\n",
    "            ## 5. The first linear layer should have out_features as args['attn_size']\n",
    "            ## 6. You can assume we only have one \"head\" for the attention.\n",
    "            ## 7. We recommend you to implement the mean aggregation first. After \n",
    "            ## the mean aggregation works well in the training, then you can \n",
    "            ## implement this part.\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "            #########################################\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        # TODO: Implement this function that aggregates all message type results.\n",
    "        # Here, xs is a list of tensors (embeddings) with respect to message \n",
    "        # type aggregation results.\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~2 lines of code)\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "            ##########################################\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~10 lines of code)\n",
    "            ## Note:\n",
    "            ## 1. Store the value of attention alpha (as a numpy array) to self.alpha,\n",
    "            ## which has the shape (len(xs), ) self.alpha will be not be used \n",
    "            ## to backpropagate etc. in the model. We will use it to see how much \n",
    "            ## attention the layer pays on different message types.\n",
    "            ## 2. torch.softmax and torch.cat are useful.\n",
    "            ## 3. You might need to reshape the tensors by using the \n",
    "            ## `view()` function https://pytorch.org/docs/stable/tensor_view.html\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out\n",
    "            ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    # TODO: Implement this function that returns a dictionary of `HeteroGNNConv` \n",
    "    # layers where the keys are message types. `hetero_graph` is deepsnap `HeteroGraph`\n",
    "    # object and the `conv` is the `HeteroGNNConv`.\n",
    "\n",
    "    convs = {}\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## (~9 lines of code)\n",
    "\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    ##########################################\n",
    "    \n",
    "    return convs\n",
    "\n",
    "# def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "#     convs = {}\n",
    "\n",
    "#     # Retrieve the number of features for all node types\n",
    "#     node_feature_dims = {\n",
    "#         node_type: hetero_graph.num_node_features(node_type)\n",
    "#         for node_type in hetero_graph.node_types\n",
    "#     }\n",
    "\n",
    "#     for message_type in hetero_graph.message_types:\n",
    "#         src_type, _, dst_type = message_type\n",
    "#         if first_layer:\n",
    "#             # Initialize input channels based on the number of features for each node type\n",
    "#             in_channels_src = node_feature_dims[src_type]\n",
    "#             in_channels_dst = node_feature_dims[dst_type]\n",
    "#         else:\n",
    "#             # For subsequent layers, use the hidden_size as the dimension\n",
    "#             in_channels_src = hidden_size\n",
    "#             in_channels_dst = hidden_size\n",
    "#         out_channels = hidden_size\n",
    "#         convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    \n",
    "#     return convs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, num_layers, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        # self.convs1 = None\n",
    "        # self.convs2 = None\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.convs = None\n",
    "        self.bns = [nn.ModuleDict() for _ in range(num_layers)]\n",
    "        self.relus = [nn.ModuleDict() for _ in range(num_layers)]\n",
    "        \n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~10 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For self.convs1 and self.convs2, call generate_convs at first and then\n",
    "        ## pass the returned dictionary of `HeteroGNNConv` to `HeteroGNNWrapperConv`.\n",
    "        ## 2. For self.bns, self.relus and self.post_mps, the keys are node_types.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.node_types` will be helpful.\n",
    "        ## 3. Initialize all batchnorms to torch.nn.BatchNorm1d(hidden_size, eps=1.0).\n",
    "        ## 4. Initialize all relus to nn.LeakyReLU().\n",
    "        ## 5. For self.post_mps, each value in the ModuleDict is a linear layer \n",
    "        ## where the `out_features` is the number of classes for that node type.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.num_node_labels(node_type)` will be\n",
    "        ## useful.\n",
    "\n",
    "        # self.convs = [HeteroGNNWrapperConv(generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), args, self.aggr),\n",
    "        #     *[HeteroGNNWrapperConv(generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), args, self.aggr)\n",
    "        #         for _ in range(self.num_layers - 1)]\n",
    "        # ]\n",
    "        \n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            # for i in range(num_layers):\n",
    "            #     self.bns[i][node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            #     self.relus[i][node_type] = nn.LeakyReLU()\n",
    "            \n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            \n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            #self.post_mps[node_type] = nn.Linear(self.hidden_size, hetero_graph.num_node_labels(node_type))\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "            \n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(self, node_feature, edge_index):\n",
    "        # TODO: Implement the forward function. Notice that `node_feature` is \n",
    "        # a dictionary of tensors where keys are node types and values are \n",
    "        # corresponding feature tensors. The `edge_index` is a dictionary of \n",
    "        # tensors where keys are message types and values are corresponding\n",
    "        # edge index tensors (with respect to each message type).\n",
    "        \n",
    "        \n",
    "        x = node_feature\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~7 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. `deepsnap.hetero_gnn.forward_op` can be helpful.\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        # for conv, bn, relu in zip(self.convs, self.bns, self.relus):\n",
    "        #     x = conv(x, edge_index)\n",
    "        #     x = forward_op(x, bn)\n",
    "        #     x = forward_op(x, relu)\n",
    "        \n",
    "        x = forward_op(x, self.fc)\n",
    "        #print(\"X\", x)\n",
    "\n",
    "        ##########################################\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        loss = 0\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        def mape_loss(y_pred, y_true):\n",
    "            \"\"\"\n",
    "            Calculate the Mean Absolute Percentage Error (MAPE) for PyTorch tensors.\n",
    "\n",
    "            Args:\n",
    "            y_true (Tensor): True values.\n",
    "            y_pred (Tensor): Predicted values.\n",
    "\n",
    "            Returns:\n",
    "            Tensor: MAPE loss.\n",
    "            \"\"\"\n",
    "            # Ensuring y_true and y_pred are on the same device\n",
    "            #y_pred = y_pred.to(y_true.device)\n",
    "\n",
    "            # Avoiding division by zero\n",
    "            non_zero_mask = y_true != 0\n",
    "\n",
    "            # Calculating MAPE\n",
    "            loss = torch.mean(torch.abs((torch.masked_select(y_true, non_zero_mask) - torch.masked_select(y_pred, non_zero_mask) / torch.masked_select(y_true, non_zero_mask)))) * 100\n",
    "    \n",
    "            return loss\n",
    "\n",
    "\n",
    "        #loss_func = mape_loss\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For each node type in preds, accumulate computed loss to `loss`\n",
    "        ## 2. Loss need to be computed with respect to the given \n",
    "        \n",
    "        # print(\"Preds\", preds)\n",
    "        # print(\"Y\", y)\n",
    "        # print(\"Indices\", indices)\n",
    "         \n",
    "        mask = y['event'][indices['event'], 0] != -1\n",
    "        non_zero_idx = torch.masked_select(indices['event'], mask)\n",
    "                \n",
    "        loss += loss_func(preds['event'][non_zero_idx], y['event'][non_zero_idx, 0])\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    loss = None\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## Note:\n",
    "    ## 1. `deepsnap.hetero_graph.HeteroGraph.node_label` is useful\n",
    "    ## 2. Compute the loss here\n",
    "    \n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "    ##########################################\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(model, graph, indices, best_model=None, best_val=0):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        \n",
    "        #print(\"Index\", index)\n",
    "        #print(\"Preds\", preds['event'])\n",
    "\n",
    "        idx = index['event']\n",
    "\n",
    "        L1 = torch.sum(torch.abs(preds['event'][idx] - graph.node_target['event'][idx]))\n",
    "        \n",
    "        accs.append(L1)\n",
    "        #print(\"ACC\", s)\n",
    "\n",
    "        #pred = preds['event'][idx]\n",
    "        \n",
    "        # num_node_types = 0\n",
    "        # micro = 0\n",
    "        # macro = 0\n",
    "\n",
    "        # for node_type in preds:\n",
    "        #     idx = index[node_type]\n",
    "        #     pred = preds[node_type][idx]\n",
    "        #     pred = pred.max(1)[1]\n",
    "        #     label_np = graph.node_label[node_type][idx].cpu().numpy()\n",
    "        #     pred_np = pred.cpu().numpy()\n",
    "        #     micro = f1_score(label_np, pred_np, average='micro')\n",
    "        #     macro = f1_score(label_np, pred_np, average='macro')\n",
    "        #     num_node_types += 1\n",
    "        # Averaging f1 score might not make sense, but in our example we only\n",
    "        # have one node type\n",
    "        # micro /= num_node_types\n",
    "        # macro /= num_node_types\n",
    "        #accs.append((micro, macro))\n",
    "    if accs[1] < best_val:\n",
    "        best_val = accs[1]\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    return accs, best_model, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change the following parameters\n",
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 64,\n",
    "    'epochs': 200,\n",
    "    'weight_decay': 1e-4,\n",
    "    'lr': 0.201,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [1, 1, 1],   # event 0\n",
    "                [2, 2, 2]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([[50], [2000]], dtype=torch.float32),\n",
    "    # \"concept\": torch.tensor([[0], [0]], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}\n",
    "\n",
    "# Testing\n",
    "hetero_graph = HeteroGraph(\n",
    "    node_feature=S_node_feature,\n",
    "    node_target=S_node_targets,\n",
    "    edge_index=S_edge_index\n",
    ")\n",
    "\n",
    "train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./1_concepts_similar_llm.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "    # Convert to directed graph for compatibility with Deepsnap\n",
    "    G = G.to_directed()\n",
    "    \n",
    "\n",
    "hetero_graph = HeteroGraph(G, netlib=nx, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 770\n",
      "\t Feature 1\n",
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 770\n",
      "\t Feature 770\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 1\n",
      "\t Feature 770\n",
      "KEY ('event', 'related', 'concept') <class 'tuple'>\n",
      "KEY NUMS ('event', 'related', 'concept') 8487 8729\n",
      "MAX EDGES tensor(8448) tensor(8728) 8487 8729\n",
      "KEY ('event', 'similar', 'event') <class 'tuple'>\n",
      "KEY NUMS ('event', 'similar', 'event') 8487 8487\n",
      "MAX EDGES tensor(8486) tensor(8486) 8487 8487\n",
      "KEY ('concept', 'related', 'event') <class 'tuple'>\n",
      "KEY NUMS ('concept', 'related', 'event') 8729 8487\n",
      "MAX EDGES tensor(8728) tensor(8448) 8729 8487\n"
     ]
    }
   ],
   "source": [
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "\n",
    "# Node feature and node label to device\n",
    "\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "\n",
    "# edge_index1 = hetero_graph.edge_index[(\"concept\", \"related\", \"event\")]\n",
    "# edge_index1 = hetero_graph.edge_index[(\"event\", \"related\", \"concept\")]\n",
    "\n",
    "# Edge_index to sparse tensor and to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key, type(key))\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    \n",
    "    \n",
    "    # TODO: remove quick fix\n",
    "    # if key == ('event', 'related', 'concept'):\n",
    "    #     edge_index = hetero_graph.edge_index[(\"concept\", \"related\", \"event\")]\n",
    "    # else:\n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max(), hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
    "    \n",
    "\n",
    "\n",
    "for key in hetero_graph.node_target:\n",
    "    hetero_graph.node_target[key] = hetero_graph.node_target[key].to(args['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5940])\n",
      "torch.Size([1698])\n",
      "torch.Size([849])\n"
     ]
    }
   ],
   "source": [
    "nEvents = hetero_graph.num_nodes(\"event\")\n",
    "nConcepts = hetero_graph.num_nodes(\"concept\")\n",
    "\n",
    "s1 = 0.7\n",
    "s2 = 0.8\n",
    "\n",
    "train_idx = {   \"event\": torch.tensor(range(0, int(nEvents * s1))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(0, int(nConcepts * s1))).to(args[\"device\"])\n",
    "            }\n",
    "val_idx = {   \"event\": torch.tensor(range(int(nEvents * s1), int(nEvents * s2))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s1), int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "test_idx = {   \"event\": torch.tensor(range(int(nEvents * s2), nEvents)).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s2), nConcepts)).to(args[\"device\"])\n",
    "            }\n",
    "\n",
    "print(train_idx[\"event\"].shape)\n",
    "print(test_idx[\"event\"].shape)\n",
    "print(val_idx[\"event\"].shape)\n",
    "\n",
    "# dataset = deepsnap.dataset.GraphDataset([hetero_graph], task='node')\n",
    "\n",
    "# dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
    "# datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([421])) that is different to the input size (torch.Size([421, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 1978.026123046875 Accs [tensor(1.1290e+08, device='cuda:0', grad_fn=<SumBackward0>), tensor(16280198., device='cuda:0', grad_fn=<SumBackward0>), tensor(31922418., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 1 Loss 2220.028076171875 Accs [tensor(1.8205e+08, device='cuda:0', grad_fn=<SumBackward0>), tensor(26175188., device='cuda:0', grad_fn=<SumBackward0>), tensor(51618576., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 2 Loss 1887.7928466796875 Accs [tensor(25772072., device='cuda:0', grad_fn=<SumBackward0>), tensor(3699837.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(7308452.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 3 Loss 1895.126953125 Accs [tensor(6086276., device='cuda:0', grad_fn=<SumBackward0>), tensor(872276.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(1721313.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 4 Loss 1834.5260009765625 Accs [tensor(3149292.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(450755.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(886367.4375, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 5 Loss 1760.876220703125 Accs [tensor(2100242.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(300037.0938, device='cuda:0', grad_fn=<SumBackward0>), tensor(586770., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 6 Loss 1748.3070068359375 Accs [tensor(1517759.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(216649.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(422352.5625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 7 Loss 1762.810302734375 Accs [tensor(1082594.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(154768.0938, device='cuda:0', grad_fn=<SumBackward0>), tensor(302577.5312, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 8 Loss 1745.6136474609375 Accs [tensor(690909.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(98727.5859, device='cuda:0', grad_fn=<SumBackward0>), tensor(193642.2812, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 9 Loss 1722.484375 Accs [tensor(486307.8125, device='cuda:0', grad_fn=<SumBackward0>), tensor(69460., device='cuda:0', grad_fn=<SumBackward0>), tensor(136638.1562, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 10 Loss 1723.37890625 Accs [tensor(391394.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(55879.3711, device='cuda:0', grad_fn=<SumBackward0>), tensor(110313.9141, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 11 Loss 1734.9935302734375 Accs [tensor(356479.3125, device='cuda:0', grad_fn=<SumBackward0>), tensor(50867.4180, device='cuda:0', grad_fn=<SumBackward0>), tensor(100772.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 12 Loss 1730.5772705078125 Accs [tensor(344402.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(49137.4844, device='cuda:0', grad_fn=<SumBackward0>), tensor(97380.1562, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 13 Loss 1722.019287109375 Accs [tensor(327984.7812, device='cuda:0', grad_fn=<SumBackward0>), tensor(46736.1641, device='cuda:0', grad_fn=<SumBackward0>), tensor(92647.0703, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 14 Loss 1719.91162109375 Accs [tensor(299069.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(42561.9727, device='cuda:0', grad_fn=<SumBackward0>), tensor(84428.5938, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 15 Loss 1721.446533203125 Accs [tensor(259574.1406, device='cuda:0', grad_fn=<SumBackward0>), tensor(36964.6641, device='cuda:0', grad_fn=<SumBackward0>), tensor(73327.2969, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 16 Loss 1724.7119140625 Accs [tensor(218884.8594, device='cuda:0', grad_fn=<SumBackward0>), tensor(31326.1992, device='cuda:0', grad_fn=<SumBackward0>), tensor(61958.3438, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 17 Loss 1723.970458984375 Accs [tensor(181800.2188, device='cuda:0', grad_fn=<SumBackward0>), tensor(26180.1719, device='cuda:0', grad_fn=<SumBackward0>), tensor(51578.2891, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 18 Loss 1718.685302734375 Accs [tensor(151156.5938, device='cuda:0', grad_fn=<SumBackward0>), tensor(21898.3047, device='cuda:0', grad_fn=<SumBackward0>), tensor(42981.0781, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 19 Loss 1715.917724609375 Accs [tensor(126306.9219, device='cuda:0', grad_fn=<SumBackward0>), tensor(18384.1211, device='cuda:0', grad_fn=<SumBackward0>), tensor(35960., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 20 Loss 1718.3056640625 Accs [tensor(106222.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(15539.7734, device='cuda:0', grad_fn=<SumBackward0>), tensor(30244.7129, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 21 Loss 1720.4954833984375 Accs [tensor(90471.6719, device='cuda:0', grad_fn=<SumBackward0>), tensor(13307.2461, device='cuda:0', grad_fn=<SumBackward0>), tensor(25759.0391, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 22 Loss 1718.9239501953125 Accs [tensor(77730.4219, device='cuda:0', grad_fn=<SumBackward0>), tensor(11490.9834, device='cuda:0', grad_fn=<SumBackward0>), tensor(22124.2578, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 23 Loss 1716.5780029296875 Accs [tensor(67333.2344, device='cuda:0', grad_fn=<SumBackward0>), tensor(10002.2324, device='cuda:0', grad_fn=<SumBackward0>), tensor(19160.2383, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 24 Loss 1716.219970703125 Accs [tensor(58443.5938, device='cuda:0', grad_fn=<SumBackward0>), tensor(8721.2461, device='cuda:0', grad_fn=<SumBackward0>), tensor(16627.2188, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 25 Loss 1717.0660400390625 Accs [tensor(50158.8359, device='cuda:0', grad_fn=<SumBackward0>), tensor(7523.8086, device='cuda:0', grad_fn=<SumBackward0>), tensor(14262.2031, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 26 Loss 1717.388671875 Accs [tensor(41976.3398, device='cuda:0', grad_fn=<SumBackward0>), tensor(6343.1255, device='cuda:0', grad_fn=<SumBackward0>), tensor(11930.0967, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 27 Loss 1716.8653564453125 Accs [tensor(33938.9336, device='cuda:0', grad_fn=<SumBackward0>), tensor(5191.1299, device='cuda:0', grad_fn=<SumBackward0>), tensor(9641.4990, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 28 Loss 1716.066162109375 Accs [tensor(26451.2559, device='cuda:0', grad_fn=<SumBackward0>), tensor(4120.5537, device='cuda:0', grad_fn=<SumBackward0>), tensor(7529.0654, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 29 Loss 1715.62939453125 Accs [tensor(19814.4199, device='cuda:0', grad_fn=<SumBackward0>), tensor(3204.7437, device='cuda:0', grad_fn=<SumBackward0>), tensor(5683.6592, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 30 Loss 1716.0277099609375 Accs [tensor(19618.6406, device='cuda:0', grad_fn=<SumBackward0>), tensor(3193.3860, device='cuda:0', grad_fn=<SumBackward0>), tensor(5653.8691, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 31 Loss 1716.6629638671875 Accs [tensor(23724.3066, device='cuda:0', grad_fn=<SumBackward0>), tensor(3790.8516, device='cuda:0', grad_fn=<SumBackward0>), tensor(6827.6460, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 32 Loss 1716.145263671875 Accs [tensor(28640.7617, device='cuda:0', grad_fn=<SumBackward0>), tensor(4499.3506, device='cuda:0', grad_fn=<SumBackward0>), tensor(8224.1406, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 33 Loss 1715.397216796875 Accs [tensor(33431.3438, device='cuda:0', grad_fn=<SumBackward0>), tensor(5188.5879, device='cuda:0', grad_fn=<SumBackward0>), tensor(9579.6602, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 34 Loss 1715.512939453125 Accs [tensor(37251.5703, device='cuda:0', grad_fn=<SumBackward0>), tensor(5743.8525, device='cuda:0', grad_fn=<SumBackward0>), tensor(10661.7988, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 35 Loss 1715.8572998046875 Accs [tensor(39896.9453, device='cuda:0', grad_fn=<SumBackward0>), tensor(6135.5215, device='cuda:0', grad_fn=<SumBackward0>), tensor(11417.0703, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 36 Loss 1715.7747802734375 Accs [tensor(41412.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(6362.7158, device='cuda:0', grad_fn=<SumBackward0>), tensor(11853.5703, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 37 Loss 1715.5155029296875 Accs [tensor(42682.7969, device='cuda:0', grad_fn=<SumBackward0>), tensor(6547.1582, device='cuda:0', grad_fn=<SumBackward0>), tensor(12215.4541, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 38 Loss 1715.4229736328125 Accs [tensor(43957.5312, device='cuda:0', grad_fn=<SumBackward0>), tensor(6720.9736, device='cuda:0', grad_fn=<SumBackward0>), tensor(12584.3281, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 39 Loss 1715.4261474609375 Accs [tensor(45611.7188, device='cuda:0', grad_fn=<SumBackward0>), tensor(6950.0352, device='cuda:0', grad_fn=<SumBackward0>), tensor(13060.5039, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 40 Loss 1715.512451171875 Accs [tensor(47726.4531, device='cuda:0', grad_fn=<SumBackward0>), tensor(7249.5820, device='cuda:0', grad_fn=<SumBackward0>), tensor(13659.3359, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 41 Loss 1715.5615234375 Accs [tensor(49883.2148, device='cuda:0', grad_fn=<SumBackward0>), tensor(7563.2305, device='cuda:0', grad_fn=<SumBackward0>), tensor(14271.7871, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 42 Loss 1715.35791015625 Accs [tensor(51660.1953, device='cuda:0', grad_fn=<SumBackward0>), tensor(7828.9668, device='cuda:0', grad_fn=<SumBackward0>), tensor(14772.9746, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 43 Loss 1715.2144775390625 Accs [tensor(52952.6094, device='cuda:0', grad_fn=<SumBackward0>), tensor(8022.8901, device='cuda:0', grad_fn=<SumBackward0>), tensor(15136.7129, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 44 Loss 1715.359619140625 Accs [tensor(53784.7422, device='cuda:0', grad_fn=<SumBackward0>), tensor(8141.4023, device='cuda:0', grad_fn=<SumBackward0>), tensor(15372.6631, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 45 Loss 1715.447998046875 Accs [tensor(54293.6016, device='cuda:0', grad_fn=<SumBackward0>), tensor(8208.7715, device='cuda:0', grad_fn=<SumBackward0>), tensor(15523.5762, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 46 Loss 1715.28955078125 Accs [tensor(54547.7852, device='cuda:0', grad_fn=<SumBackward0>), tensor(8236.4844, device='cuda:0', grad_fn=<SumBackward0>), tensor(15600.9668, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 47 Loss 1715.1834716796875 Accs [tensor(54564.8086, device='cuda:0', grad_fn=<SumBackward0>), tensor(8229.4258, device='cuda:0', grad_fn=<SumBackward0>), tensor(15614.6670, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 48 Loss 1715.258056640625 Accs [tensor(54477.1797, device='cuda:0', grad_fn=<SumBackward0>), tensor(8213.7891, device='cuda:0', grad_fn=<SumBackward0>), tensor(15600.8516, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 49 Loss 1715.298583984375 Accs [tensor(54504.4727, device='cuda:0', grad_fn=<SumBackward0>), tensor(8220.1348, device='cuda:0', grad_fn=<SumBackward0>), tensor(15613.8184, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 50 Loss 1715.25146484375 Accs [tensor(54834.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(8271.8691, device='cuda:0', grad_fn=<SumBackward0>), tensor(15706.6699, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 51 Loss 1715.1981201171875 Accs [tensor(55444.6367, device='cuda:0', grad_fn=<SumBackward0>), tensor(8362.6641, device='cuda:0', grad_fn=<SumBackward0>), tensor(15875.9688, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 52 Loss 1715.17333984375 Accs [tensor(56053.2422, device='cuda:0', grad_fn=<SumBackward0>), tensor(8450.4590, device='cuda:0', grad_fn=<SumBackward0>), tensor(16046.0684, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 53 Loss 1715.2152099609375 Accs [tensor(56315.5742, device='cuda:0', grad_fn=<SumBackward0>), tensor(8486.1533, device='cuda:0', grad_fn=<SumBackward0>), tensor(16123.2754, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 54 Loss 1715.2374267578125 Accs [tensor(56113.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(8453.7939, device='cuda:0', grad_fn=<SumBackward0>), tensor(16074.0586, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 55 Loss 1715.1651611328125 Accs [tensor(55669.0781, device='cuda:0', grad_fn=<SumBackward0>), tensor(8385.9102, device='cuda:0', grad_fn=<SumBackward0>), tensor(15958.6797, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 56 Loss 1715.1337890625 Accs [tensor(55407.1172, device='cuda:0', grad_fn=<SumBackward0>), tensor(8344.2314, device='cuda:0', grad_fn=<SumBackward0>), tensor(15894.0977, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 57 Loss 1715.1866455078125 Accs [tensor(55557.4961, device='cuda:0', grad_fn=<SumBackward0>), tensor(8363.7764, device='cuda:0', grad_fn=<SumBackward0>), tensor(15941.8057, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 58 Loss 1715.18310546875 Accs [tensor(55990.1562, device='cuda:0', grad_fn=<SumBackward0>), tensor(8426.3213, device='cuda:0', grad_fn=<SumBackward0>), tensor(16063.3984, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 59 Loss 1715.1390380859375 Accs [tensor(56368.7344, device='cuda:0', grad_fn=<SumBackward0>), tensor(8482.8398, device='cuda:0', grad_fn=<SumBackward0>), tensor(16168.9209, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 60 Loss 1715.13330078125 Accs [tensor(56456.0039, device='cuda:0', grad_fn=<SumBackward0>), tensor(8498.1699, device='cuda:0', grad_fn=<SumBackward0>), tensor(16194.1455, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 61 Loss 1715.140869140625 Accs [tensor(56295.5469, device='cuda:0', grad_fn=<SumBackward0>), tensor(8476.5469, device='cuda:0', grad_fn=<SumBackward0>), tensor(16152.7354, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 62 Loss 1715.14697265625 Accs [tensor(56126.8594, device='cuda:0', grad_fn=<SumBackward0>), tensor(8450.5332, device='cuda:0', grad_fn=<SumBackward0>), tensor(16110.8027, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 63 Loss 1715.1341552734375 Accs [tensor(56119.9844, device='cuda:0', grad_fn=<SumBackward0>), tensor(8445.4580, device='cuda:0', grad_fn=<SumBackward0>), tensor(16114.8711, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 64 Loss 1715.106201171875 Accs [tensor(56223.4219, device='cuda:0', grad_fn=<SumBackward0>), tensor(8456.0381, device='cuda:0', grad_fn=<SumBackward0>), tensor(16149.0898, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 65 Loss 1715.1170654296875 Accs [tensor(56269.0508, device='cuda:0', grad_fn=<SumBackward0>), tensor(8460.3477, device='cuda:0', grad_fn=<SumBackward0>), tensor(16165.4043, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 66 Loss 1715.1300048828125 Accs [tensor(56193.0703, device='cuda:0', grad_fn=<SumBackward0>), tensor(8449.9492, device='cuda:0', grad_fn=<SumBackward0>), tensor(16145.9199, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 67 Loss 1715.1048583984375 Accs [tensor(56107.3008, device='cuda:0', grad_fn=<SumBackward0>), tensor(8439.3301, device='cuda:0', grad_fn=<SumBackward0>), tensor(16122.4180, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 68 Loss 1715.0960693359375 Accs [tensor(56140.9883, device='cuda:0', grad_fn=<SumBackward0>), tensor(8444.9365, device='cuda:0', grad_fn=<SumBackward0>), tensor(16132.3535, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 69 Loss 1715.1036376953125 Accs [tensor(56261.4844, device='cuda:0', grad_fn=<SumBackward0>), tensor(8460.8828, device='cuda:0', grad_fn=<SumBackward0>), tensor(16167.8018, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 70 Loss 1715.101806640625 Accs [tensor(56305.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(8463.8428, device='cuda:0', grad_fn=<SumBackward0>), tensor(16182.9629, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 71 Loss 1715.0941162109375 Accs [tensor(56179.2031, device='cuda:0', grad_fn=<SumBackward0>), tensor(8441.6885, device='cuda:0', grad_fn=<SumBackward0>), tensor(16150.7842, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 72 Loss 1715.0821533203125 Accs [tensor(55983.2969, device='cuda:0', grad_fn=<SumBackward0>), tensor(8412.0020, device='cuda:0', grad_fn=<SumBackward0>), tensor(16098.3359, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 73 Loss 1715.0863037109375 Accs [tensor(55911.3828, device='cuda:0', grad_fn=<SumBackward0>), tensor(8400.7109, device='cuda:0', grad_fn=<SumBackward0>), tensor(16079.5527, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 74 Loss 1715.0899658203125 Accs [tensor(56005.7227, device='cuda:0', grad_fn=<SumBackward0>), tensor(8413.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(16107.2051, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 75 Loss 1715.076171875 Accs [tensor(56109.1172, device='cuda:0', grad_fn=<SumBackward0>), tensor(8428.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(16136.9902, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 76 Loss 1715.0750732421875 Accs [tensor(56068.4570, device='cuda:0', grad_fn=<SumBackward0>), tensor(8422.6367, device='cuda:0', grad_fn=<SumBackward0>), tensor(16126.3789, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 77 Loss 1715.0772705078125 Accs [tensor(55920.0703, device='cuda:0', grad_fn=<SumBackward0>), tensor(8400.3057, device='cuda:0', grad_fn=<SumBackward0>), tensor(16086.1045, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 78 Loss 1715.0743408203125 Accs [tensor(55824.4805, device='cuda:0', grad_fn=<SumBackward0>), tensor(8384.1895, device='cuda:0', grad_fn=<SumBackward0>), tensor(16060.8975, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 79 Loss 1715.070068359375 Accs [tensor(55849.1641, device='cuda:0', grad_fn=<SumBackward0>), tensor(8384.3691, device='cuda:0', grad_fn=<SumBackward0>), tensor(16069.5508, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 80 Loss 1715.0650634765625 Accs [tensor(55889.1836, device='cuda:0', grad_fn=<SumBackward0>), tensor(8387.2354, device='cuda:0', grad_fn=<SumBackward0>), tensor(16082.1777, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 81 Loss 1715.06884765625 Accs [tensor(55828.8242, device='cuda:0', grad_fn=<SumBackward0>), tensor(8377.0488, device='cuda:0', grad_fn=<SumBackward0>), tensor(16065.5127, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 82 Loss 1715.064697265625 Accs [tensor(55703.5898, device='cuda:0', grad_fn=<SumBackward0>), tensor(8358.3350, device='cuda:0', grad_fn=<SumBackward0>), tensor(16029.3359, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 83 Loss 1715.0594482421875 Accs [tensor(55635.1289, device='cuda:0', grad_fn=<SumBackward0>), tensor(8347.4043, device='cuda:0', grad_fn=<SumBackward0>), tensor(16009.0293, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 84 Loss 1715.0599365234375 Accs [tensor(55650.6445, device='cuda:0', grad_fn=<SumBackward0>), tensor(8347.4180, device='cuda:0', grad_fn=<SumBackward0>), tensor(16013.1074, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 85 Loss 1715.058349609375 Accs [tensor(55654.6719, device='cuda:0', grad_fn=<SumBackward0>), tensor(8344.6426, device='cuda:0', grad_fn=<SumBackward0>), tensor(16013.4951, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 86 Loss 1715.056396484375 Accs [tensor(55578.7461, device='cuda:0', grad_fn=<SumBackward0>), tensor(8329.9277, device='cuda:0', grad_fn=<SumBackward0>), tensor(15990.2676, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 87 Loss 1715.0518798828125 Accs [tensor(55480.7383, device='cuda:0', grad_fn=<SumBackward0>), tensor(8313.3779, device='cuda:0', grad_fn=<SumBackward0>), tensor(15960.5186, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 88 Loss 1715.0523681640625 Accs [tensor(55450.0703, device='cuda:0', grad_fn=<SumBackward0>), tensor(8307.1221, device='cuda:0', grad_fn=<SumBackward0>), tensor(15949.9336, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 89 Loss 1715.0509033203125 Accs [tensor(55465.1562, device='cuda:0', grad_fn=<SumBackward0>), tensor(8308.2881, device='cuda:0', grad_fn=<SumBackward0>), tensor(15952.8848, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 90 Loss 1715.04736328125 Accs [tensor(55436.1406, device='cuda:0', grad_fn=<SumBackward0>), tensor(8303.5840, device='cuda:0', grad_fn=<SumBackward0>), tensor(15942.6719, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 91 Loss 1715.046630859375 Accs [tensor(55349.6836, device='cuda:0', grad_fn=<SumBackward0>), tensor(8290.7129, device='cuda:0', grad_fn=<SumBackward0>), tensor(15915.2441, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 92 Loss 1715.0455322265625 Accs [tensor(55285.8984, device='cuda:0', grad_fn=<SumBackward0>), tensor(8280.4219, device='cuda:0', grad_fn=<SumBackward0>), tensor(15894.3340, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 93 Loss 1715.044189453125 Accs [tensor(55289.4570, device='cuda:0', grad_fn=<SumBackward0>), tensor(8279.1562, device='cuda:0', grad_fn=<SumBackward0>), tensor(15894.1738, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 94 Loss 1715.0411376953125 Accs [tensor(55302.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(8279.2969, device='cuda:0', grad_fn=<SumBackward0>), tensor(15897.0840, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 95 Loss 1715.040771484375 Accs [tensor(55261.5234, device='cuda:0', grad_fn=<SumBackward0>), tensor(8272.5264, device='cuda:0', grad_fn=<SumBackward0>), tensor(15884.1074, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 96 Loss 1715.03955078125 Accs [tensor(55199.8281, device='cuda:0', grad_fn=<SumBackward0>), tensor(8263.3965, device='cuda:0', grad_fn=<SumBackward0>), tensor(15864.9590, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 97 Loss 1715.0379638671875 Accs [tensor(55180.5312, device='cuda:0', grad_fn=<SumBackward0>), tensor(8260.3672, device='cuda:0', grad_fn=<SumBackward0>), tensor(15858.3496, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 98 Loss 1715.0364990234375 Accs [tensor(55193.3945, device='cuda:0', grad_fn=<SumBackward0>), tensor(8261.6504, device='cuda:0', grad_fn=<SumBackward0>), tensor(15861.2480, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 99 Loss 1715.0357666015625 Accs [tensor(55182.8906, device='cuda:0', grad_fn=<SumBackward0>), tensor(8259.4590, device='cuda:0', grad_fn=<SumBackward0>), tensor(15857.4238, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 100 Loss 1715.0345458984375 Accs [tensor(55150.2930, device='cuda:0', grad_fn=<SumBackward0>), tensor(8254.1934, device='cuda:0', grad_fn=<SumBackward0>), tensor(15847.4004, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 101 Loss 1715.0328369140625 Accs [tensor(55145.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(8253.1602, device='cuda:0', grad_fn=<SumBackward0>), tensor(15845.8740, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 102 Loss 1715.0321044921875 Accs [tensor(55171.1484, device='cuda:0', grad_fn=<SumBackward0>), tensor(8256.4043, device='cuda:0', grad_fn=<SumBackward0>), tensor(15852.8906, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 103 Loss 1715.03076171875 Accs [tensor(55176.4219, device='cuda:0', grad_fn=<SumBackward0>), tensor(8257.1768, device='cuda:0', grad_fn=<SumBackward0>), tensor(15854.2529, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 104 Loss 1715.0294189453125 Accs [tensor(55153.3281, device='cuda:0', grad_fn=<SumBackward0>), tensor(8254.2051, device='cuda:0', grad_fn=<SumBackward0>), tensor(15847.5371, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 105 Loss 1715.028564453125 Accs [tensor(55149.3594, device='cuda:0', grad_fn=<SumBackward0>), tensor(8253.7285, device='cuda:0', grad_fn=<SumBackward0>), tensor(15846.2891, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 106 Loss 1715.0279541015625 Accs [tensor(55182.4570, device='cuda:0', grad_fn=<SumBackward0>), tensor(8258.0762, device='cuda:0', grad_fn=<SumBackward0>), tensor(15855.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 107 Loss 1715.026611328125 Accs [tensor(55211.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(8261.7715, device='cuda:0', grad_fn=<SumBackward0>), tensor(15864.1387, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 108 Loss 1715.025634765625 Accs [tensor(55212.7930, device='cuda:0', grad_fn=<SumBackward0>), tensor(8261.8145, device='cuda:0', grad_fn=<SumBackward0>), tensor(15864.7412, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 109 Loss 1715.02490234375 Accs [tensor(55216.8008, device='cuda:0', grad_fn=<SumBackward0>), tensor(8262.4102, device='cuda:0', grad_fn=<SumBackward0>), tensor(15865.9658, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 110 Loss 1715.0240478515625 Accs [tensor(55240.5977, device='cuda:0', grad_fn=<SumBackward0>), tensor(8265.8613, device='cuda:0', grad_fn=<SumBackward0>), tensor(15872.7578, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 111 Loss 1715.022705078125 Accs [tensor(55256.4922, device='cuda:0', grad_fn=<SumBackward0>), tensor(8268.2852, device='cuda:0', grad_fn=<SumBackward0>), tensor(15877.2305, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 112 Loss 1715.0224609375 Accs [tensor(55254.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(8268.1738, device='cuda:0', grad_fn=<SumBackward0>), tensor(15876.4883, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 113 Loss 1715.021484375 Accs [tensor(55264.9688, device='cuda:0', grad_fn=<SumBackward0>), tensor(8269.7217, device='cuda:0', grad_fn=<SumBackward0>), tensor(15879.6309, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 114 Loss 1715.0206298828125 Accs [tensor(55298.1953, device='cuda:0', grad_fn=<SumBackward0>), tensor(8274.1973, device='cuda:0', grad_fn=<SumBackward0>), tensor(15889.1816, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 115 Loss 1715.0198974609375 Accs [tensor(55320.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(8277.2305, device='cuda:0', grad_fn=<SumBackward0>), tensor(15895.4375, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 116 Loss 1715.0191650390625 Accs [tensor(55320.0508, device='cuda:0', grad_fn=<SumBackward0>), tensor(8277.4697, device='cuda:0', grad_fn=<SumBackward0>), tensor(15895.3516, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 117 Loss 1715.0184326171875 Accs [tensor(55328.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(8278.9053, device='cuda:0', grad_fn=<SumBackward0>), tensor(15897.7012, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 118 Loss 1715.0177001953125 Accs [tensor(55352.5664, device='cuda:0', grad_fn=<SumBackward0>), tensor(8282.3203, device='cuda:0', grad_fn=<SumBackward0>), tensor(15904.4600, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 119 Loss 1715.0172119140625 Accs [tensor(55367.6055, device='cuda:0', grad_fn=<SumBackward0>), tensor(8284.4756, device='cuda:0', grad_fn=<SumBackward0>), tensor(15908.7510, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 120 Loss 1715.0164794921875 Accs [tensor(55374.9570, device='cuda:0', grad_fn=<SumBackward0>), tensor(8285.5605, device='cuda:0', grad_fn=<SumBackward0>), tensor(15910.8750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 121 Loss 1715.015869140625 Accs [tensor(55397.9453, device='cuda:0', grad_fn=<SumBackward0>), tensor(8288.7412, device='cuda:0', grad_fn=<SumBackward0>), tensor(15917.4824, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 122 Loss 1715.01513671875 Accs [tensor(55425.6562, device='cuda:0', grad_fn=<SumBackward0>), tensor(8292.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(15925.4219, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 123 Loss 1715.0146484375 Accs [tensor(55435.8203, device='cuda:0', grad_fn=<SumBackward0>), tensor(8294.2539, device='cuda:0', grad_fn=<SumBackward0>), tensor(15928.2861, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 124 Loss 1715.0140380859375 Accs [tensor(55443.6641, device='cuda:0', grad_fn=<SumBackward0>), tensor(8295.5850, device='cuda:0', grad_fn=<SumBackward0>), tensor(15930.4766, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 125 Loss 1715.013427734375 Accs [tensor(55469.1328, device='cuda:0', grad_fn=<SumBackward0>), tensor(8299.2061, device='cuda:0', grad_fn=<SumBackward0>), tensor(15937.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 126 Loss 1715.0130615234375 Accs [tensor(55495.9180, device='cuda:0', grad_fn=<SumBackward0>), tensor(8302.9570, device='cuda:0', grad_fn=<SumBackward0>), tensor(15945.4346, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 127 Loss 1715.012451171875 Accs [tensor(55510.7266, device='cuda:0', grad_fn=<SumBackward0>), tensor(8305.1084, device='cuda:0', grad_fn=<SumBackward0>), tensor(15949.6963, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 128 Loss 1715.011962890625 Accs [tensor(55530.0234, device='cuda:0', grad_fn=<SumBackward0>), tensor(8307.8828, device='cuda:0', grad_fn=<SumBackward0>), tensor(15955.2324, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 129 Loss 1715.0113525390625 Accs [tensor(55556.8984, device='cuda:0', grad_fn=<SumBackward0>), tensor(8311.7041, device='cuda:0', grad_fn=<SumBackward0>), tensor(15962.9277, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 130 Loss 1715.0108642578125 Accs [tensor(55573.5859, device='cuda:0', grad_fn=<SumBackward0>), tensor(8314.1934, device='cuda:0', grad_fn=<SumBackward0>), tensor(15967.6895, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 131 Loss 1715.010498046875 Accs [tensor(55585.7109, device='cuda:0', grad_fn=<SumBackward0>), tensor(8316.0547, device='cuda:0', grad_fn=<SumBackward0>), tensor(15971.1660, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 132 Loss 1715.0101318359375 Accs [tensor(55610.7812, device='cuda:0', grad_fn=<SumBackward0>), tensor(8319.5801, device='cuda:0', grad_fn=<SumBackward0>), tensor(15978.3828, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 133 Loss 1715.0096435546875 Accs [tensor(55637., device='cuda:0', grad_fn=<SumBackward0>), tensor(8323.2324, device='cuda:0', grad_fn=<SumBackward0>), tensor(15985.9434, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 134 Loss 1715.00927734375 Accs [tensor(55650.9219, device='cuda:0', grad_fn=<SumBackward0>), tensor(8325.2666, device='cuda:0', grad_fn=<SumBackward0>), tensor(15989.9590, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 135 Loss 1715.0086669921875 Accs [tensor(55666.6328, device='cuda:0', grad_fn=<SumBackward0>), tensor(8327.5703, device='cuda:0', grad_fn=<SumBackward0>), tensor(15994.4629, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 136 Loss 1715.00830078125 Accs [tensor(55689.6406, device='cuda:0', grad_fn=<SumBackward0>), tensor(8330.8604, device='cuda:0', grad_fn=<SumBackward0>), tensor(16001.0537, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 137 Loss 1715.0079345703125 Accs [tensor(55705.0391, device='cuda:0', grad_fn=<SumBackward0>), tensor(8333.1260, device='cuda:0', grad_fn=<SumBackward0>), tensor(16005.4629, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 138 Loss 1715.007568359375 Accs [tensor(55718.3828, device='cuda:0', grad_fn=<SumBackward0>), tensor(8335.0830, device='cuda:0', grad_fn=<SumBackward0>), tensor(16009.2969, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 139 Loss 1715.0072021484375 Accs [tensor(55742.2578, device='cuda:0', grad_fn=<SumBackward0>), tensor(8338.4102, device='cuda:0', grad_fn=<SumBackward0>), tensor(16016.1738, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 140 Loss 1715.0068359375 Accs [tensor(55763.6992, device='cuda:0', grad_fn=<SumBackward0>), tensor(8341.4219, device='cuda:0', grad_fn=<SumBackward0>), tensor(16022.3457, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 141 Loss 1715.0067138671875 Accs [tensor(55775.7891, device='cuda:0', grad_fn=<SumBackward0>), tensor(8343.2305, device='cuda:0', grad_fn=<SumBackward0>), tensor(16025.8164, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 142 Loss 1715.00634765625 Accs [tensor(55792.8906, device='cuda:0', grad_fn=<SumBackward0>), tensor(8345.7207, device='cuda:0', grad_fn=<SumBackward0>), tensor(16030.7168, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 143 Loss 1715.0059814453125 Accs [tensor(55813.5352, device='cuda:0', grad_fn=<SumBackward0>), tensor(8348.6670, device='cuda:0', grad_fn=<SumBackward0>), tensor(16036.6406, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 144 Loss 1715.0057373046875 Accs [tensor(55827.6797, device='cuda:0', grad_fn=<SumBackward0>), tensor(8350.7539, device='cuda:0', grad_fn=<SumBackward0>), tensor(16040.7031, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 145 Loss 1715.00537109375 Accs [tensor(55844.7812, device='cuda:0', grad_fn=<SumBackward0>), tensor(8353.2168, device='cuda:0', grad_fn=<SumBackward0>), tensor(16045.6182, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 146 Loss 1715.005126953125 Accs [tensor(55866.5234, device='cuda:0', grad_fn=<SumBackward0>), tensor(8356.2871, device='cuda:0', grad_fn=<SumBackward0>), tensor(16051.8613, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 147 Loss 1715.0047607421875 Accs [tensor(55881.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(8358.5098, device='cuda:0', grad_fn=<SumBackward0>), tensor(16056.1826, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 148 Loss 1715.0045166015625 Accs [tensor(55895.1172, device='cuda:0', grad_fn=<SumBackward0>), tensor(8360.5332, device='cuda:0', grad_fn=<SumBackward0>), tensor(16060.0254, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 149 Loss 1715.00439453125 Accs [tensor(55914.2617, device='cuda:0', grad_fn=<SumBackward0>), tensor(8363.2881, device='cuda:0', grad_fn=<SumBackward0>), tensor(16065.4961, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 150 Loss 1715.0040283203125 Accs [tensor(55931.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(8365.7295, device='cuda:0', grad_fn=<SumBackward0>), tensor(16070.3018, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 151 Loss 1715.003662109375 Accs [tensor(55946.1094, device='cuda:0', grad_fn=<SumBackward0>), tensor(8367.9326, device='cuda:0', grad_fn=<SumBackward0>), tensor(16074.6074, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 152 Loss 1715.003662109375 Accs [tensor(55965.7656, device='cuda:0', grad_fn=<SumBackward0>), tensor(8370.7324, device='cuda:0', grad_fn=<SumBackward0>), tensor(16080.2354, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 153 Loss 1715.0032958984375 Accs [tensor(55982.6484, device='cuda:0', grad_fn=<SumBackward0>), tensor(8373.1768, device='cuda:0', grad_fn=<SumBackward0>), tensor(16085.0605, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 154 Loss 1715.0030517578125 Accs [tensor(55995.2852, device='cuda:0', grad_fn=<SumBackward0>), tensor(8375.0723, device='cuda:0', grad_fn=<SumBackward0>), tensor(16088.6572, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 155 Loss 1715.0029296875 Accs [tensor(56011.9023, device='cuda:0', grad_fn=<SumBackward0>), tensor(8377.4785, device='cuda:0', grad_fn=<SumBackward0>), tensor(16093.4033, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 156 Loss 1715.002685546875 Accs [tensor(56029.0703, device='cuda:0', grad_fn=<SumBackward0>), tensor(8379.9473, device='cuda:0', grad_fn=<SumBackward0>), tensor(16098.3145, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 157 Loss 1715.0025634765625 Accs [tensor(56043.1016, device='cuda:0', grad_fn=<SumBackward0>), tensor(8382., device='cuda:0', grad_fn=<SumBackward0>), tensor(16102.3281, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 158 Loss 1715.002197265625 Accs [tensor(56059.8281, device='cuda:0', grad_fn=<SumBackward0>), tensor(8384.4023, device='cuda:0', grad_fn=<SumBackward0>), tensor(16107.1123, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 159 Loss 1715.001953125 Accs [tensor(56076.0117, device='cuda:0', grad_fn=<SumBackward0>), tensor(8386.7432, device='cuda:0', grad_fn=<SumBackward0>), tensor(16111.7285, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 160 Loss 1715.0018310546875 Accs [tensor(56088.9844, device='cuda:0', grad_fn=<SumBackward0>), tensor(8388.6738, device='cuda:0', grad_fn=<SumBackward0>), tensor(16115.4150, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 161 Loss 1715.0015869140625 Accs [tensor(56104.8906, device='cuda:0', grad_fn=<SumBackward0>), tensor(8390.9824, device='cuda:0', grad_fn=<SumBackward0>), tensor(16119.9453, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 162 Loss 1715.00146484375 Accs [tensor(56121.6562, device='cuda:0', grad_fn=<SumBackward0>), tensor(8393.3984, device='cuda:0', grad_fn=<SumBackward0>), tensor(16124.7246, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 163 Loss 1715.00146484375 Accs [tensor(56136.2266, device='cuda:0', grad_fn=<SumBackward0>), tensor(8395.5254, device='cuda:0', grad_fn=<SumBackward0>), tensor(16128.8730, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 164 Loss 1715.001220703125 Accs [tensor(56152.9141, device='cuda:0', grad_fn=<SumBackward0>), tensor(8397.9287, device='cuda:0', grad_fn=<SumBackward0>), tensor(16133.6250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 165 Loss 1715.0009765625 Accs [tensor(56169.0078, device='cuda:0', grad_fn=<SumBackward0>), tensor(8400.2598, device='cuda:0', grad_fn=<SumBackward0>), tensor(16138.2012, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 166 Loss 1715.000732421875 Accs [tensor(56182.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(8402.3096, device='cuda:0', grad_fn=<SumBackward0>), tensor(16142.1445, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 167 Loss 1715.000732421875 Accs [tensor(56199.3242, device='cuda:0', grad_fn=<SumBackward0>), tensor(8404.6816, device='cuda:0', grad_fn=<SumBackward0>), tensor(16146.8105, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 168 Loss 1715.000732421875 Accs [tensor(56215.7461, device='cuda:0', grad_fn=<SumBackward0>), tensor(8407.0508, device='cuda:0', grad_fn=<SumBackward0>), tensor(16151.4805, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 169 Loss 1715.0003662109375 Accs [tensor(56230.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(8409.2480, device='cuda:0', grad_fn=<SumBackward0>), tensor(16155.7695, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 170 Loss 1715.000244140625 Accs [tensor(56247.8047, device='cuda:0', grad_fn=<SumBackward0>), tensor(8411.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(16160.5957, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 171 Loss 1715.000244140625 Accs [tensor(56262.4258, device='cuda:0', grad_fn=<SumBackward0>), tensor(8413.8242, device='cuda:0', grad_fn=<SumBackward0>), tensor(16164.7432, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 172 Loss 1714.9998779296875 Accs [tensor(56276.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(8415.9229, device='cuda:0', grad_fn=<SumBackward0>), tensor(16168.8066, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 173 Loss 1714.9998779296875 Accs [tensor(56293.4297, device='cuda:0', grad_fn=<SumBackward0>), tensor(8418.3262, device='cuda:0', grad_fn=<SumBackward0>), tensor(16173.5527, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 174 Loss 1714.9996337890625 Accs [tensor(56308.1055, device='cuda:0', grad_fn=<SumBackward0>), tensor(8420.4658, device='cuda:0', grad_fn=<SumBackward0>), tensor(16177.7236, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 175 Loss 1714.9996337890625 Accs [tensor(56323.5859, device='cuda:0', grad_fn=<SumBackward0>), tensor(8422.7070, device='cuda:0', grad_fn=<SumBackward0>), tensor(16182.1250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 176 Loss 1714.999267578125 Accs [tensor(56339.1328, device='cuda:0', grad_fn=<SumBackward0>), tensor(8424.9570, device='cuda:0', grad_fn=<SumBackward0>), tensor(16186.5508, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 177 Loss 1714.999267578125 Accs [tensor(56353.3242, device='cuda:0', grad_fn=<SumBackward0>), tensor(8427.0215, device='cuda:0', grad_fn=<SumBackward0>), tensor(16190.6094, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 178 Loss 1714.9989013671875 Accs [tensor(56369.4492, device='cuda:0', grad_fn=<SumBackward0>), tensor(8429.3398, device='cuda:0', grad_fn=<SumBackward0>), tensor(16195.2490, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 179 Loss 1714.9989013671875 Accs [tensor(56386.8125, device='cuda:0', grad_fn=<SumBackward0>), tensor(8431.8350, device='cuda:0', grad_fn=<SumBackward0>), tensor(16200.2188, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 180 Loss 1714.9989013671875 Accs [tensor(56403.5859, device='cuda:0', grad_fn=<SumBackward0>), tensor(8434.2432, device='cuda:0', grad_fn=<SumBackward0>), tensor(16204.9941, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 181 Loss 1714.998779296875 Accs [tensor(56421.9219, device='cuda:0', grad_fn=<SumBackward0>), tensor(8436.8633, device='cuda:0', grad_fn=<SumBackward0>), tensor(16210.2227, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 182 Loss 1714.99853515625 Accs [tensor(56439.5547, device='cuda:0', grad_fn=<SumBackward0>), tensor(8439.3965, device='cuda:0', grad_fn=<SumBackward0>), tensor(16215.2588, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 183 Loss 1714.99853515625 Accs [tensor(56457.1797, device='cuda:0', grad_fn=<SumBackward0>), tensor(8441.9395, device='cuda:0', grad_fn=<SumBackward0>), tensor(16220.2930, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 184 Loss 1714.9984130859375 Accs [tensor(56476.3984, device='cuda:0', grad_fn=<SumBackward0>), tensor(8444.6953, device='cuda:0', grad_fn=<SumBackward0>), tensor(16225.7891, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 185 Loss 1714.9981689453125 Accs [tensor(56494.7539, device='cuda:0', grad_fn=<SumBackward0>), tensor(8447.3389, device='cuda:0', grad_fn=<SumBackward0>), tensor(16231.0156, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 186 Loss 1714.9981689453125 Accs [tensor(56513.6641, device='cuda:0', grad_fn=<SumBackward0>), tensor(8450.0488, device='cuda:0', grad_fn=<SumBackward0>), tensor(16236.3867, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 187 Loss 1714.9981689453125 Accs [tensor(56532.7617, device='cuda:0', grad_fn=<SumBackward0>), tensor(8452.7852, device='cuda:0', grad_fn=<SumBackward0>), tensor(16241.8076, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 188 Loss 1714.997802734375 Accs [tensor(56550.6016, device='cuda:0', grad_fn=<SumBackward0>), tensor(8455.3613, device='cuda:0', grad_fn=<SumBackward0>), tensor(16246.8730, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 189 Loss 1714.997802734375 Accs [tensor(56569.5312, device='cuda:0', grad_fn=<SumBackward0>), tensor(8458.0762, device='cuda:0', grad_fn=<SumBackward0>), tensor(16252.2510, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 190 Loss 1714.997802734375 Accs [tensor(56587.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(8460.6035, device='cuda:0', grad_fn=<SumBackward0>), tensor(16257.2227, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 191 Loss 1714.9976806640625 Accs [tensor(56604.9023, device='cuda:0', grad_fn=<SumBackward0>), tensor(8463.1680, device='cuda:0', grad_fn=<SumBackward0>), tensor(16262.2852, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 192 Loss 1714.9976806640625 Accs [tensor(56622.8164, device='cuda:0', grad_fn=<SumBackward0>), tensor(8465.7363, device='cuda:0', grad_fn=<SumBackward0>), tensor(16267.3633, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 193 Loss 1714.9974365234375 Accs [tensor(56639.3906, device='cuda:0', grad_fn=<SumBackward0>), tensor(8468.1201, device='cuda:0', grad_fn=<SumBackward0>), tensor(16272.0518, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 194 Loss 1714.9974365234375 Accs [tensor(56656., device='cuda:0', grad_fn=<SumBackward0>), tensor(8470.5039, device='cuda:0', grad_fn=<SumBackward0>), tensor(16276.7588, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 195 Loss 1714.997314453125 Accs [tensor(56672.3906, device='cuda:0', grad_fn=<SumBackward0>), tensor(8472.8643, device='cuda:0', grad_fn=<SumBackward0>), tensor(16281.4004, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 196 Loss 1714.997314453125 Accs [tensor(56687.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(8475.0654, device='cuda:0', grad_fn=<SumBackward0>), tensor(16285.6777, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 197 Loss 1714.9970703125 Accs [tensor(56702.6953, device='cuda:0', grad_fn=<SumBackward0>), tensor(8477.2666, device='cuda:0', grad_fn=<SumBackward0>), tensor(16289.9854, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 198 Loss 1714.9970703125 Accs [tensor(56717.5703, device='cuda:0', grad_fn=<SumBackward0>), tensor(8479.4219, device='cuda:0', grad_fn=<SumBackward0>), tensor(16294.2031, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 199 Loss 1714.9970703125 Accs [tensor(56731.3906, device='cuda:0', grad_fn=<SumBackward0>), tensor(8481.4355, device='cuda:0', grad_fn=<SumBackward0>), tensor(16298.1182, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Best accs [tensor(19618.6406, device='cuda:0', grad_fn=<SumBackward0>), tensor(3193.3860, device='cuda:0', grad_fn=<SumBackward0>), tensor(5653.8691, device='cuda:0', grad_fn=<SumBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n",
    "    # print(\n",
    "    #     f\"Epoch {epoch + 1}: loss {round(loss, 5)}, \"\n",
    "    #     f\"train micro {round(accs[0][0] * 100, 2)}%, train macro {round(accs[0][1] * 100, 2)}%, \"\n",
    "    #     f\"valid micro {round(accs[1][0] * 100, 2)}%, valid macro {round(accs[1][1] * 100, 2)}%, \"\n",
    "    #     f\"test micro {round(accs[2][0] * 100, 2)}%, test macro {round(accs[2][1] * 100, 2)}%\"\n",
    "    # )\n",
    "    print(f\"Epoch {epoch} Loss {loss} Accs {accs}\")\n",
    "\n",
    "best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx])\n",
    "\n",
    "print(\"Best accs\", best_accs)\n",
    "\n",
    "\n",
    "\n",
    "# print(\n",
    "#     f\"Best model: \"\n",
    "#     f\"train micro {round(best_accs[0][0] * 100, 2)}%, train macro {round(best_accs[0][1] * 100, 2)}%, \"\n",
    "#     f\"valid micro {round(best_accs[1][0] * 100, 2)}%, valid macro {round(best_accs[1][1] * 100, 2)}%, \"\n",
    "#     f\"test micro {round(best_accs[2][0] * 100, 2)}%, test macro {round(best_accs[2][1] * 100, 2)}%\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([59.0337], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([58.8257], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([58.6085], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([58.1807], device='cuda:0', grad_fn=<SelectBackward0>) tensor([12.], device='cuda:0')\n",
      "tensor([58.2879], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([59.9404], device='cuda:0', grad_fn=<SelectBackward0>) tensor([56.], device='cuda:0')\n",
      "tensor([58.1645], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([58.3219], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([59.4205], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([17.2723], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([17.2728], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([17.2837], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([17.2762], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([17.2773], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([17.2730], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([17.2735], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([17.2750], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([17.2671], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([59.8344], device='cuda:0', grad_fn=<SelectBackward0>) tensor([94.], device='cuda:0')\n",
      "tensor([15.3625], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([17.2760], device='cuda:0', grad_fn=<SelectBackward0>) tensor([11.], device='cuda:0')\n",
      "tensor([17.2748], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([17.2748], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([60.2732], device='cuda:0', grad_fn=<SelectBackward0>) tensor([48.], device='cuda:0')\n",
      "tensor([17.2719], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([17.2696], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([17.2745], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([17.2783], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([59.6066], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([60.3696], device='cuda:0', grad_fn=<SelectBackward0>) tensor([111.], device='cuda:0')\n",
      "tensor([58.8703], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([59.2057], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([17.2742], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([57.8927], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([58.9564], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([17.2703], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([59.1522], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([58.5417], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([59.1044], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([58.2013], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([17.2705], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([58.7853], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([17.2751], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([58.7184], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([15.3642], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([59.9286], device='cuda:0', grad_fn=<SelectBackward0>) tensor([58.], device='cuda:0')\n",
      "tensor([58.2654], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([57.7154], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([58.3209], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([58.5135], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([59.7782], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([57.9691], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([58.6161], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([59.3895], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([15.3657], device='cuda:0', grad_fn=<SelectBackward0>) tensor([14.], device='cuda:0')\n",
      "tensor([59.5585], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([17.2785], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([17.2659], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([15.3666], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([58.7484], device='cuda:0', grad_fn=<SelectBackward0>) tensor([32.], device='cuda:0')\n",
      "tensor([17.2733], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([59.1406], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([59.5323], device='cuda:0', grad_fn=<SelectBackward0>) tensor([19.], device='cuda:0')\n",
      "tensor([59.4257], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([60.3401], device='cuda:0', grad_fn=<SelectBackward0>) tensor([144.], device='cuda:0')\n",
      "tensor([60.1875], device='cuda:0', grad_fn=<SelectBackward0>) tensor([79.], device='cuda:0')\n",
      "tensor([58.6301], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([58.2042], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([17.2740], device='cuda:0', grad_fn=<SelectBackward0>) tensor([12.], device='cuda:0')\n",
      "tensor([59.1874], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([15.3613], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([58.2879], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([15.3670], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([15.3650], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([17.2818], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([58.4511], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([59.4666], device='cuda:0', grad_fn=<SelectBackward0>) tensor([51.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "preds = best_model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "# mask = preds['event'] > 0\n",
    "# preds['event'][preds['event'] > 0].shape\n",
    "\n",
    "# print(preds['event'][0], hetero_graph.node_target['event'][0]) \n",
    "\n",
    "\n",
    "#print(hetero_graph.node_feature['event'])\n",
    "\n",
    "# for i in range(3000):\n",
    "#     if hetero_graph.node_target['event'][i] != -1: # concepts have node target -1\n",
    "#         print(preds['event'][i], hetero_graph.node_target['event'][i])\n",
    "        \n",
    "    \n",
    "for i in range(1000):    \n",
    "    if hetero_graph.node_target['event'][test_idx['event']][i] != -1:\n",
    "        print(preds['event'][test_idx['event']][i], hetero_graph.node_target['event'][test_idx['event']][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
