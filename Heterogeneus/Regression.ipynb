{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        self.lin_dst = nn.Linear(in_channels_dst, out_channels)\n",
    "        self.lin_src = nn.Linear(in_channels_src, out_channels)\n",
    "        self.lin_update = nn.Linear(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "        ):\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        \"\"\"\n",
    "        Initializes the HeteroGNNWrapperConv instance.\n",
    "\n",
    "        :param convs: Dictionary of convolution layers for each message type.\n",
    "        :param args: Arguments dictionary containing hyperparameters like hidden_size and attn_size.\n",
    "        :param aggr: Aggregation method, defaults to 'mean'.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_features: Dictionary of node features for each node type.\n",
    "        :param edge_indices: Dictionary of edge indices for each message type.\n",
    "        :return: Aggregated node embeddings for each node type.\n",
    "        \"\"\"\n",
    "        \n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        \n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        \n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "                \n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        \"\"\"\n",
    "        Aggregates node embeddings using the specified aggregation method.\n",
    "\n",
    "        :param xs: List of node embeddings to aggregate.\n",
    "        :return: Aggregated node embeddings as a torch.Tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates convolutional layers for each message type in a heterogeneous graph.\n",
    "\n",
    "    :param hetero_graph: The heterogeneous graph for which convolutions are to be created.\n",
    "    :param conv: The convolutional layer class or constructor.\n",
    "    :param hidden_size: The number of features in the hidden layer.\n",
    "    :param first_layer: Boolean indicating if this is the first layer in the network.\n",
    "    \n",
    "    :return: A dictionary of convolutional layers, keyed by message type.\n",
    "    \"\"\"\n",
    "\n",
    "    convs = {}\n",
    "    \n",
    "    # Extracting all types of messages/edges in the heterogeneous graph.\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        \n",
    "        # Determine the input feature size for source and destination nodes.\n",
    "        # If it's the first layer, use the feature size of the nodes.\n",
    "        # Otherwise, use the hidden size, since from there on the size of embeddings\n",
    "        # is the same for all nodes.\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        \n",
    "        # Create a convolutional layer for this message type and add it to the dictionary.\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, num_layers, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "        \n",
    "        # Initialize the graph convolutional layers\n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        # Initialize batch normalization, ReLU, and fully connected layers for each node type\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            \n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            \n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "            \n",
    "    def forward(self, node_feature, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_feature: Dictionary of node features for each node type.\n",
    "        :param edge_index: Dictionary of edge indices for each message type.\n",
    "        :return: The output embeddings for each node type after passing through the model.\n",
    "        \"\"\"\n",
    "        x = node_feature\n",
    "\n",
    "        # Apply graph convolutional, batch normalization, and ReLU layers\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        x = forward_op(x, self.fc)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        \"\"\"\n",
    "        Computes the loss for the model.\n",
    "\n",
    "        :param preds: Predictions made by the model.\n",
    "        :param y: Ground truth target values.\n",
    "        :param indices: Indices of nodes for which loss should be calculated.\n",
    "        \n",
    "        :return: The computed loss value.\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = 0\n",
    "        loss_func = torch.nn.MSELoss() \n",
    "             \n",
    "        mask = y['event'][indices['event'], 0] != -1\n",
    "        non_zero_idx = torch.masked_select(indices['event'], mask)\n",
    "                \n",
    "        loss += loss_func(preds['event'][non_zero_idx], y['event'][non_zero_idx])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    \"\"\"\n",
    "    Trains the model on the given heterogeneous graph using the specified indices.\n",
    "\n",
    "    :param model: The graph neural network model to train.\n",
    "    :param optimizer: The optimizer used for training the model.\n",
    "    :param hetero_graph: The heterogeneous graph data.\n",
    "    :param train_idx: Indices for training nodes.\n",
    "\n",
    "    :return: The training loss as a float.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "    optimizer.zero_grad() # Zero out any existing gradients \n",
    "    \n",
    "    # Compute predictions using the model\n",
    "    # TODO: Use only train_idx instead of edge_index\n",
    "    # TODO: Train only on events not on concepts\n",
    "    \n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    # Compute the loss using model's loss function\n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "\n",
    "    loss.backward() # Backward pass: compute gradient of the loss\n",
    "    optimizer.step() # Perform a single optimization step, updates parameters\n",
    "    \n",
    "    return loss.item() \n",
    "\n",
    "def test(model, graph, indices, best_model=None, best_val=0):\n",
    "    \"\"\"\n",
    "    Tests the model on given indices and updates the best model based on validation loss.\n",
    "\n",
    "    :param model: The trained graph neural network model.\n",
    "    :param graph: The heterogeneous graph data.\n",
    "    :param indices: List of indices for training, validation, and testing nodes.\n",
    "    :param best_model: The current best model based on validation loss.\n",
    "    :param best_val: The current best validation loss.\n",
    "    \n",
    "    :return: A tuple containing the list of losses for each dataset, the best model, and the best validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    accs = []\n",
    "    \n",
    "    # Evaluate the model on each set of indices\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        \n",
    "        idx = index['event']\n",
    "\n",
    "        L1 = torch.sum(torch.abs(preds['event'][idx] - graph.node_target['event'][idx]))\n",
    "        \n",
    "        accs.append(L1)\n",
    "    \n",
    "    # Update the best model and validation loss if the current model performs better\n",
    "    if accs[1] < best_val:\n",
    "        best_val = accs[1]\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    return accs, best_model, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 64,\n",
    "    'epochs': 200,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.201,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell creates a small heterogeneous graph, primarily for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [1, 1, 1],   # event 0\n",
    "                [2, 2, 2]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([[50], [2000]], dtype=torch.float32),\n",
    "    # \"concept\": torch.tensor([[0], [0]], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}\n",
    "\n",
    "# Testing\n",
    "hetero_graph = HeteroGraph(\n",
    "    node_feature=S_node_feature,\n",
    "    node_target=S_node_targets,\n",
    "    edge_index=S_edge_index\n",
    ")\n",
    "\n",
    "train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "with open(\"./10_concepts_similar_llm_noUnknown.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "    # Convert to directed graph for compatibility with Deepsnap\n",
    "    # G = G.to_directed()\n",
    "\n",
    "hetero_graph = HeteroGraph(G, netlib=nx, directed=True)\n",
    "print(hetero_graph['node_target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 1\n",
      "\t Feature 769\n",
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 769\n",
      "\t Feature 1\n",
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 769\n",
      "\t Feature 769\n",
      "KEY ('concept', 'related', 'event') <class 'tuple'>\n",
      "KEY NUMS ('concept', 'related', 'event') 36989 5000\n",
      "MAX EDGES tensor(36988) tensor(4999) 36989 5000\n",
      "KEY ('event', 'related', 'concept') <class 'tuple'>\n",
      "KEY NUMS ('event', 'related', 'concept') 5000 36989\n",
      "MAX EDGES tensor(4999) tensor(36988) 5000 36989\n",
      "KEY ('event', 'similar', 'event') <class 'tuple'>\n",
      "KEY NUMS ('event', 'similar', 'event') 5000 5000\n",
      "MAX EDGES tensor(4986) tensor(4986) 5000 5000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HeteroGraph' object has no attribute 'node_target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Adrian\\AI Lab Dropbox\\Adrian MladeniÄ‡ Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\Heterogeneus\\Regression.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     hetero_graph\u001b[39m.\u001b[39medge_index[key] \u001b[39m=\u001b[39m adj\u001b[39m.\u001b[39mt()\u001b[39m.\u001b[39mto(args[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Send node targets to device\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m hetero_graph\u001b[39m.\u001b[39;49mnode_target:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X30sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     hetero_graph\u001b[39m.\u001b[39mnode_target[key] \u001b[39m=\u001b[39m hetero_graph\u001b[39m.\u001b[39mnode_target[key]\u001b[39m.\u001b[39mto(args[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HeteroGraph' object has no attribute 'node_target'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block ensures that all tensors in a heterogeneous graph are transferred to the same \n",
    "computational device, as specified in the 'args' dictionary.\n",
    "\"\"\"\n",
    "\n",
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "# Send node features to device\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "# Create a torch.SparseTensor from edge_index and send it to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key, type(key))\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    \n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max(), hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
    "    \n",
    "# Send node targets to device\n",
    "for key in hetero_graph.node_target:\n",
    "    hetero_graph.node_target[key] = hetero_graph.node_target[key].to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5940])\n",
      "torch.Size([1698])\n",
      "torch.Size([849])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block creates a basic split of a graph's nodes into training, validation, and testing sets. \n",
    "It uses predefined ratios to divide 'event' and 'concept' nodes in the heterogeneous graph for a simple \n",
    "dataset split, mainly for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "nEvents = hetero_graph.num_nodes(\"event\")\n",
    "nConcepts = hetero_graph.num_nodes(\"concept\")\n",
    "\n",
    "s1 = 0.7\n",
    "s2 = 0.8\n",
    "\n",
    "train_idx = {   \"event\": torch.tensor(range(0, int(nEvents * s1))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(0, int(nConcepts * s1))).to(args[\"device\"])\n",
    "            }\n",
    "val_idx = {   \"event\": torch.tensor(range(int(nEvents * s1), int(nEvents * s2))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s1), int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "test_idx = {   \"event\": torch.tensor(range(int(nEvents * s2), nEvents)).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s2), nConcepts)).to(args[\"device\"])\n",
    "            }\n",
    "\n",
    "print(train_idx[\"event\"].shape)\n",
    "print(test_idx[\"event\"].shape)\n",
    "print(val_idx[\"event\"].shape)\n",
    "\n",
    "\n",
    "# TODO: Add node labels to the nodes and try to make the deepsnap split work even for regression!\n",
    "\n",
    "# dataset = deepsnap.dataset.GraphDataset([hetero_graph], task='node')\n",
    "\n",
    "# dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
    "# datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 1975.555908203125 Accs [tensor(5.5297e+08, device='cuda:0', grad_fn=<SumBackward0>), tensor(79210232., device='cuda:0', grad_fn=<SumBackward0>), tensor(1.5709e+08, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 1 Loss 1686.09326171875 Accs [tensor(2.8904e+08, device='cuda:0', grad_fn=<SumBackward0>), tensor(41578960., device='cuda:0', grad_fn=<SumBackward0>), tensor(80936048., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 2 Loss 1438.0908203125 Accs [tensor(27264688., device='cuda:0', grad_fn=<SumBackward0>), tensor(3921568.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(7600623.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 3 Loss 1343.2333984375 Accs [tensor(16289234., device='cuda:0', grad_fn=<SumBackward0>), tensor(2347209.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(4543867., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 4 Loss 1208.93115234375 Accs [tensor(12156182., device='cuda:0', grad_fn=<SumBackward0>), tensor(1752394., device='cuda:0', grad_fn=<SumBackward0>), tensor(3387916.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 5 Loss 1043.08740234375 Accs [tensor(8245012., device='cuda:0', grad_fn=<SumBackward0>), tensor(1188867.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(2298525.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 6 Loss 994.5971069335938 Accs [tensor(4738276.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(683641.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(1322363.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 7 Loss 843.8605346679688 Accs [tensor(2651710.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(383411.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(743529.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 8 Loss 712.9716796875 Accs [tensor(2180512.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(315127.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(611763.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 9 Loss 572.29931640625 Accs [tensor(1855382.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(268258.9688, device='cuda:0', grad_fn=<SumBackward0>), tensor(519651.4375, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 10 Loss 450.58013916015625 Accs [tensor(1688379.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(243374.1562, device='cuda:0', grad_fn=<SumBackward0>), tensor(471474.1562, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 11 Loss 283.815673828125 Accs [tensor(1564492., device='cuda:0', grad_fn=<SumBackward0>), tensor(225610.9531, device='cuda:0', grad_fn=<SumBackward0>), tensor(436959.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 12 Loss 268.1213684082031 Accs [tensor(1346741.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(194307.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(376066.4062, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 13 Loss 314.7353820800781 Accs [tensor(938044.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(135560.3594, device='cuda:0', grad_fn=<SumBackward0>), tensor(261808.3594, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 14 Loss 308.50018310546875 Accs [tensor(260108.3281, device='cuda:0', grad_fn=<SumBackward0>), tensor(38155.4297, device='cuda:0', grad_fn=<SumBackward0>), tensor(73407.2891, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 15 Loss 315.3747253417969 Accs [tensor(320770.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(47442.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(91883.5391, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 16 Loss 272.43133544921875 Accs [tensor(527556.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(77446.9609, device='cuda:0', grad_fn=<SumBackward0>), tensor(150624.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 17 Loss 253.40963745117188 Accs [tensor(651810.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(95487.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(185889.9219, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 18 Loss 232.98904418945312 Accs [tensor(710848., device='cuda:0', grad_fn=<SumBackward0>), tensor(104122.2812, device='cuda:0', grad_fn=<SumBackward0>), tensor(202634.6250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 19 Loss 228.88629150390625 Accs [tensor(824675.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(120644.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(235118., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 20 Loss 225.14077758789062 Accs [tensor(985701.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(143922.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(280778.1250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 21 Loss 217.50930786132812 Accs [tensor(1068031.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(155790.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(304115.5625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 22 Loss 218.42587280273438 Accs [tensor(1058977., device='cuda:0', grad_fn=<SumBackward0>), tensor(154446.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(301528.4062, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 23 Loss 209.9807891845703 Accs [tensor(1034650.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(150917.3438, device='cuda:0', grad_fn=<SumBackward0>), tensor(294613.9375, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 24 Loss 204.68780517578125 Accs [tensor(1038080.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(151422.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(295637.8125, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 25 Loss 200.3850860595703 Accs [tensor(1077736.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(157165.9219, device='cuda:0', grad_fn=<SumBackward0>), tensor(306935.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 26 Loss 195.60301208496094 Accs [tensor(1093574., device='cuda:0', grad_fn=<SumBackward0>), tensor(159459., device='cuda:0', grad_fn=<SumBackward0>), tensor(311451.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 27 Loss 193.37750244140625 Accs [tensor(1061121.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(154757.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(302215.0625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 28 Loss 186.76515197753906 Accs [tensor(1022950.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(149227.1562, device='cuda:0', grad_fn=<SumBackward0>), tensor(291516.4375, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 29 Loss 182.05320739746094 Accs [tensor(1038599.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(151510.6094, device='cuda:0', grad_fn=<SumBackward0>), tensor(295896.4375, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 30 Loss 175.076171875 Accs [tensor(1048834.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(152947.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(298785.1562, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 31 Loss 170.72402954101562 Accs [tensor(942444.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(137510.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(268522.0312, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 32 Loss 165.244140625 Accs [tensor(842238.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(122955.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(240014.9219, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 33 Loss 161.48385620117188 Accs [tensor(859381.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(125332.8672, device='cuda:0', grad_fn=<SumBackward0>), tensor(245032.2344, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 34 Loss 156.51026916503906 Accs [tensor(692105.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(101023., device='cuda:0', grad_fn=<SumBackward0>), tensor(197494.3750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 35 Loss 150.48236083984375 Accs [tensor(605699.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(88460.9609, device='cuda:0', grad_fn=<SumBackward0>), tensor(172939.9062, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 36 Loss 145.63075256347656 Accs [tensor(790651.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(115180.7969, device='cuda:0', grad_fn=<SumBackward0>), tensor(225545.6250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 37 Loss 142.12234497070312 Accs [tensor(553400.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(80789.7031, device='cuda:0', grad_fn=<SumBackward0>), tensor(158128.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 38 Loss 138.58151245117188 Accs [tensor(767241.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(111723.1094, device='cuda:0', grad_fn=<SumBackward0>), tensor(218514.8750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 39 Loss 135.8765411376953 Accs [tensor(436980.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(63849.3477, device='cuda:0', grad_fn=<SumBackward0>), tensor(124751.5625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 40 Loss 135.8367462158203 Accs [tensor(671107.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(97909.2109, device='cuda:0', grad_fn=<SumBackward0>), tensor(191095.4219, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 41 Loss 128.88409423828125 Accs [tensor(383367.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(56170.9648, device='cuda:0', grad_fn=<SumBackward0>), tensor(109161.8281, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 42 Loss 119.2193374633789 Accs [tensor(374547.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(54889.9727, device='cuda:0', grad_fn=<SumBackward0>), tensor(106543.6875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 43 Loss 109.72422790527344 Accs [tensor(335779.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(49308.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(95568.9531, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 44 Loss 107.21898651123047 Accs [tensor(206587.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(30373.8711, device='cuda:0', grad_fn=<SumBackward0>), tensor(58682.7969, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 45 Loss 106.12010192871094 Accs [tensor(207447.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(30584.7578, device='cuda:0', grad_fn=<SumBackward0>), tensor(58922.0234, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 46 Loss 96.83897399902344 Accs [tensor(180857.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(26500.9316, device='cuda:0', grad_fn=<SumBackward0>), tensor(51339.1953, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 47 Loss 88.16271209716797 Accs [tensor(162145., device='cuda:0', grad_fn=<SumBackward0>), tensor(23802.2539, device='cuda:0', grad_fn=<SumBackward0>), tensor(46081.3984, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 48 Loss 85.23011016845703 Accs [tensor(164078.3125, device='cuda:0', grad_fn=<SumBackward0>), tensor(24070.6758, device='cuda:0', grad_fn=<SumBackward0>), tensor(46670.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 49 Loss 84.8750991821289 Accs [tensor(143478.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(21072.4277, device='cuda:0', grad_fn=<SumBackward0>), tensor(40887.6719, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 50 Loss 83.33433532714844 Accs [tensor(150803.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(22182.1895, device='cuda:0', grad_fn=<SumBackward0>), tensor(42936.1367, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 51 Loss 76.48674774169922 Accs [tensor(140510.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(20715.9258, device='cuda:0', grad_fn=<SumBackward0>), tensor(40040.9766, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 52 Loss 68.43414306640625 Accs [tensor(139615.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(20594.8477, device='cuda:0', grad_fn=<SumBackward0>), tensor(39780.2773, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 53 Loss 60.51240158081055 Accs [tensor(137456.2344, device='cuda:0', grad_fn=<SumBackward0>), tensor(20279.2246, device='cuda:0', grad_fn=<SumBackward0>), tensor(39249.6133, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 54 Loss 57.75662612915039 Accs [tensor(127495.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(18867.2520, device='cuda:0', grad_fn=<SumBackward0>), tensor(36455.1875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 55 Loss 58.08790588378906 Accs [tensor(122010.4688, device='cuda:0', grad_fn=<SumBackward0>), tensor(18069.3047, device='cuda:0', grad_fn=<SumBackward0>), tensor(34925.0156, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 56 Loss 55.43886947631836 Accs [tensor(113147.6406, device='cuda:0', grad_fn=<SumBackward0>), tensor(16830.0254, device='cuda:0', grad_fn=<SumBackward0>), tensor(32425.8320, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 57 Loss 53.279075622558594 Accs [tensor(110659.6406, device='cuda:0', grad_fn=<SumBackward0>), tensor(16473.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(31736.8984, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 58 Loss 48.68998718261719 Accs [tensor(110517.5078, device='cuda:0', grad_fn=<SumBackward0>), tensor(16473.9395, device='cuda:0', grad_fn=<SumBackward0>), tensor(31724.3457, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 59 Loss 45.81460952758789 Accs [tensor(112061.0469, device='cuda:0', grad_fn=<SumBackward0>), tensor(16685.8789, device='cuda:0', grad_fn=<SumBackward0>), tensor(32161.3203, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 60 Loss 43.531742095947266 Accs [tensor(114539.3203, device='cuda:0', grad_fn=<SumBackward0>), tensor(17053.5449, device='cuda:0', grad_fn=<SumBackward0>), tensor(32787.3906, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 61 Loss 41.87617111206055 Accs [tensor(113676.4844, device='cuda:0', grad_fn=<SumBackward0>), tensor(16915.9492, device='cuda:0', grad_fn=<SumBackward0>), tensor(32542.6680, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 62 Loss 40.48387908935547 Accs [tensor(109081.6562, device='cuda:0', grad_fn=<SumBackward0>), tensor(16261.1836, device='cuda:0', grad_fn=<SumBackward0>), tensor(31236.5391, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 63 Loss 40.98418045043945 Accs [tensor(110713.8906, device='cuda:0', grad_fn=<SumBackward0>), tensor(16482.9219, device='cuda:0', grad_fn=<SumBackward0>), tensor(31754.9570, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 64 Loss 46.05720901489258 Accs [tensor(94283.4297, device='cuda:0', grad_fn=<SumBackward0>), tensor(14141.2861, device='cuda:0', grad_fn=<SumBackward0>), tensor(27261.2637, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 65 Loss 75.39799499511719 Accs [tensor(75321.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(11354.7383, device='cuda:0', grad_fn=<SumBackward0>), tensor(21698.9219, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 66 Loss 168.92303466796875 Accs [tensor(73301.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(10807.2637, device='cuda:0', grad_fn=<SumBackward0>), tensor(21041.4492, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 67 Loss 51.64201354980469 Accs [tensor(97781.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(14198.7109, device='cuda:0', grad_fn=<SumBackward0>), tensor(27977.9805, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 68 Loss 130.8644256591797 Accs [tensor(87839.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(12939.4883, device='cuda:0', grad_fn=<SumBackward0>), tensor(25227.7168, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 69 Loss 54.75197219848633 Accs [tensor(40577.7148, device='cuda:0', grad_fn=<SumBackward0>), tensor(6217.9634, device='cuda:0', grad_fn=<SumBackward0>), tensor(11736.0518, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 70 Loss 99.61872863769531 Accs [tensor(10472.8574, device='cuda:0', grad_fn=<SumBackward0>), tensor(1843.0007, device='cuda:0', grad_fn=<SumBackward0>), tensor(3037.3223, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 71 Loss 40.777854919433594 Accs [tensor(8991.0742, device='cuda:0', grad_fn=<SumBackward0>), tensor(1495.2524, device='cuda:0', grad_fn=<SumBackward0>), tensor(2555.4712, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 72 Loss 91.81305694580078 Accs [tensor(10780.3174, device='cuda:0', grad_fn=<SumBackward0>), tensor(1824.8628, device='cuda:0', grad_fn=<SumBackward0>), tensor(3148.3369, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 73 Loss 44.967044830322266 Accs [tensor(21711.0195, device='cuda:0', grad_fn=<SumBackward0>), tensor(3340.2322, device='cuda:0', grad_fn=<SumBackward0>), tensor(6184.5039, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 74 Loss 64.42839050292969 Accs [tensor(34862.8828, device='cuda:0', grad_fn=<SumBackward0>), tensor(5196.8848, device='cuda:0', grad_fn=<SumBackward0>), tensor(10053.3027, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 75 Loss 63.66050720214844 Accs [tensor(52069.8281, device='cuda:0', grad_fn=<SumBackward0>), tensor(7641.9854, device='cuda:0', grad_fn=<SumBackward0>), tensor(15100.9258, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 76 Loss 44.25642776489258 Accs [tensor(57536.5547, device='cuda:0', grad_fn=<SumBackward0>), tensor(8464.8730, device='cuda:0', grad_fn=<SumBackward0>), tensor(16638.0977, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 77 Loss 61.375736236572266 Accs [tensor(78352.5312, device='cuda:0', grad_fn=<SumBackward0>), tensor(11479.4697, device='cuda:0', grad_fn=<SumBackward0>), tensor(22503.1211, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 78 Loss 45.3658447265625 Accs [tensor(98760.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(14457.4736, device='cuda:0', grad_fn=<SumBackward0>), tensor(28362.0586, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 79 Loss 46.937679290771484 Accs [tensor(131837.2344, device='cuda:0', grad_fn=<SumBackward0>), tensor(19247.6484, device='cuda:0', grad_fn=<SumBackward0>), tensor(37828.9492, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 80 Loss 50.5417366027832 Accs [tensor(126364.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(18612.4297, device='cuda:0', grad_fn=<SumBackward0>), tensor(36202.1094, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 81 Loss 38.40201187133789 Accs [tensor(142564.5938, device='cuda:0', grad_fn=<SumBackward0>), tensor(20959.4844, device='cuda:0', grad_fn=<SumBackward0>), tensor(40741.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 82 Loss 49.0316276550293 Accs [tensor(201334.5312, device='cuda:0', grad_fn=<SumBackward0>), tensor(29414.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(57469.9141, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 83 Loss 40.05830001831055 Accs [tensor(372202.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(53946.5273, device='cuda:0', grad_fn=<SumBackward0>), tensor(106488.8281, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 84 Loss 39.683067321777344 Accs [tensor(487935.3125, device='cuda:0', grad_fn=<SumBackward0>), tensor(70760.3281, device='cuda:0', grad_fn=<SumBackward0>), tensor(139624.6250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 85 Loss 44.48260498046875 Accs [tensor(404694.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(58756.3672, device='cuda:0', grad_fn=<SumBackward0>), tensor(115883.2578, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 86 Loss 36.56776809692383 Accs [tensor(353759.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(51497.1484, device='cuda:0', grad_fn=<SumBackward0>), tensor(101076.7031, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 87 Loss 41.732086181640625 Accs [tensor(529230.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(76548.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(151312.7969, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 88 Loss 37.651405334472656 Accs [tensor(762869.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(110385.2188, device='cuda:0', grad_fn=<SumBackward0>), tensor(217894.0469, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 89 Loss 38.35825729370117 Accs [tensor(744532.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(107763.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(212675.3438, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 90 Loss 38.767391204833984 Accs [tensor(540120.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(78226.5156, device='cuda:0', grad_fn=<SumBackward0>), tensor(154518.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 91 Loss 34.358802795410156 Accs [tensor(441323.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(63897.1797, device='cuda:0', grad_fn=<SumBackward0>), tensor(126365.5078, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 92 Loss 38.426788330078125 Accs [tensor(641150.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(92750.8203, device='cuda:0', grad_fn=<SumBackward0>), tensor(183343.3750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 93 Loss 34.02423095703125 Accs [tensor(876354.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(126692.7656, device='cuda:0', grad_fn=<SumBackward0>), tensor(250040.7969, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 94 Loss 35.5540771484375 Accs [tensor(906392.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(130995.0156, device='cuda:0', grad_fn=<SumBackward0>), tensor(258382.1406, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 95 Loss 34.98422622680664 Accs [tensor(777250.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(112368.2891, device='cuda:0', grad_fn=<SumBackward0>), tensor(221591.2188, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 96 Loss 33.279361724853516 Accs [tensor(678199.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(98391.4297, device='cuda:0', grad_fn=<SumBackward0>), tensor(193318.6094, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 97 Loss 34.62467575073242 Accs [tensor(753467.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(109591.9531, device='cuda:0', grad_fn=<SumBackward0>), tensor(214851.9219, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 98 Loss 32.830867767333984 Accs [tensor(773482.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(112451.7031, device='cuda:0', grad_fn=<SumBackward0>), tensor(220525.5312, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 99 Loss 33.83226776123047 Accs [tensor(623155.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(90594.4844, device='cuda:0', grad_fn=<SumBackward0>), tensor(177830.4375, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 100 Loss 31.948898315429688 Accs [tensor(478373.3438, device='cuda:0', grad_fn=<SumBackward0>), tensor(69421.8125, device='cuda:0', grad_fn=<SumBackward0>), tensor(136468.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 101 Loss 34.68193435668945 Accs [tensor(807105., device='cuda:0', grad_fn=<SumBackward0>), tensor(117530.9531, device='cuda:0', grad_fn=<SumBackward0>), tensor(230254.0781, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 102 Loss 32.18714141845703 Accs [tensor(1636653.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(237828.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(465950.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 103 Loss 33.49264907836914 Accs [tensor(2080506.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(302109.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(592028.6250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 104 Loss 33.261653900146484 Accs [tensor(2197625., device='cuda:0', grad_fn=<SumBackward0>), tensor(319112.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(625235.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 105 Loss 32.492923736572266 Accs [tensor(2281873., device='cuda:0', grad_fn=<SumBackward0>), tensor(330934.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(648704.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 106 Loss 32.90531921386719 Accs [tensor(2329569.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(337797.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(662302.5625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 107 Loss 32.31260299682617 Accs [tensor(2252590.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(326785.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(640596.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 108 Loss 32.72089767456055 Accs [tensor(2068299.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(300179.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(588272.6875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 109 Loss 31.55558204650879 Accs [tensor(1877128.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(272380.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(533961.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 110 Loss 31.87478256225586 Accs [tensor(1777423., device='cuda:0', grad_fn=<SumBackward0>), tensor(257885.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(505736.3750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 111 Loss 31.121231079101562 Accs [tensor(1686244.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(244635.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(479898.9062, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 112 Loss 31.423564910888672 Accs [tensor(1515700.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(219809.8125, device='cuda:0', grad_fn=<SumBackward0>), tensor(431397.6875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 113 Loss 31.294649124145508 Accs [tensor(1289156.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(187052.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(367008.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 114 Loss 30.916780471801758 Accs [tensor(1108840., device='cuda:0', grad_fn=<SumBackward0>), tensor(161279.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(315826.3750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 115 Loss 30.82753562927246 Accs [tensor(1013921.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(147736.3438, device='cuda:0', grad_fn=<SumBackward0>), tensor(288854.3438, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 116 Loss 30.585433959960938 Accs [tensor(899074.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(130816.3594, device='cuda:0', grad_fn=<SumBackward0>), tensor(256264.6875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 117 Loss 30.592607498168945 Accs [tensor(791586.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(115322.1719, device='cuda:0', grad_fn=<SumBackward0>), tensor(225588.5938, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 118 Loss 30.535919189453125 Accs [tensor(704371.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(102754.7891, device='cuda:0', grad_fn=<SumBackward0>), tensor(201174.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 119 Loss 30.339813232421875 Accs [tensor(692935.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(101429.0234, device='cuda:0', grad_fn=<SumBackward0>), tensor(197898.4688, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 120 Loss 30.165544509887695 Accs [tensor(642862.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(94219.5469, device='cuda:0', grad_fn=<SumBackward0>), tensor(183624.4844, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 121 Loss 30.035236358642578 Accs [tensor(546668.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(80305.1953, device='cuda:0', grad_fn=<SumBackward0>), tensor(156278.8594, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 122 Loss 29.911853790283203 Accs [tensor(499174.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(73443.0078, device='cuda:0', grad_fn=<SumBackward0>), tensor(142775.4375, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 123 Loss 29.8575439453125 Accs [tensor(455767.7812, device='cuda:0', grad_fn=<SumBackward0>), tensor(66694.5312, device='cuda:0', grad_fn=<SumBackward0>), tensor(130169.6719, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 124 Loss 48.60411071777344 Accs [tensor(2423701., device='cuda:0', grad_fn=<SumBackward0>), tensor(352157.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(689678.8750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 125 Loss 67.06514739990234 Accs [tensor(4111578.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(596374.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(1169508.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 126 Loss 45.409461975097656 Accs [tensor(5055503.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(732962.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(1437462.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 127 Loss 49.75434875488281 Accs [tensor(5313984., device='cuda:0', grad_fn=<SumBackward0>), tensor(770460.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(1510538.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 128 Loss 47.91154098510742 Accs [tensor(5266262., device='cuda:0', grad_fn=<SumBackward0>), tensor(763541.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(1496976.8750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 129 Loss 42.4144172668457 Accs [tensor(5122752.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(742566.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(1456254.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 130 Loss 46.73145294189453 Accs [tensor(4913867., device='cuda:0', grad_fn=<SumBackward0>), tensor(712248.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(1396872.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 131 Loss 39.650516510009766 Accs [tensor(4852664., device='cuda:0', grad_fn=<SumBackward0>), tensor(703370.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(1379416., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 132 Loss 41.191158294677734 Accs [tensor(4929995., device='cuda:0', grad_fn=<SumBackward0>), tensor(714518., device='cuda:0', grad_fn=<SumBackward0>), tensor(1401386.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 133 Loss 36.71009826660156 Accs [tensor(4868132.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(705507.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(1383811.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 134 Loss 42.23073196411133 Accs [tensor(4634829.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(671677.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(1317494.6250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 135 Loss 38.336524963378906 Accs [tensor(4375012., device='cuda:0', grad_fn=<SumBackward0>), tensor(634023.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(1243643.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 136 Loss 40.17367172241211 Accs [tensor(4177737.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(605366.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(1187653.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 137 Loss 34.80852508544922 Accs [tensor(3995888., device='cuda:0', grad_fn=<SumBackward0>), tensor(578968.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(1135975., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 138 Loss 35.69326400756836 Accs [tensor(3748018.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(543031., device='cuda:0', grad_fn=<SumBackward0>), tensor(1065496.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 139 Loss 36.418697357177734 Accs [tensor(3479416.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(504165.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(989105.1250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 140 Loss 36.336944580078125 Accs [tensor(3368731.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(488235.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(957544.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 141 Loss 36.37699508666992 Accs [tensor(3397254., device='cuda:0', grad_fn=<SumBackward0>), tensor(492345.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(965575.0625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 142 Loss 33.35150909423828 Accs [tensor(3363329.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(487413.9688, device='cuda:0', grad_fn=<SumBackward0>), tensor(955858.6250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 143 Loss 34.09251403808594 Accs [tensor(3218020., device='cuda:0', grad_fn=<SumBackward0>), tensor(466334.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(914450.4375, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 144 Loss 33.14521026611328 Accs [tensor(3021908.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(437883.9688, device='cuda:0', grad_fn=<SumBackward0>), tensor(858658.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 145 Loss 34.32583236694336 Accs [tensor(2858158., device='cuda:0', grad_fn=<SumBackward0>), tensor(414131.0938, device='cuda:0', grad_fn=<SumBackward0>), tensor(812056.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 146 Loss 32.569725036621094 Accs [tensor(2678434.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(388049.4688, device='cuda:0', grad_fn=<SumBackward0>), tensor(760913.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 147 Loss 32.56362533569336 Accs [tensor(2422624.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(350946.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(688188.8125, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 148 Loss 31.844507217407227 Accs [tensor(2155945., device='cuda:0', grad_fn=<SumBackward0>), tensor(312287.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(612414.6250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 149 Loss 32.74782180786133 Accs [tensor(2006799.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(290692.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(570004.8750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 150 Loss 32.386741638183594 Accs [tensor(1936777.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(280548.0938, device='cuda:0', grad_fn=<SumBackward0>), tensor(550173.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 151 Loss 31.80887794494629 Accs [tensor(1808700.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(261925.1719, device='cuda:0', grad_fn=<SumBackward0>), tensor(514015.4688, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 152 Loss 31.058956146240234 Accs [tensor(1628640.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(235886.4531, device='cuda:0', grad_fn=<SumBackward0>), tensor(462803.8125, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 153 Loss 31.044776916503906 Accs [tensor(1499966., device='cuda:0', grad_fn=<SumBackward0>), tensor(217231.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(426399.8750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 154 Loss 31.046838760375977 Accs [tensor(1391602.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(201605.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(395525.8125, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 155 Loss 30.858905792236328 Accs [tensor(1185116., device='cuda:0', grad_fn=<SumBackward0>), tensor(171918.0781, device='cuda:0', grad_fn=<SumBackward0>), tensor(337039.2812, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 156 Loss 30.339792251586914 Accs [tensor(1030983.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(149431.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(293219.9062, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 157 Loss 30.383909225463867 Accs [tensor(926378.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(134259.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(263513.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 158 Loss 30.368669509887695 Accs [tensor(863894.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(125154.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(245756.1875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 159 Loss 30.370811462402344 Accs [tensor(789779.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(114379.7656, device='cuda:0', grad_fn=<SumBackward0>), tensor(224631.0312, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 160 Loss 29.758615493774414 Accs [tensor(730384.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(105749.2188, device='cuda:0', grad_fn=<SumBackward0>), tensor(207651.6719, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 161 Loss 29.836727142333984 Accs [tensor(720161.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(104232.6328, device='cuda:0', grad_fn=<SumBackward0>), tensor(204690.6250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 162 Loss 29.613523483276367 Accs [tensor(684099.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(98992.4062, device='cuda:0', grad_fn=<SumBackward0>), tensor(194417.4844, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 163 Loss 29.833370208740234 Accs [tensor(576860.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(83446.3281, device='cuda:0', grad_fn=<SumBackward0>), tensor(163939.4062, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 164 Loss 29.37738037109375 Accs [tensor(495649.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(71699.2031, device='cuda:0', grad_fn=<SumBackward0>), tensor(140832.9062, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 165 Loss 29.43206787109375 Accs [tensor(471898.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(68302.4219, device='cuda:0', grad_fn=<SumBackward0>), tensor(134195.4062, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 166 Loss 29.44644546508789 Accs [tensor(466558.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(67513.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(132824.2188, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 167 Loss 29.337268829345703 Accs [tensor(468619.7188, device='cuda:0', grad_fn=<SumBackward0>), tensor(67840.4453, device='cuda:0', grad_fn=<SumBackward0>), tensor(133446.9688, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 168 Loss 29.235139846801758 Accs [tensor(503006.3125, device='cuda:0', grad_fn=<SumBackward0>), tensor(72914.2188, device='cuda:0', grad_fn=<SumBackward0>), tensor(143146.0781, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 169 Loss 29.05073356628418 Accs [tensor(524697.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(75989.1641, device='cuda:0', grad_fn=<SumBackward0>), tensor(149651.4688, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 170 Loss 29.209003448486328 Accs [tensor(492080.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(71314.3281, device='cuda:0', grad_fn=<SumBackward0>), tensor(140423.0938, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 171 Loss 29.040613174438477 Accs [tensor(465259.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(67401.9219, device='cuda:0', grad_fn=<SumBackward0>), tensor(132798.2188, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 172 Loss 28.963178634643555 Accs [tensor(460994.9688, device='cuda:0', grad_fn=<SumBackward0>), tensor(66827.2578, device='cuda:0', grad_fn=<SumBackward0>), tensor(131576.0156, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 173 Loss 29.006629943847656 Accs [tensor(428658.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(62185.0117, device='cuda:0', grad_fn=<SumBackward0>), tensor(122325.6875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 174 Loss 28.94537353515625 Accs [tensor(402492.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(58417.7031, device='cuda:0', grad_fn=<SumBackward0>), tensor(114763.8125, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 175 Loss 28.93513298034668 Accs [tensor(417429.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(60529.7109, device='cuda:0', grad_fn=<SumBackward0>), tensor(119054.6719, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 176 Loss 28.83401107788086 Accs [tensor(407520.9688, device='cuda:0', grad_fn=<SumBackward0>), tensor(59110.8008, device='cuda:0', grad_fn=<SumBackward0>), tensor(116370.6953, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 177 Loss 28.860280990600586 Accs [tensor(369063.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(53479.4453, device='cuda:0', grad_fn=<SumBackward0>), tensor(105442.2969, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 178 Loss 28.863311767578125 Accs [tensor(342244.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(49650.8594, device='cuda:0', grad_fn=<SumBackward0>), tensor(97797.8750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 179 Loss 28.753568649291992 Accs [tensor(308029.2188, device='cuda:0', grad_fn=<SumBackward0>), tensor(44653.1523, device='cuda:0', grad_fn=<SumBackward0>), tensor(88024.6875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 180 Loss 28.780719757080078 Accs [tensor(270930.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(39276.8281, device='cuda:0', grad_fn=<SumBackward0>), tensor(77457.1562, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 181 Loss 28.770009994506836 Accs [tensor(270120.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(39160.9453, device='cuda:0', grad_fn=<SumBackward0>), tensor(77263.1719, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 182 Loss 28.719581604003906 Accs [tensor(290162.3438, device='cuda:0', grad_fn=<SumBackward0>), tensor(42087.5938, device='cuda:0', grad_fn=<SumBackward0>), tensor(83024.0156, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 183 Loss 28.705917358398438 Accs [tensor(289627.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(42028., device='cuda:0', grad_fn=<SumBackward0>), tensor(82898.9297, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 184 Loss 28.677064895629883 Accs [tensor(279387.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(40530.5273, device='cuda:0', grad_fn=<SumBackward0>), tensor(80010.8438, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 185 Loss 28.676116943359375 Accs [tensor(266223.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(38643.8203, device='cuda:0', grad_fn=<SumBackward0>), tensor(76251.7812, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 186 Loss 28.635787963867188 Accs [tensor(240298.4062, device='cuda:0', grad_fn=<SumBackward0>), tensor(34936.5547, device='cuda:0', grad_fn=<SumBackward0>), tensor(68763.7578, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 187 Loss 28.600095748901367 Accs [tensor(226409.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(32980.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(64711.3203, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 188 Loss 28.618417739868164 Accs [tensor(232759.2344, device='cuda:0', grad_fn=<SumBackward0>), tensor(33900.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(66594.6875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 189 Loss 28.581707000732422 Accs [tensor(235542.7969, device='cuda:0', grad_fn=<SumBackward0>), tensor(34284.1211, device='cuda:0', grad_fn=<SumBackward0>), tensor(67420.1172, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 190 Loss 28.549152374267578 Accs [tensor(231732.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(33725.4258, device='cuda:0', grad_fn=<SumBackward0>), tensor(66335.3047, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 191 Loss 28.55727195739746 Accs [tensor(229426.5938, device='cuda:0', grad_fn=<SumBackward0>), tensor(33396.4453, device='cuda:0', grad_fn=<SumBackward0>), tensor(65684.0234, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 192 Loss 28.52849769592285 Accs [tensor(217604.6719, device='cuda:0', grad_fn=<SumBackward0>), tensor(31680.1543, device='cuda:0', grad_fn=<SumBackward0>), tensor(62276.1523, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 193 Loss 28.501012802124023 Accs [tensor(214891.9688, device='cuda:0', grad_fn=<SumBackward0>), tensor(31291.7305, device='cuda:0', grad_fn=<SumBackward0>), tensor(61507.0234, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 194 Loss 28.480539321899414 Accs [tensor(224383.0156, device='cuda:0', grad_fn=<SumBackward0>), tensor(32663.6562, device='cuda:0', grad_fn=<SumBackward0>), tensor(64251.9414, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 195 Loss 28.460491180419922 Accs [tensor(232945.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(33949.8555, device='cuda:0', grad_fn=<SumBackward0>), tensor(66757.8359, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 196 Loss 28.44464874267578 Accs [tensor(238189.4688, device='cuda:0', grad_fn=<SumBackward0>), tensor(34721.2031, device='cuda:0', grad_fn=<SumBackward0>), tensor(68288.8047, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 197 Loss 28.431140899658203 Accs [tensor(250101.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(36472.6680, device='cuda:0', grad_fn=<SumBackward0>), tensor(71727.5469, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 198 Loss 28.412187576293945 Accs [tensor(249854.7188, device='cuda:0', grad_fn=<SumBackward0>), tensor(36438.8203, device='cuda:0', grad_fn=<SumBackward0>), tensor(71660.0938, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 199 Loss 28.400461196899414 Accs [tensor(236738.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(34536.3984, device='cuda:0', grad_fn=<SumBackward0>), tensor(67895.9062, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Best accs [tensor(8991.0742, device='cuda:0', grad_fn=<SumBackward0>), tensor(1495.2524, device='cuda:0', grad_fn=<SumBackward0>), tensor(2555.4712, device='cuda:0', grad_fn=<SumBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creates a HeteroGNN model from the previously constructed hetero graph and trains it.\n",
    "\"\"\"\n",
    "\n",
    "best_model = None\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    # Train\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    # Test for the accuracy of the model\n",
    "    accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n",
    "    print(f\"Epoch {epoch} Loss {loss} Accs {accs}\")\n",
    "\n",
    "# Get the accuracy of the best model\n",
    "best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx])\n",
    "\n",
    "print(\"Best accs\", best_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8501], device='cuda:0', grad_fn=<SelectBackward0>) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.], device='cuda:0')\n",
      "tensor([-2.2553], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([-0.8907], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([-2.0342], device='cuda:0', grad_fn=<SelectBackward0>) tensor([12.], device='cuda:0')\n",
      "tensor([-1.7243], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-0.1494], device='cuda:0', grad_fn=<SelectBackward0>) tensor([56.], device='cuda:0')\n",
      "tensor([-2.2864], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([-1.5379], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([0.5765], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([8.4400], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([8.4223], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([8.4919], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([8.4630], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([8.4808], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([8.4601], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([8.4360], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([8.4644], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([8.3521], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([9.5982], device='cuda:0', grad_fn=<SelectBackward0>) tensor([94.], device='cuda:0')\n",
      "tensor([13.7424], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([8.4677], device='cuda:0', grad_fn=<SelectBackward0>) tensor([11.], device='cuda:0')\n",
      "tensor([8.4512], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([8.4652], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([5.7118], device='cuda:0', grad_fn=<SelectBackward0>) tensor([48.], device='cuda:0')\n",
      "tensor([8.4565], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([8.4060], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([8.4509], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([8.4630], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([-1.8033], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([24.0448], device='cuda:0', grad_fn=<SelectBackward0>) tensor([111.], device='cuda:0')\n",
      "tensor([-2.1219], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([-1.1882], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([8.4334], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-2.1716], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-1.5010], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([8.4146], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([-2.0006], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([-2.2447], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([1.1549], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([-2.4760], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([8.4293], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([-1.2785], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([8.4551], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([-2.0785], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([13.9142], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([1.7714], device='cuda:0', grad_fn=<SelectBackward0>) tensor([58.], device='cuda:0')\n",
      "tensor([-1.5513], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-1.9507], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-1.7750], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([-0.8824], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([-2.1158], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-1.5677], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-1.6572], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([-1.5377], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([14.1030], device='cuda:0', grad_fn=<SelectBackward0>) tensor([14.], device='cuda:0')\n",
      "tensor([-1.9474], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([8.4945], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([8.3973], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([14.0665], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([-0.2735], device='cuda:0', grad_fn=<SelectBackward0>) tensor([32.], device='cuda:0')\n",
      "tensor([8.4321], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-0.8989], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([-1.0301], device='cuda:0', grad_fn=<SelectBackward0>) tensor([19.], device='cuda:0')\n",
      "tensor([-1.4847], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([43.5335], device='cuda:0', grad_fn=<SelectBackward0>) tensor([144.], device='cuda:0')\n",
      "tensor([13.3081], device='cuda:0', grad_fn=<SelectBackward0>) tensor([79.], device='cuda:0')\n",
      "tensor([-1.2767], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([-2.2304], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([8.4476], device='cuda:0', grad_fn=<SelectBackward0>) tensor([12.], device='cuda:0')\n",
      "tensor([-1.6753], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([13.7146], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([-2.1559], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([14.3222], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([13.8544], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([8.4742], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-2.2106], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([1.2550], device='cuda:0', grad_fn=<SelectBackward0>) tensor([51.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "preds = best_model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "# mask = preds['event'] > 0\n",
    "# preds['event'][preds['event'] > 0].shape\n",
    "\n",
    "# print(preds['event'][0], hetero_graph.node_target['event'][0]) \n",
    "\n",
    "\n",
    "#print(hetero_graph.node_feature['event'])\n",
    "\n",
    "# for i in range(3000):\n",
    "#     if hetero_graph.node_target['event'][i] != -1: # concepts have node target -1\n",
    "#         print(preds['event'][i], hetero_graph.node_target['event'][i])\n",
    "        \n",
    "    \n",
    "for i in range(1000):    \n",
    "    if hetero_graph.node_target['event'][test_idx['event']][i] != -1:\n",
    "        print(preds['event'][test_idx['event']][i], hetero_graph.node_target['event'][test_idx['event']][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
