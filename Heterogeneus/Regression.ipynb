{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # To simplify implementation, please initialize both self.lin_dst\n",
    "        # and self.lin_src out_features to out_channels\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "        \n",
    "        # Old code from colab, didn't work:\n",
    "        # self.lin_dst = nn.Linear(self.in_channels_src, self.out_channels)\n",
    "        # self.lin_src = nn.Linear(self.in_channels_dst, self.out_channels)\n",
    "        # self.lin_update = nn.Linear(2*self.out_channels, self.out_channels)\n",
    "        \n",
    "        self.lin_dst = nn.Linear(in_channels_dst, out_channels)\n",
    "        self.lin_src = nn.Linear(in_channels_src, out_channels)\n",
    "        self.lin_update = nn.Linear(2 * out_channels, out_channels)\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "    ):\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "        ##########################################\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "        ## Note:\n",
    "        ## 1. Different from what we implemented in Colab 3, we use message_and_aggregate\n",
    "        ## to replace the message and aggregate. The benefit is that we can avoid\n",
    "        ## materializing x_i and x_j, and make the implementation more efficient.\n",
    "        ## 2. To implement efficiently, following PyG documentation is helpful:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html\n",
    "        ## 3. Here edge_index is torch_sparse SparseTensor.\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "        ##########################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~4 lines of code)\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        # print(aggr_out.shape, dst_out.shape)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        # print(aggr_out.shape, )\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "        ##########################################\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "            ############# Your code here #############\n",
    "            ## (~1 line of code)\n",
    "            ## Note:\n",
    "            ## 1. Initialize self.attn_proj here.\n",
    "            ## 2. You should use nn.Sequential for self.attn_proj\n",
    "            ## 3. nn.Linear and nn.Tanh are useful.\n",
    "            ## 4. You can create a vector parameter by using:\n",
    "            ## nn.Linear(some_size, 1, bias=False)\n",
    "            ## 5. The first linear layer should have out_features as args['attn_size']\n",
    "            ## 6. You can assume we only have one \"head\" for the attention.\n",
    "            ## 7. We recommend you to implement the mean aggregation first. After \n",
    "            ## the mean aggregation works well in the training, then you can \n",
    "            ## implement this part.\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "            #########################################\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        # TODO: Implement this function that aggregates all message type results.\n",
    "        # Here, xs is a list of tensors (embeddings) with respect to message \n",
    "        # type aggregation results.\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~2 lines of code)\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "            ##########################################\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~10 lines of code)\n",
    "            ## Note:\n",
    "            ## 1. Store the value of attention alpha (as a numpy array) to self.alpha,\n",
    "            ## which has the shape (len(xs), ) self.alpha will be not be used \n",
    "            ## to backpropagate etc. in the model. We will use it to see how much \n",
    "            ## attention the layer pays on different message types.\n",
    "            ## 2. torch.softmax and torch.cat are useful.\n",
    "            ## 3. You might need to reshape the tensors by using the \n",
    "            ## `view()` function https://pytorch.org/docs/stable/tensor_view.html\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out\n",
    "            ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    # TODO: Implement this function that returns a dictionary of `HeteroGNNConv` \n",
    "    # layers where the keys are message types. `hetero_graph` is deepsnap `HeteroGraph`\n",
    "    # object and the `conv` is the `HeteroGNNConv`.\n",
    "\n",
    "    convs = {}\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## (~9 lines of code)\n",
    "\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    ##########################################\n",
    "    \n",
    "    return convs\n",
    "\n",
    "# def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "#     convs = {}\n",
    "\n",
    "#     # Retrieve the number of features for all node types\n",
    "#     node_feature_dims = {\n",
    "#         node_type: hetero_graph.num_node_features(node_type)\n",
    "#         for node_type in hetero_graph.node_types\n",
    "#     }\n",
    "\n",
    "#     for message_type in hetero_graph.message_types:\n",
    "#         src_type, _, dst_type = message_type\n",
    "#         if first_layer:\n",
    "#             # Initialize input channels based on the number of features for each node type\n",
    "#             in_channels_src = node_feature_dims[src_type]\n",
    "#             in_channels_dst = node_feature_dims[dst_type]\n",
    "#         else:\n",
    "#             # For subsequent layers, use the hidden_size as the dimension\n",
    "#             in_channels_src = hidden_size\n",
    "#             in_channels_dst = hidden_size\n",
    "#         out_channels = hidden_size\n",
    "#         convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    \n",
    "#     return convs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, num_layers, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        # self.convs1 = None\n",
    "        # self.convs2 = None\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.convs = None\n",
    "        self.bns = [nn.ModuleDict() for _ in range(num_layers)]\n",
    "        self.relus = [nn.ModuleDict() for _ in range(num_layers)]\n",
    "        \n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~10 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For self.convs1 and self.convs2, call generate_convs at first and then\n",
    "        ## pass the returned dictionary of `HeteroGNNConv` to `HeteroGNNWrapperConv`.\n",
    "        ## 2. For self.bns, self.relus and self.post_mps, the keys are node_types.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.node_types` will be helpful.\n",
    "        ## 3. Initialize all batchnorms to torch.nn.BatchNorm1d(hidden_size, eps=1.0).\n",
    "        ## 4. Initialize all relus to nn.LeakyReLU().\n",
    "        ## 5. For self.post_mps, each value in the ModuleDict is a linear layer \n",
    "        ## where the `out_features` is the number of classes for that node type.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.num_node_labels(node_type)` will be\n",
    "        ## useful.\n",
    "\n",
    "        # self.convs = [HeteroGNNWrapperConv(generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), args, self.aggr),\n",
    "        #     *[HeteroGNNWrapperConv(generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), args, self.aggr)\n",
    "        #         for _ in range(self.num_layers - 1)]\n",
    "        # ]\n",
    "        \n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            # for i in range(num_layers):\n",
    "            #     self.bns[i][node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            #     self.relus[i][node_type] = nn.LeakyReLU()\n",
    "            \n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            \n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            #self.post_mps[node_type] = nn.Linear(self.hidden_size, hetero_graph.num_node_labels(node_type))\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "            \n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(self, node_feature, edge_index):\n",
    "        # TODO: Implement the forward function. Notice that `node_feature` is \n",
    "        # a dictionary of tensors where keys are node types and values are \n",
    "        # corresponding feature tensors. The `edge_index` is a dictionary of \n",
    "        # tensors where keys are message types and values are corresponding\n",
    "        # edge index tensors (with respect to each message type).\n",
    "        \n",
    "        \n",
    "        x = node_feature\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~7 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. `deepsnap.hetero_gnn.forward_op` can be helpful.\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        # for conv, bn, relu in zip(self.convs, self.bns, self.relus):\n",
    "        #     x = conv(x, edge_index)\n",
    "        #     x = forward_op(x, bn)\n",
    "        #     x = forward_op(x, relu)\n",
    "        \n",
    "        x = forward_op(x, self.fc)\n",
    "        #print(\"X\", x)\n",
    "\n",
    "        ##########################################\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        loss = 0\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        def mape_loss(y_pred, y_true):\n",
    "            \"\"\"\n",
    "            Calculate the Mean Absolute Percentage Error (MAPE) for PyTorch tensors.\n",
    "\n",
    "            Args:\n",
    "            y_true (Tensor): True values.\n",
    "            y_pred (Tensor): Predicted values.\n",
    "\n",
    "            Returns:\n",
    "            Tensor: MAPE loss.\n",
    "            \"\"\"\n",
    "            # Ensuring y_true and y_pred are on the same device\n",
    "            #y_pred = y_pred.to(y_true.device)\n",
    "\n",
    "            # Avoiding division by zero\n",
    "            non_zero_mask = y_true != 0\n",
    "\n",
    "            # Calculating MAPE\n",
    "            loss = torch.mean(torch.abs((torch.masked_select(y_true, non_zero_mask) - torch.masked_select(y_pred, non_zero_mask) / torch.masked_select(y_true, non_zero_mask)))) * 100\n",
    "    \n",
    "            return loss\n",
    "\n",
    "\n",
    "        #loss_func = mape_loss\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For each node type in preds, accumulate computed loss to `loss`\n",
    "        ## 2. Loss need to be computed with respect to the given \n",
    "        \n",
    "        # print(\"Preds\", preds)\n",
    "        # print(\"Y\", y)\n",
    "        # print(\"Indices\", indices)\n",
    "         \n",
    "        mask = y['event'][indices['event'], 0] != -1\n",
    "        non_zero_idx = torch.masked_select(indices['event'], mask)\n",
    "                \n",
    "        loss += loss_func(preds['event'][non_zero_idx], y['event'][non_zero_idx, 0])\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    loss = None\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## Note:\n",
    "    ## 1. `deepsnap.hetero_graph.HeteroGraph.node_label` is useful\n",
    "    ## 2. Compute the loss here\n",
    "    \n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "    ##########################################\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(model, graph, indices, best_model=None, best_val=0):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        \n",
    "        #print(\"Index\", index)\n",
    "        #print(\"Preds\", preds['event'])\n",
    "\n",
    "        idx = index['event']\n",
    "\n",
    "        L1 = torch.sum(torch.abs(preds['event'][idx] - graph.node_target['event'][idx]))\n",
    "        \n",
    "        accs.append(L1)\n",
    "        #print(\"ACC\", s)\n",
    "\n",
    "        #pred = preds['event'][idx]\n",
    "        \n",
    "        # num_node_types = 0\n",
    "        # micro = 0\n",
    "        # macro = 0\n",
    "\n",
    "        # for node_type in preds:\n",
    "        #     idx = index[node_type]\n",
    "        #     pred = preds[node_type][idx]\n",
    "        #     pred = pred.max(1)[1]\n",
    "        #     label_np = graph.node_label[node_type][idx].cpu().numpy()\n",
    "        #     pred_np = pred.cpu().numpy()\n",
    "        #     micro = f1_score(label_np, pred_np, average='micro')\n",
    "        #     macro = f1_score(label_np, pred_np, average='macro')\n",
    "        #     num_node_types += 1\n",
    "        # Averaging f1 score might not make sense, but in our example we only\n",
    "        # have one node type\n",
    "        # micro /= num_node_types\n",
    "        # macro /= num_node_types\n",
    "        #accs.append((micro, macro))\n",
    "    if accs[1] < best_val:\n",
    "        best_val = accs[1]\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    return accs, best_model, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change the following parameters\n",
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 64,\n",
    "    'epochs': 200,\n",
    "    'weight_decay': 1e-4,\n",
    "    'lr': 0.201,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [1, 1, 1],   # event 0\n",
    "                [2, 2, 2]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([[50], [2000]], dtype=torch.float32),\n",
    "    # \"concept\": torch.tensor([[0], [0]], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}\n",
    "\n",
    "# Testing\n",
    "hetero_graph = HeteroGraph(\n",
    "    node_feature=S_node_feature,\n",
    "    node_target=S_node_targets,\n",
    "    edge_index=S_edge_index\n",
    ")\n",
    "\n",
    "train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./1_concepts_similar_llm.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "    # Convert to directed graph for compatibility with Deepsnap\n",
    "    G = G.to_directed()\n",
    "    \n",
    "\n",
    "hetero_graph = HeteroGraph(G, netlib=nx, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 770\n",
      "\t Feature 1\n",
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 770\n",
      "\t Feature 770\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 1\n",
      "\t Feature 770\n",
      "KEY ('event', 'related', 'concept') <class 'tuple'>\n",
      "KEY NUMS ('event', 'related', 'concept') 8487 8729\n",
      "MAX EDGES tensor(8448) tensor(8728) 8487 8729\n",
      "KEY ('event', 'similar', 'event') <class 'tuple'>\n",
      "KEY NUMS ('event', 'similar', 'event') 8487 8487\n",
      "MAX EDGES tensor(8486) tensor(8486) 8487 8487\n",
      "KEY ('concept', 'related', 'event') <class 'tuple'>\n",
      "KEY NUMS ('concept', 'related', 'event') 8729 8487\n",
      "MAX EDGES tensor(8728) tensor(8448) 8729 8487\n"
     ]
    }
   ],
   "source": [
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "\n",
    "# Node feature and node label to device\n",
    "\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "\n",
    "# edge_index1 = hetero_graph.edge_index[(\"concept\", \"related\", \"event\")]\n",
    "# edge_index1 = hetero_graph.edge_index[(\"event\", \"related\", \"concept\")]\n",
    "\n",
    "# Edge_index to sparse tensor and to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key, type(key))\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    \n",
    "    \n",
    "    # TODO: remove quick fix\n",
    "    # if key == ('event', 'related', 'concept'):\n",
    "    #     edge_index = hetero_graph.edge_index[(\"concept\", \"related\", \"event\")]\n",
    "    # else:\n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max(), hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
    "    \n",
    "\n",
    "\n",
    "for key in hetero_graph.node_target:\n",
    "    hetero_graph.node_target[key] = hetero_graph.node_target[key].to(args['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5940])\n",
      "torch.Size([1698])\n",
      "torch.Size([849])\n"
     ]
    }
   ],
   "source": [
    "nEvents = hetero_graph.num_nodes(\"event\")\n",
    "nConcepts = hetero_graph.num_nodes(\"concept\")\n",
    "\n",
    "s1 = 0.7\n",
    "s2 = 0.8\n",
    "\n",
    "train_idx = {   \"event\": torch.tensor(range(0, int(nEvents * s1))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(0, int(nConcepts * s1))).to(args[\"device\"])\n",
    "            }\n",
    "val_idx = {   \"event\": torch.tensor(range(int(nEvents * s1), int(nEvents * s2))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s1), int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "test_idx = {   \"event\": torch.tensor(range(int(nEvents * s2), nEvents)).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s2), nConcepts)).to(args[\"device\"])\n",
    "            }\n",
    "\n",
    "print(train_idx[\"event\"].shape)\n",
    "print(test_idx[\"event\"].shape)\n",
    "print(val_idx[\"event\"].shape)\n",
    "\n",
    "# dataset = deepsnap.dataset.GraphDataset([hetero_graph], task='node')\n",
    "\n",
    "# dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
    "# datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([421])) that is different to the input size (torch.Size([421, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 1979.5784912109375 Accs [tensor(75566312., device='cuda:0', grad_fn=<SumBackward0>), tensor(10772876., device='cuda:0', grad_fn=<SumBackward0>), tensor(21423520., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 1 Loss 2143.34619140625 Accs [tensor(7.0874e+08, device='cuda:0', grad_fn=<SumBackward0>), tensor(1.0187e+08, device='cuda:0', grad_fn=<SumBackward0>), tensor(2.0182e+08, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 2 Loss 1757.8538818359375 Accs [tensor(3048727., device='cuda:0', grad_fn=<SumBackward0>), tensor(422222.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(860904., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 3 Loss 1900.925537109375 Accs [tensor(2984781.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(429750.4062, device='cuda:0', grad_fn=<SumBackward0>), tensor(845222.1250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 4 Loss 1845.0560302734375 Accs [tensor(2073996.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(298066.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(581496.5625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 5 Loss 1790.582763671875 Accs [tensor(2584236.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(370503.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(720973.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 6 Loss 1724.9798583984375 Accs [tensor(2722519., device='cuda:0', grad_fn=<SumBackward0>), tensor(389927.5938, device='cuda:0', grad_fn=<SumBackward0>), tensor(757974.5625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 7 Loss 1764.4754638671875 Accs [tensor(2120471.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(303642.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(590005., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 8 Loss 1778.7064208984375 Accs [tensor(1370385.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(196138.7031, device='cuda:0', grad_fn=<SumBackward0>), tensor(381261.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 9 Loss 1743.7333984375 Accs [tensor(791421.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(113095.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(220258.3750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 10 Loss 1726.7122802734375 Accs [tensor(436814.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(62212.2383, device='cuda:0', grad_fn=<SumBackward0>), tensor(121648.8750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 11 Loss 1726.7303466796875 Accs [tensor(243656.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(34451.6680, device='cuda:0', grad_fn=<SumBackward0>), tensor(67923.7422, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 12 Loss 1733.4251708984375 Accs [tensor(143428.1406, device='cuda:0', grad_fn=<SumBackward0>), tensor(20225.2461, device='cuda:0', grad_fn=<SumBackward0>), tensor(40484.8828, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 13 Loss 1736.8531494140625 Accs [tensor(99876.7812, device='cuda:0', grad_fn=<SumBackward0>), tensor(14281.8047, device='cuda:0', grad_fn=<SumBackward0>), tensor(28413.7031, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 14 Loss 1734.78564453125 Accs [tensor(86165.1406, device='cuda:0', grad_fn=<SumBackward0>), tensor(12324.1621, device='cuda:0', grad_fn=<SumBackward0>), tensor(24580.4609, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 15 Loss 1730.7569580078125 Accs [tensor(85748.3516, device='cuda:0', grad_fn=<SumBackward0>), tensor(12191.3018, device='cuda:0', grad_fn=<SumBackward0>), tensor(24428.7422, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 16 Loss 1724.9573974609375 Accs [tensor(89098.3438, device='cuda:0', grad_fn=<SumBackward0>), tensor(12598.9531, device='cuda:0', grad_fn=<SumBackward0>), tensor(25294.5469, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 17 Loss 1719.212158203125 Accs [tensor(88322.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(12477.2363, device='cuda:0', grad_fn=<SumBackward0>), tensor(25013.5234, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 18 Loss 1717.8897705078125 Accs [tensor(79799.0234, device='cuda:0', grad_fn=<SumBackward0>), tensor(11337.8857, device='cuda:0', grad_fn=<SumBackward0>), tensor(22604.9512, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 19 Loss 1719.46240234375 Accs [tensor(62877.4062, device='cuda:0', grad_fn=<SumBackward0>), tensor(9085.2549, device='cuda:0', grad_fn=<SumBackward0>), tensor(17890.2031, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 20 Loss 1720.0213623046875 Accs [tensor(43064.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(6401.4209, device='cuda:0', grad_fn=<SumBackward0>), tensor(12301., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 21 Loss 1719.931396484375 Accs [tensor(29105.4473, device='cuda:0', grad_fn=<SumBackward0>), tensor(4510.7734, device='cuda:0', grad_fn=<SumBackward0>), tensor(8296.3672, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 22 Loss 1719.0244140625 Accs [tensor(22589.2266, device='cuda:0', grad_fn=<SumBackward0>), tensor(3598.7537, device='cuda:0', grad_fn=<SumBackward0>), tensor(6467.9932, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 23 Loss 1716.7506103515625 Accs [tensor(20490.0039, device='cuda:0', grad_fn=<SumBackward0>), tensor(3267.6597, device='cuda:0', grad_fn=<SumBackward0>), tensor(5897.3213, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 24 Loss 1715.5419921875 Accs [tensor(20650.1660, device='cuda:0', grad_fn=<SumBackward0>), tensor(3256.1584, device='cuda:0', grad_fn=<SumBackward0>), tensor(5970.6001, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 25 Loss 1717.24658203125 Accs [tensor(20548.4023, device='cuda:0', grad_fn=<SumBackward0>), tensor(3224.2461, device='cuda:0', grad_fn=<SumBackward0>), tensor(5973.3574, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 26 Loss 1718.7960205078125 Accs [tensor(19378.0664, device='cuda:0', grad_fn=<SumBackward0>), tensor(3056.1558, device='cuda:0', grad_fn=<SumBackward0>), tensor(5682.7715, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 27 Loss 1717.114501953125 Accs [tensor(18125.6699, device='cuda:0', grad_fn=<SumBackward0>), tensor(2881.6646, device='cuda:0', grad_fn=<SumBackward0>), tensor(5331.8071, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 28 Loss 1715.4451904296875 Accs [tensor(18066.7852, device='cuda:0', grad_fn=<SumBackward0>), tensor(2894.2490, device='cuda:0', grad_fn=<SumBackward0>), tensor(5288.4521, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 29 Loss 1715.9154052734375 Accs [tensor(19408.1055, device='cuda:0', grad_fn=<SumBackward0>), tensor(3114.0332, device='cuda:0', grad_fn=<SumBackward0>), tensor(5675.2622, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 30 Loss 1716.4859619140625 Accs [tensor(21471.4258, device='cuda:0', grad_fn=<SumBackward0>), tensor(3434.9341, device='cuda:0', grad_fn=<SumBackward0>), tensor(6258.4160, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 31 Loss 1716.5052490234375 Accs [tensor(22774.2656, device='cuda:0', grad_fn=<SumBackward0>), tensor(3619.7422, device='cuda:0', grad_fn=<SumBackward0>), tensor(6623.3096, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 32 Loss 1716.67578125 Accs [tensor(22487.0039, device='cuda:0', grad_fn=<SumBackward0>), tensor(3559.4082, device='cuda:0', grad_fn=<SumBackward0>), tensor(6560.1484, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 33 Loss 1716.2459716796875 Accs [tensor(21151.0391, device='cuda:0', grad_fn=<SumBackward0>), tensor(3327.9802, device='cuda:0', grad_fn=<SumBackward0>), tensor(6165.3320, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 34 Loss 1715.2562255859375 Accs [tensor(20165.5703, device='cuda:0', grad_fn=<SumBackward0>), tensor(3222.5176, device='cuda:0', grad_fn=<SumBackward0>), tensor(5882.2749, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 35 Loss 1715.2657470703125 Accs [tensor(20681.5918, device='cuda:0', grad_fn=<SumBackward0>), tensor(3309.8403, device='cuda:0', grad_fn=<SumBackward0>), tensor(5987.5420, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 36 Loss 1716.071533203125 Accs [tensor(20828.9727, device='cuda:0', grad_fn=<SumBackward0>), tensor(3327.7681, device='cuda:0', grad_fn=<SumBackward0>), tensor(6036.4102, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 37 Loss 1716.0474853515625 Accs [tensor(20821.3555, device='cuda:0', grad_fn=<SumBackward0>), tensor(3309.3848, device='cuda:0', grad_fn=<SumBackward0>), tensor(6076.0552, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 38 Loss 1715.5072021484375 Accs [tensor(21265.5117, device='cuda:0', grad_fn=<SumBackward0>), tensor(3345.5127, device='cuda:0', grad_fn=<SumBackward0>), tensor(6220.4189, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 39 Loss 1715.3004150390625 Accs [tensor(21604.5684, device='cuda:0', grad_fn=<SumBackward0>), tensor(3395.5740, device='cuda:0', grad_fn=<SumBackward0>), tensor(6315.9014, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 40 Loss 1715.2935791015625 Accs [tensor(21657.4141, device='cuda:0', grad_fn=<SumBackward0>), tensor(3409.8567, device='cuda:0', grad_fn=<SumBackward0>), tensor(6330.2021, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 41 Loss 1715.3997802734375 Accs [tensor(21776.1562, device='cuda:0', grad_fn=<SumBackward0>), tensor(3430.9927, device='cuda:0', grad_fn=<SumBackward0>), tensor(6368.8496, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 42 Loss 1715.5958251953125 Accs [tensor(22205.4512, device='cuda:0', grad_fn=<SumBackward0>), tensor(3481.7510, device='cuda:0', grad_fn=<SumBackward0>), tensor(6496.4512, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 43 Loss 1715.3426513671875 Accs [tensor(23015.3574, device='cuda:0', grad_fn=<SumBackward0>), tensor(3568.5227, device='cuda:0', grad_fn=<SumBackward0>), tensor(6730.3047, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 44 Loss 1715.072998046875 Accs [tensor(23763.1309, device='cuda:0', grad_fn=<SumBackward0>), tensor(3663.4292, device='cuda:0', grad_fn=<SumBackward0>), tensor(6947.8037, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 45 Loss 1715.4442138671875 Accs [tensor(24303.4023, device='cuda:0', grad_fn=<SumBackward0>), tensor(3758.1138, device='cuda:0', grad_fn=<SumBackward0>), tensor(7102.5459, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 46 Loss 1715.2178955078125 Accs [tensor(25129.8867, device='cuda:0', grad_fn=<SumBackward0>), tensor(3886.2891, device='cuda:0', grad_fn=<SumBackward0>), tensor(7329.6035, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 47 Loss 1715.2186279296875 Accs [tensor(26404.5273, device='cuda:0', grad_fn=<SumBackward0>), tensor(4057.9883, device='cuda:0', grad_fn=<SumBackward0>), tensor(7684.3042, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 48 Loss 1715.061279296875 Accs [tensor(27807.4062, device='cuda:0', grad_fn=<SumBackward0>), tensor(4253.8711, device='cuda:0', grad_fn=<SumBackward0>), tensor(8081.1895, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 49 Loss 1715.115234375 Accs [tensor(28919.6465, device='cuda:0', grad_fn=<SumBackward0>), tensor(4412.6455, device='cuda:0', grad_fn=<SumBackward0>), tensor(8390.6064, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 50 Loss 1715.19775390625 Accs [tensor(29624.5332, device='cuda:0', grad_fn=<SumBackward0>), tensor(4520.3711, device='cuda:0', grad_fn=<SumBackward0>), tensor(8588.1191, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 51 Loss 1715.058349609375 Accs [tensor(30213.6758, device='cuda:0', grad_fn=<SumBackward0>), tensor(4608.3770, device='cuda:0', grad_fn=<SumBackward0>), tensor(8753.8047, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 52 Loss 1715.0975341796875 Accs [tensor(30964.9023, device='cuda:0', grad_fn=<SumBackward0>), tensor(4712.5957, device='cuda:0', grad_fn=<SumBackward0>), tensor(8969.0176, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 53 Loss 1715.081298828125 Accs [tensor(31872.8086, device='cuda:0', grad_fn=<SumBackward0>), tensor(4834.4331, device='cuda:0', grad_fn=<SumBackward0>), tensor(9230.2979, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 54 Loss 1715.0496826171875 Accs [tensor(32707.7363, device='cuda:0', grad_fn=<SumBackward0>), tensor(4948.0264, device='cuda:0', grad_fn=<SumBackward0>), tensor(9467.7363, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 55 Loss 1715.1231689453125 Accs [tensor(33302.8828, device='cuda:0', grad_fn=<SumBackward0>), tensor(5035.9229, device='cuda:0', grad_fn=<SumBackward0>), tensor(9636.4619, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 56 Loss 1715.0379638671875 Accs [tensor(33757.3242, device='cuda:0', grad_fn=<SumBackward0>), tensor(5107.1299, device='cuda:0', grad_fn=<SumBackward0>), tensor(9763.9453, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 57 Loss 1715.050048828125 Accs [tensor(34304.9219, device='cuda:0', grad_fn=<SumBackward0>), tensor(5185.8521, device='cuda:0', grad_fn=<SumBackward0>), tensor(9918.4238, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 58 Loss 1715.08056640625 Accs [tensor(34955.9453, device='cuda:0', grad_fn=<SumBackward0>), tensor(5272.4766, device='cuda:0', grad_fn=<SumBackward0>), tensor(10103.2627, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 59 Loss 1715.0330810546875 Accs [tensor(35483.5117, device='cuda:0', grad_fn=<SumBackward0>), tensor(5343.9141, device='cuda:0', grad_fn=<SumBackward0>), tensor(10253.3027, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 60 Loss 1715.067138671875 Accs [tensor(35754.4961, device='cuda:0', grad_fn=<SumBackward0>), tensor(5382.0234, device='cuda:0', grad_fn=<SumBackward0>), tensor(10330.6172, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 61 Loss 1715.0224609375 Accs [tensor(35918.2812, device='cuda:0', grad_fn=<SumBackward0>), tensor(5408.6436, device='cuda:0', grad_fn=<SumBackward0>), tensor(10377.3457, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 62 Loss 1715.0361328125 Accs [tensor(36210.5195, device='cuda:0', grad_fn=<SumBackward0>), tensor(5450.5171, device='cuda:0', grad_fn=<SumBackward0>), tensor(10460.0459, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 63 Loss 1715.0518798828125 Accs [tensor(36663.7031, device='cuda:0', grad_fn=<SumBackward0>), tensor(5511.6201, device='cuda:0', grad_fn=<SumBackward0>), tensor(10587.8936, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 64 Loss 1715.016845703125 Accs [tensor(37087.7422, device='cuda:0', grad_fn=<SumBackward0>), tensor(5571.7373, device='cuda:0', grad_fn=<SumBackward0>), tensor(10707.1807, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 65 Loss 1715.040771484375 Accs [tensor(37331.1523, device='cuda:0', grad_fn=<SumBackward0>), tensor(5608.4873, device='cuda:0', grad_fn=<SumBackward0>), tensor(10775.1240, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 66 Loss 1715.020263671875 Accs [tensor(37470.2422, device='cuda:0', grad_fn=<SumBackward0>), tensor(5630.8467, device='cuda:0', grad_fn=<SumBackward0>), tensor(10813.7891, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 67 Loss 1715.0303955078125 Accs [tensor(37680.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(5661.3276, device='cuda:0', grad_fn=<SumBackward0>), tensor(10873.1807, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 68 Loss 1715.0338134765625 Accs [tensor(37989.5391, device='cuda:0', grad_fn=<SumBackward0>), tensor(5702.4756, device='cuda:0', grad_fn=<SumBackward0>), tensor(10960.6807, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 69 Loss 1715.011962890625 Accs [tensor(38252.0273, device='cuda:0', grad_fn=<SumBackward0>), tensor(5738.2559, device='cuda:0', grad_fn=<SumBackward0>), tensor(11035.1611, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 70 Loss 1715.0296630859375 Accs [tensor(38376.5195, device='cuda:0', grad_fn=<SumBackward0>), tensor(5757.3359, device='cuda:0', grad_fn=<SumBackward0>), tensor(11070.2900, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 71 Loss 1715.015869140625 Accs [tensor(38463.6094, device='cuda:0', grad_fn=<SumBackward0>), tensor(5773.5947, device='cuda:0', grad_fn=<SumBackward0>), tensor(11094.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 72 Loss 1715.0224609375 Accs [tensor(38660.9336, device='cuda:0', grad_fn=<SumBackward0>), tensor(5802.8779, device='cuda:0', grad_fn=<SumBackward0>), tensor(11149.7227, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 73 Loss 1715.017578125 Accs [tensor(38940.0547, device='cuda:0', grad_fn=<SumBackward0>), tensor(5841.2749, device='cuda:0', grad_fn=<SumBackward0>), tensor(11228.0859, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 74 Loss 1715.0115966796875 Accs [tensor(39130.7148, device='cuda:0', grad_fn=<SumBackward0>), tensor(5868.5513, device='cuda:0', grad_fn=<SumBackward0>), tensor(11281.5781, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 75 Loss 1715.0224609375 Accs [tensor(39161.9727, device='cuda:0', grad_fn=<SumBackward0>), tensor(5875.5703, device='cuda:0', grad_fn=<SumBackward0>), tensor(11290.1934, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 76 Loss 1715.01123046875 Accs [tensor(39155.2031, device='cuda:0', grad_fn=<SumBackward0>), tensor(5877.1201, device='cuda:0', grad_fn=<SumBackward0>), tensor(11288.3398, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 77 Loss 1715.017333984375 Accs [tensor(39241.4727, device='cuda:0', grad_fn=<SumBackward0>), tensor(5888.8799, device='cuda:0', grad_fn=<SumBackward0>), tensor(11312.9502, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 78 Loss 1715.01123046875 Accs [tensor(39378.1523, device='cuda:0', grad_fn=<SumBackward0>), tensor(5906.6440, device='cuda:0', grad_fn=<SumBackward0>), tensor(11351.7988, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 79 Loss 1715.0140380859375 Accs [tensor(39436.4180, device='cuda:0', grad_fn=<SumBackward0>), tensor(5915.6670, device='cuda:0', grad_fn=<SumBackward0>), tensor(11368.1689, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 80 Loss 1715.0140380859375 Accs [tensor(39412.0859, device='cuda:0', grad_fn=<SumBackward0>), tensor(5915.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(11360.6035, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 81 Loss 1715.0086669921875 Accs [tensor(39426.6719, device='cuda:0', grad_fn=<SumBackward0>), tensor(5919.0566, device='cuda:0', grad_fn=<SumBackward0>), tensor(11363.8096, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 82 Loss 1715.013427734375 Accs [tensor(39523.8047, device='cuda:0', grad_fn=<SumBackward0>), tensor(5932.6133, device='cuda:0', grad_fn=<SumBackward0>), tensor(11391.7207, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 83 Loss 1715.0086669921875 Accs [tensor(39605.6289, device='cuda:0', grad_fn=<SumBackward0>), tensor(5943.5898, device='cuda:0', grad_fn=<SumBackward0>), tensor(11415.1094, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 84 Loss 1715.0126953125 Accs [tensor(39586.1172, device='cuda:0', grad_fn=<SumBackward0>), tensor(5941.7783, device='cuda:0', grad_fn=<SumBackward0>), tensor(11408.9395, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 85 Loss 1715.0079345703125 Accs [tensor(39534.8164, device='cuda:0', grad_fn=<SumBackward0>), tensor(5935.9087, device='cuda:0', grad_fn=<SumBackward0>), tensor(11393.0928, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 86 Loss 1715.0101318359375 Accs [tensor(39565.8516, device='cuda:0', grad_fn=<SumBackward0>), tensor(5940.6025, device='cuda:0', grad_fn=<SumBackward0>), tensor(11401.5684, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 87 Loss 1715.0096435546875 Accs [tensor(39665.3438, device='cuda:0', grad_fn=<SumBackward0>), tensor(5954.3027, device='cuda:0', grad_fn=<SumBackward0>), tensor(11430.1875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 88 Loss 1715.00830078125 Accs [tensor(39721.7266, device='cuda:0', grad_fn=<SumBackward0>), tensor(5962.9668, device='cuda:0', grad_fn=<SumBackward0>), tensor(11445.8574, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 89 Loss 1715.0089111328125 Accs [tensor(39705.3906, device='cuda:0', grad_fn=<SumBackward0>), tensor(5962.4922, device='cuda:0', grad_fn=<SumBackward0>), tensor(11439.9473, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 90 Loss 1715.0072021484375 Accs [tensor(39703.0703, device='cuda:0', grad_fn=<SumBackward0>), tensor(5963.3311, device='cuda:0', grad_fn=<SumBackward0>), tensor(11438.7715, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 91 Loss 1715.00927734375 Accs [tensor(39756.5781, device='cuda:0', grad_fn=<SumBackward0>), tensor(5970.5547, device='cuda:0', grad_fn=<SumBackward0>), tensor(11454.0508, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 92 Loss 1715.00634765625 Accs [tensor(39798.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(5976.1006, device='cuda:0', grad_fn=<SumBackward0>), tensor(11465.9688, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 93 Loss 1715.0078125 Accs [tensor(39770.5234, device='cuda:0', grad_fn=<SumBackward0>), tensor(5973.9336, device='cuda:0', grad_fn=<SumBackward0>), tensor(11457.8770, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 94 Loss 1715.00634765625 Accs [tensor(39728.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(5970.5044, device='cuda:0', grad_fn=<SumBackward0>), tensor(11445.6211, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 95 Loss 1715.0072021484375 Accs [tensor(39746.9727, device='cuda:0', grad_fn=<SumBackward0>), tensor(5973.8105, device='cuda:0', grad_fn=<SumBackward0>), tensor(11450.6211, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 96 Loss 1715.0059814453125 Accs [tensor(39791.5508, device='cuda:0', grad_fn=<SumBackward0>), tensor(5979.7373, device='cuda:0', grad_fn=<SumBackward0>), tensor(11463.2705, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 97 Loss 1715.006103515625 Accs [tensor(39781.5195, device='cuda:0', grad_fn=<SumBackward0>), tensor(5979.8018, device='cuda:0', grad_fn=<SumBackward0>), tensor(11460.4668, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 98 Loss 1715.0059814453125 Accs [tensor(39725.0195, device='cuda:0', grad_fn=<SumBackward0>), tensor(5975.9478, device='cuda:0', grad_fn=<SumBackward0>), tensor(11445.6582, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 99 Loss 1715.00537109375 Accs [tensor(39697.4062, device='cuda:0', grad_fn=<SumBackward0>), tensor(5974.4463, device='cuda:0', grad_fn=<SumBackward0>), tensor(11438.7822, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 100 Loss 1715.00537109375 Accs [tensor(39711.6172, device='cuda:0', grad_fn=<SumBackward0>), tensor(5976.5859, device='cuda:0', grad_fn=<SumBackward0>), tensor(11443.1768, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 101 Loss 1715.0047607421875 Accs [tensor(39707.8086, device='cuda:0', grad_fn=<SumBackward0>), tensor(5977.4985, device='cuda:0', grad_fn=<SumBackward0>), tensor(11442.6523, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 102 Loss 1715.0052490234375 Accs [tensor(39674.4453, device='cuda:0', grad_fn=<SumBackward0>), tensor(5975.7651, device='cuda:0', grad_fn=<SumBackward0>), tensor(11433.1875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 103 Loss 1715.00439453125 Accs [tensor(39661.4648, device='cuda:0', grad_fn=<SumBackward0>), tensor(5975.8862, device='cuda:0', grad_fn=<SumBackward0>), tensor(11429.3828, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 104 Loss 1715.0047607421875 Accs [tensor(39684.2188, device='cuda:0', grad_fn=<SumBackward0>), tensor(5979.4395, device='cuda:0', grad_fn=<SumBackward0>), tensor(11435.8047, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 105 Loss 1715.0040283203125 Accs [tensor(39696.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(5981.7900, device='cuda:0', grad_fn=<SumBackward0>), tensor(11439.2627, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 106 Loss 1715.00439453125 Accs [tensor(39672.6328, device='cuda:0', grad_fn=<SumBackward0>), tensor(5980.3774, device='cuda:0', grad_fn=<SumBackward0>), tensor(11432.1426, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 107 Loss 1715.003662109375 Accs [tensor(39655.1797, device='cuda:0', grad_fn=<SumBackward0>), tensor(5980.3608, device='cuda:0', grad_fn=<SumBackward0>), tensor(11426.2705, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 108 Loss 1715.0037841796875 Accs [tensor(39670.3047, device='cuda:0', grad_fn=<SumBackward0>), tensor(5983.3345, device='cuda:0', grad_fn=<SumBackward0>), tensor(11430.2314, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 109 Loss 1715.0032958984375 Accs [tensor(39687.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(5986.5566, device='cuda:0', grad_fn=<SumBackward0>), tensor(11434.8555, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 110 Loss 1715.0032958984375 Accs [tensor(39676.7188, device='cuda:0', grad_fn=<SumBackward0>), tensor(5987.7251, device='cuda:0', grad_fn=<SumBackward0>), tensor(11430.5488, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 111 Loss 1715.0025634765625 Accs [tensor(39656.6523, device='cuda:0', grad_fn=<SumBackward0>), tensor(5988.1084, device='cuda:0', grad_fn=<SumBackward0>), tensor(11423.3018, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 112 Loss 1715.0018310546875 Accs [tensor(39656.3359, device='cuda:0', grad_fn=<SumBackward0>), tensor(5989.6758, device='cuda:0', grad_fn=<SumBackward0>), tensor(11422.1738, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 113 Loss 1714.99951171875 Accs [tensor(39678.3984, device='cuda:0', grad_fn=<SumBackward0>), tensor(5992.8149, device='cuda:0', grad_fn=<SumBackward0>), tensor(11428.3447, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 114 Loss 1714.9984130859375 Accs [tensor(39702.1719, device='cuda:0', grad_fn=<SumBackward0>), tensor(5996.2046, device='cuda:0', grad_fn=<SumBackward0>), tensor(11435.1660, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 115 Loss 1714.9981689453125 Accs [tensor(39713.5430, device='cuda:0', grad_fn=<SumBackward0>), tensor(5998.6938, device='cuda:0', grad_fn=<SumBackward0>), tensor(11437.9434, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 116 Loss 1714.9974365234375 Accs [tensor(39737.9297, device='cuda:0', grad_fn=<SumBackward0>), tensor(6002.0674, device='cuda:0', grad_fn=<SumBackward0>), tensor(11445.1934, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 117 Loss 1714.9974365234375 Accs [tensor(39762.8047, device='cuda:0', grad_fn=<SumBackward0>), tensor(6005.0098, device='cuda:0', grad_fn=<SumBackward0>), tensor(11452.6904, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 118 Loss 1714.997802734375 Accs [tensor(39772.7695, device='cuda:0', grad_fn=<SumBackward0>), tensor(6006.4766, device='cuda:0', grad_fn=<SumBackward0>), tensor(11455.4688, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 119 Loss 1714.997314453125 Accs [tensor(39796.7344, device='cuda:0', grad_fn=<SumBackward0>), tensor(6008.7031, device='cuda:0', grad_fn=<SumBackward0>), tensor(11462.8262, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 120 Loss 1714.997314453125 Accs [tensor(39821.0195, device='cuda:0', grad_fn=<SumBackward0>), tensor(6011.0869, device='cuda:0', grad_fn=<SumBackward0>), tensor(11470.2285, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 121 Loss 1714.9974365234375 Accs [tensor(39838.0898, device='cuda:0', grad_fn=<SumBackward0>), tensor(6013.1982, device='cuda:0', grad_fn=<SumBackward0>), tensor(11475.2344, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 122 Loss 1714.9970703125 Accs [tensor(39869.8477, device='cuda:0', grad_fn=<SumBackward0>), tensor(6016.3672, device='cuda:0', grad_fn=<SumBackward0>), tensor(11484.7666, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 123 Loss 1714.9969482421875 Accs [tensor(39893.9219, device='cuda:0', grad_fn=<SumBackward0>), tensor(6018.7285, device='cuda:0', grad_fn=<SumBackward0>), tensor(11491.9785, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 124 Loss 1714.9970703125 Accs [tensor(39908.1719, device='cuda:0', grad_fn=<SumBackward0>), tensor(6020.0127, device='cuda:0', grad_fn=<SumBackward0>), tensor(11496.2090, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 125 Loss 1714.9967041015625 Accs [tensor(39933.1758, device='cuda:0', grad_fn=<SumBackward0>), tensor(6021.9990, device='cuda:0', grad_fn=<SumBackward0>), tensor(11503.7539, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 126 Loss 1714.9967041015625 Accs [tensor(39951.1094, device='cuda:0', grad_fn=<SumBackward0>), tensor(6023.8228, device='cuda:0', grad_fn=<SumBackward0>), tensor(11508.9111, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 127 Loss 1714.9967041015625 Accs [tensor(39956.1602, device='cuda:0', grad_fn=<SumBackward0>), tensor(6024.6279, device='cuda:0', grad_fn=<SumBackward0>), tensor(11510.1904, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 128 Loss 1714.99658203125 Accs [tensor(39970.8086, device='cuda:0', grad_fn=<SumBackward0>), tensor(6026.4131, device='cuda:0', grad_fn=<SumBackward0>), tensor(11514.2783, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 129 Loss 1714.9964599609375 Accs [tensor(39972.7344, device='cuda:0', grad_fn=<SumBackward0>), tensor(6026.7563, device='cuda:0', grad_fn=<SumBackward0>), tensor(11514.7305, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 130 Loss 1714.9964599609375 Accs [tensor(39975.1992, device='cuda:0', grad_fn=<SumBackward0>), tensor(6026.9746, device='cuda:0', grad_fn=<SumBackward0>), tensor(11515.4238, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 131 Loss 1714.9964599609375 Accs [tensor(39983.5781, device='cuda:0', grad_fn=<SumBackward0>), tensor(6027.7637, device='cuda:0', grad_fn=<SumBackward0>), tensor(11517.8633, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 132 Loss 1714.9962158203125 Accs [tensor(39983.9844, device='cuda:0', grad_fn=<SumBackward0>), tensor(6027.9204, device='cuda:0', grad_fn=<SumBackward0>), tensor(11517.9238, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 133 Loss 1714.9962158203125 Accs [tensor(39993.7148, device='cuda:0', grad_fn=<SumBackward0>), tensor(6029.2764, device='cuda:0', grad_fn=<SumBackward0>), tensor(11520.5938, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 134 Loss 1714.99609375 Accs [tensor(40004.5469, device='cuda:0', grad_fn=<SumBackward0>), tensor(6030.8940, device='cuda:0', grad_fn=<SumBackward0>), tensor(11523.5039, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 135 Loss 1714.99609375 Accs [tensor(40006.9141, device='cuda:0', grad_fn=<SumBackward0>), tensor(6031.6797, device='cuda:0', grad_fn=<SumBackward0>), tensor(11524.0117, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 136 Loss 1714.99609375 Accs [tensor(40016.5469, device='cuda:0', grad_fn=<SumBackward0>), tensor(6033.0615, device='cuda:0', grad_fn=<SumBackward0>), tensor(11526.6816, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 137 Loss 1714.9957275390625 Accs [tensor(40020.6406, device='cuda:0', grad_fn=<SumBackward0>), tensor(6033.8418, device='cuda:0', grad_fn=<SumBackward0>), tensor(11527.7861, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 138 Loss 1714.9957275390625 Accs [tensor(40024.6836, device='cuda:0', grad_fn=<SumBackward0>), tensor(6034.6699, device='cuda:0', grad_fn=<SumBackward0>), tensor(11528.8496, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 139 Loss 1714.9957275390625 Accs [tensor(40033.8398, device='cuda:0', grad_fn=<SumBackward0>), tensor(6036.0957, device='cuda:0', grad_fn=<SumBackward0>), tensor(11531.3633, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 140 Loss 1714.9957275390625 Accs [tensor(40035.3047, device='cuda:0', grad_fn=<SumBackward0>), tensor(6036.7334, device='cuda:0', grad_fn=<SumBackward0>), tensor(11531.6328, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 141 Loss 1714.9957275390625 Accs [tensor(40039.5469, device='cuda:0', grad_fn=<SumBackward0>), tensor(6037.5425, device='cuda:0', grad_fn=<SumBackward0>), tensor(11532.7529, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 142 Loss 1714.9954833984375 Accs [tensor(40042.2773, device='cuda:0', grad_fn=<SumBackward0>), tensor(6038.0688, device='cuda:0', grad_fn=<SumBackward0>), tensor(11533.4834, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 143 Loss 1714.995361328125 Accs [tensor(40040.7188, device='cuda:0', grad_fn=<SumBackward0>), tensor(6038.1260, device='cuda:0', grad_fn=<SumBackward0>), tensor(11532.9834, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 144 Loss 1714.995361328125 Accs [tensor(40046., device='cuda:0', grad_fn=<SumBackward0>), tensor(6038.9707, device='cuda:0', grad_fn=<SumBackward0>), tensor(11534.4375, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 145 Loss 1714.995361328125 Accs [tensor(40048.3906, device='cuda:0', grad_fn=<SumBackward0>), tensor(6039.6577, device='cuda:0', grad_fn=<SumBackward0>), tensor(11535.0029, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 146 Loss 1714.995361328125 Accs [tensor(40052.8672, device='cuda:0', grad_fn=<SumBackward0>), tensor(6040.6338, device='cuda:0', grad_fn=<SumBackward0>), tensor(11536.1504, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 147 Loss 1714.9951171875 Accs [tensor(40059.7812, device='cuda:0', grad_fn=<SumBackward0>), tensor(6041.8359, device='cuda:0', grad_fn=<SumBackward0>), tensor(11538.0156, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 148 Loss 1714.9951171875 Accs [tensor(40061.6289, device='cuda:0', grad_fn=<SumBackward0>), tensor(6042.4609, device='cuda:0', grad_fn=<SumBackward0>), tensor(11538.4355, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 149 Loss 1714.9949951171875 Accs [tensor(40068.0234, device='cuda:0', grad_fn=<SumBackward0>), tensor(6043.5225, device='cuda:0', grad_fn=<SumBackward0>), tensor(11540.1875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 150 Loss 1714.9949951171875 Accs [tensor(40071.7891, device='cuda:0', grad_fn=<SumBackward0>), tensor(6044.3579, device='cuda:0', grad_fn=<SumBackward0>), tensor(11541.1562, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 151 Loss 1714.9949951171875 Accs [tensor(40075.6758, device='cuda:0', grad_fn=<SumBackward0>), tensor(6045.2432, device='cuda:0', grad_fn=<SumBackward0>), tensor(11542.1348, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 152 Loss 1714.9949951171875 Accs [tensor(40081.4219, device='cuda:0', grad_fn=<SumBackward0>), tensor(6046.2949, device='cuda:0', grad_fn=<SumBackward0>), tensor(11543.6562, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 153 Loss 1714.9947509765625 Accs [tensor(40082.0391, device='cuda:0', grad_fn=<SumBackward0>), tensor(6046.7358, device='cuda:0', grad_fn=<SumBackward0>), tensor(11543.7158, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 154 Loss 1714.99462890625 Accs [tensor(40085.6953, device='cuda:0', grad_fn=<SumBackward0>), tensor(6047.4023, device='cuda:0', grad_fn=<SumBackward0>), tensor(11544.6904, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 155 Loss 1714.99462890625 Accs [tensor(40086.3594, device='cuda:0', grad_fn=<SumBackward0>), tensor(6047.7832, device='cuda:0', grad_fn=<SumBackward0>), tensor(11544.7988, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 156 Loss 1714.99462890625 Accs [tensor(40088.2188, device='cuda:0', grad_fn=<SumBackward0>), tensor(6048.3418, device='cuda:0', grad_fn=<SumBackward0>), tensor(11545.2363, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 157 Loss 1714.994384765625 Accs [tensor(40091.7656, device='cuda:0', grad_fn=<SumBackward0>), tensor(6049.0972, device='cuda:0', grad_fn=<SumBackward0>), tensor(11546.1602, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 158 Loss 1714.994384765625 Accs [tensor(40092.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(6049.6372, device='cuda:0', grad_fn=<SumBackward0>), tensor(11546.3691, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 159 Loss 1714.994384765625 Accs [tensor(40097.1797, device='cuda:0', grad_fn=<SumBackward0>), tensor(6050.4502, device='cuda:0', grad_fn=<SumBackward0>), tensor(11547.5088, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 160 Loss 1714.994384765625 Accs [tensor(40098.5391, device='cuda:0', grad_fn=<SumBackward0>), tensor(6051.0122, device='cuda:0', grad_fn=<SumBackward0>), tensor(11547.7266, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 161 Loss 1714.9942626953125 Accs [tensor(40102.3359, device='cuda:0', grad_fn=<SumBackward0>), tensor(6051.7900, device='cuda:0', grad_fn=<SumBackward0>), tensor(11548.6787, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 162 Loss 1714.9942626953125 Accs [tensor(40105.2031, device='cuda:0', grad_fn=<SumBackward0>), tensor(6052.5161, device='cuda:0', grad_fn=<SumBackward0>), tensor(11549.3223, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 163 Loss 1714.9942626953125 Accs [tensor(40107.7578, device='cuda:0', grad_fn=<SumBackward0>), tensor(6053.2568, device='cuda:0', grad_fn=<SumBackward0>), tensor(11549.8633, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 164 Loss 1714.9942626953125 Accs [tensor(40110.9961, device='cuda:0', grad_fn=<SumBackward0>), tensor(6053.9844, device='cuda:0', grad_fn=<SumBackward0>), tensor(11550.6504, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 165 Loss 1714.9940185546875 Accs [tensor(40111.5547, device='cuda:0', grad_fn=<SumBackward0>), tensor(6054.4521, device='cuda:0', grad_fn=<SumBackward0>), tensor(11550.6299, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 166 Loss 1714.9942626953125 Accs [tensor(40114.5391, device='cuda:0', grad_fn=<SumBackward0>), tensor(6055.0400, device='cuda:0', grad_fn=<SumBackward0>), tensor(11551.3809, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 167 Loss 1714.9940185546875 Accs [tensor(40114.5156, device='cuda:0', grad_fn=<SumBackward0>), tensor(6055.4199, device='cuda:0', grad_fn=<SumBackward0>), tensor(11551.1973, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 168 Loss 1714.993896484375 Accs [tensor(40117.7188, device='cuda:0', grad_fn=<SumBackward0>), tensor(6056.0376, device='cuda:0', grad_fn=<SumBackward0>), tensor(11552.0088, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 169 Loss 1714.993896484375 Accs [tensor(40118.1602, device='cuda:0', grad_fn=<SumBackward0>), tensor(6056.4785, device='cuda:0', grad_fn=<SumBackward0>), tensor(11551.9541, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 170 Loss 1714.993896484375 Accs [tensor(40121.0859, device='cuda:0', grad_fn=<SumBackward0>), tensor(6057.0879, device='cuda:0', grad_fn=<SumBackward0>), tensor(11552.6738, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 171 Loss 1714.993896484375 Accs [tensor(40122.0078, device='cuda:0', grad_fn=<SumBackward0>), tensor(6057.5630, device='cuda:0', grad_fn=<SumBackward0>), tensor(11552.7656, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 172 Loss 1714.993896484375 Accs [tensor(40124.7539, device='cuda:0', grad_fn=<SumBackward0>), tensor(6058.1812, device='cuda:0', grad_fn=<SumBackward0>), tensor(11553.4189, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 173 Loss 1714.993896484375 Accs [tensor(40126.4219, device='cuda:0', grad_fn=<SumBackward0>), tensor(6058.7505, device='cuda:0', grad_fn=<SumBackward0>), tensor(11553.7275, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 174 Loss 1714.99365234375 Accs [tensor(40128.9531, device='cuda:0', grad_fn=<SumBackward0>), tensor(6059.3862, device='cuda:0', grad_fn=<SumBackward0>), tensor(11554.2949, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 175 Loss 1714.99365234375 Accs [tensor(40130.7578, device='cuda:0', grad_fn=<SumBackward0>), tensor(6059.9492, device='cuda:0', grad_fn=<SumBackward0>), tensor(11554.6348, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 176 Loss 1714.99365234375 Accs [tensor(40132.5195, device='cuda:0', grad_fn=<SumBackward0>), tensor(6060.4795, device='cuda:0', grad_fn=<SumBackward0>), tensor(11554.9775, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 177 Loss 1714.99365234375 Accs [tensor(40134.1797, device='cuda:0', grad_fn=<SumBackward0>), tensor(6060.9824, device='cuda:0', grad_fn=<SumBackward0>), tensor(11555.2930, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 178 Loss 1714.99365234375 Accs [tensor(40135.5547, device='cuda:0', grad_fn=<SumBackward0>), tensor(6061.4707, device='cuda:0', grad_fn=<SumBackward0>), tensor(11555.5234, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 179 Loss 1714.99365234375 Accs [tensor(40137.5938, device='cuda:0', grad_fn=<SumBackward0>), tensor(6062.0176, device='cuda:0', grad_fn=<SumBackward0>), tensor(11555.9541, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 180 Loss 1714.99365234375 Accs [tensor(40138.8516, device='cuda:0', grad_fn=<SumBackward0>), tensor(6062.5156, device='cuda:0', grad_fn=<SumBackward0>), tensor(11556.1396, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 181 Loss 1714.99365234375 Accs [tensor(40141.5234, device='cuda:0', grad_fn=<SumBackward0>), tensor(6063.1128, device='cuda:0', grad_fn=<SumBackward0>), tensor(11556.7646, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 182 Loss 1714.9935302734375 Accs [tensor(40142.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(6063.6006, device='cuda:0', grad_fn=<SumBackward0>), tensor(11556.8516, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 183 Loss 1714.9935302734375 Accs [tensor(40146.4453, device='cuda:0', grad_fn=<SumBackward0>), tensor(6064.3115, device='cuda:0', grad_fn=<SumBackward0>), tensor(11557.8682, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 184 Loss 1714.9935302734375 Accs [tensor(40146.1328, device='cuda:0', grad_fn=<SumBackward0>), tensor(6064.7495, device='cuda:0', grad_fn=<SumBackward0>), tensor(11557.5264, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 185 Loss 1714.9935302734375 Accs [tensor(40152.0859, device='cuda:0', grad_fn=<SumBackward0>), tensor(6065.6177, device='cuda:0', grad_fn=<SumBackward0>), tensor(11559.1680, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 186 Loss 1714.9935302734375 Accs [tensor(40147.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(6065.7905, device='cuda:0', grad_fn=<SumBackward0>), tensor(11557.6279, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 187 Loss 1714.9935302734375 Accs [tensor(40159.7188, device='cuda:0', grad_fn=<SumBackward0>), tensor(6067.0332, device='cuda:0', grad_fn=<SumBackward0>), tensor(11561.1250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 188 Loss 1714.9935302734375 Accs [tensor(40145.7344, device='cuda:0', grad_fn=<SumBackward0>), tensor(6066.5054, device='cuda:0', grad_fn=<SumBackward0>), tensor(11556.4941, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 189 Loss 1714.99365234375 Accs [tensor(40173.7656, device='cuda:0', grad_fn=<SumBackward0>), tensor(6068.8472, device='cuda:0', grad_fn=<SumBackward0>), tensor(11565.1250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 190 Loss 1714.9942626953125 Accs [tensor(40133.3164, device='cuda:0', grad_fn=<SumBackward0>), tensor(6066.4805, device='cuda:0', grad_fn=<SumBackward0>), tensor(11552.0967, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 191 Loss 1714.9957275390625 Accs [tensor(40204.5312, device='cuda:0', grad_fn=<SumBackward0>), tensor(6071.7490, device='cuda:0', grad_fn=<SumBackward0>), tensor(11574.4316, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 192 Loss 1714.9996337890625 Accs [tensor(40096.9492, device='cuda:0', grad_fn=<SumBackward0>), tensor(6064.5425, device='cuda:0', grad_fn=<SumBackward0>), tensor(11540.0605, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 193 Loss 1715.009765625 Accs [tensor(40281.9453, device='cuda:0', grad_fn=<SumBackward0>), tensor(6078.7812, device='cuda:0', grad_fn=<SumBackward0>), tensor(11597.4785, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 194 Loss 1715.0350341796875 Accs [tensor(40023.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(6059.0903, device='cuda:0', grad_fn=<SumBackward0>), tensor(11515.0908, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 195 Loss 1715.078369140625 Accs [tensor(40390.4531, device='cuda:0', grad_fn=<SumBackward0>), tensor(6090.0918, device='cuda:0', grad_fn=<SumBackward0>), tensor(11628.5879, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 196 Loss 1715.1231689453125 Accs [tensor(40004.5938, device='cuda:0', grad_fn=<SumBackward0>), tensor(6057.3979, device='cuda:0', grad_fn=<SumBackward0>), tensor(11508.5859, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 197 Loss 1715.09716796875 Accs [tensor(40265.0078, device='cuda:0', grad_fn=<SumBackward0>), tensor(6077.2158, device='cuda:0', grad_fn=<SumBackward0>), tensor(11592.4199, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 198 Loss 1715.022705078125 Accs [tensor(40155.5742, device='cuda:0', grad_fn=<SumBackward0>), tensor(6069.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(11558.7598, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 199 Loss 1714.995361328125 Accs [tensor(39988.8945, device='cuda:0', grad_fn=<SumBackward0>), tensor(6056.2021, device='cuda:0', grad_fn=<SumBackward0>), tensor(11499.7822, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Best accs [tensor(18125.6699, device='cuda:0', grad_fn=<SumBackward0>), tensor(2881.6646, device='cuda:0', grad_fn=<SumBackward0>), tensor(5331.8071, device='cuda:0', grad_fn=<SumBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n",
    "    # print(\n",
    "    #     f\"Epoch {epoch + 1}: loss {round(loss, 5)}, \"\n",
    "    #     f\"train micro {round(accs[0][0] * 100, 2)}%, train macro {round(accs[0][1] * 100, 2)}%, \"\n",
    "    #     f\"valid micro {round(accs[1][0] * 100, 2)}%, valid macro {round(accs[1][1] * 100, 2)}%, \"\n",
    "    #     f\"test micro {round(accs[2][0] * 100, 2)}%, test macro {round(accs[2][1] * 100, 2)}%\"\n",
    "    # )\n",
    "    print(f\"Epoch {epoch} Loss {loss} Accs {accs}\")\n",
    "\n",
    "best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx])\n",
    "\n",
    "print(\"Best accs\", best_accs)\n",
    "\n",
    "\n",
    "\n",
    "# print(\n",
    "#     f\"Best model: \"\n",
    "#     f\"train micro {round(best_accs[0][0] * 100, 2)}%, train macro {round(best_accs[0][1] * 100, 2)}%, \"\n",
    "#     f\"valid micro {round(best_accs[1][0] * 100, 2)}%, valid macro {round(best_accs[1][1] * 100, 2)}%, \"\n",
    "#     f\"test micro {round(best_accs[2][0] * 100, 2)}%, test macro {round(best_accs[2][1] * 100, 2)}%\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25.1041], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([25.1997], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([25.1016], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([25.1474], device='cuda:0', grad_fn=<SelectBackward0>) tensor([12.], device='cuda:0')\n",
      "tensor([25.0410], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([25.2733], device='cuda:0', grad_fn=<SelectBackward0>) tensor([56.], device='cuda:0')\n",
      "tensor([25.0296], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([24.9846], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([25.2393], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([17.2762], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([17.2164], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([17.3574], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([17.3371], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([17.3981], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([17.3362], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([17.2639], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([17.3450], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([16.9084], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([25.6175], device='cuda:0', grad_fn=<SelectBackward0>) tensor([94.], device='cuda:0')\n",
      "tensor([18.7791], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([17.3393], device='cuda:0', grad_fn=<SelectBackward0>) tensor([11.], device='cuda:0')\n",
      "tensor([17.2989], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([17.3461], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([25.7614], device='cuda:0', grad_fn=<SelectBackward0>) tensor([48.], device='cuda:0')\n",
      "tensor([17.2847], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([17.1902], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([17.3172], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([17.2796], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([24.7885], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([25.8111], device='cuda:0', grad_fn=<SelectBackward0>) tensor([111.], device='cuda:0')\n",
      "tensor([24.2441], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([25.1023], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([17.2560], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([25.0205], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([25.3017], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([17.2164], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([25.0889], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([25.0130], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([25.2736], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([24.5261], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([17.2469], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([24.6635], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([17.3371], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([25.1217], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([18.8841], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([25.5569], device='cuda:0', grad_fn=<SelectBackward0>) tensor([58.], device='cuda:0')\n",
      "tensor([25.1253], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([25.0824], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([25.0434], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([25.2546], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([25.0668], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([24.9425], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([25.2606], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([25.1075], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([18.9935], device='cuda:0', grad_fn=<SelectBackward0>) tensor([14.], device='cuda:0')\n",
      "tensor([25.1759], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([17.4602], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([17.1365], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([18.9770], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([25.4094], device='cuda:0', grad_fn=<SelectBackward0>) tensor([32.], device='cuda:0')\n",
      "tensor([17.2664], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([25.2658], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([25.3492], device='cuda:0', grad_fn=<SelectBackward0>) tensor([19.], device='cuda:0')\n",
      "tensor([25.2809], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([25.6691], device='cuda:0', grad_fn=<SelectBackward0>) tensor([144.], device='cuda:0')\n",
      "tensor([25.4860], device='cuda:0', grad_fn=<SelectBackward0>) tensor([79.], device='cuda:0')\n",
      "tensor([25.0224], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([25.0732], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([17.2629], device='cuda:0', grad_fn=<SelectBackward0>) tensor([12.], device='cuda:0')\n",
      "tensor([25.0700], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([18.7596], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([25.0462], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([19.1231], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([18.8487], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([17.3324], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([24.9973], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([25.3752], device='cuda:0', grad_fn=<SelectBackward0>) tensor([51.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "preds = best_model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "# mask = preds['event'] > 0\n",
    "# preds['event'][preds['event'] > 0].shape\n",
    "\n",
    "# print(preds['event'][0], hetero_graph.node_target['event'][0]) \n",
    "\n",
    "\n",
    "#print(hetero_graph.node_feature['event'])\n",
    "\n",
    "# for i in range(3000):\n",
    "#     if hetero_graph.node_target['event'][i] != -1: # concepts have node target -1\n",
    "#         print(preds['event'][i], hetero_graph.node_target['event'][i])\n",
    "        \n",
    "    \n",
    "for i in range(1000):    \n",
    "    if hetero_graph.node_target['event'][test_idx['event']][i] != -1:\n",
    "        print(preds['event'][test_idx['event']][i], hetero_graph.node_target['event'][test_idx['event']][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
