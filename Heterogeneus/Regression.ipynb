{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        self.lin_dst = nn.Linear(in_channels_dst, out_channels)\n",
    "        self.lin_src = nn.Linear(in_channels_src, out_channels)\n",
    "        self.lin_update = nn.Linear(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "        ):\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        \"\"\"\n",
    "        Initializes the HeteroGNNWrapperConv instance.\n",
    "\n",
    "        :param convs: Dictionary of convolution layers for each message type.\n",
    "        :param args: Arguments dictionary containing hyperparameters like hidden_size and attn_size.\n",
    "        :param aggr: Aggregation method, defaults to 'mean'.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_features: Dictionary of node features for each node type.\n",
    "        :param edge_indices: Dictionary of edge indices for each message type.\n",
    "        :return: Aggregated node embeddings for each node type.\n",
    "        \"\"\"\n",
    "        \n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        \n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        \n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "                \n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        \"\"\"\n",
    "        Aggregates node embeddings using the specified aggregation method.\n",
    "\n",
    "        :param xs: List of node embeddings to aggregate.\n",
    "        :return: Aggregated node embeddings as a torch.Tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates convolutional layers for each message type in a heterogeneous graph.\n",
    "\n",
    "    :param hetero_graph: The heterogeneous graph for which convolutions are to be created.\n",
    "    :param conv: The convolutional layer class or constructor.\n",
    "    :param hidden_size: The number of features in the hidden layer.\n",
    "    :param first_layer: Boolean indicating if this is the first layer in the network.\n",
    "    \n",
    "    :return: A dictionary of convolutional layers, keyed by message type.\n",
    "    \"\"\"\n",
    "\n",
    "    convs = {}\n",
    "    \n",
    "    # Extracting all types of messages/edges in the heterogeneous graph.\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        \n",
    "        # Determine the input feature size for source and destination nodes.\n",
    "        # If it's the first layer, use the feature size of the nodes.\n",
    "        # Otherwise, use the hidden size, since from there on the size of embeddings\n",
    "        # is the same for all nodes.\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        \n",
    "        # Create a convolutional layer for this message type and add it to the dictionary.\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, num_layers, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "        \n",
    "        # Initialize the graph convolutional layers\n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        # Initialize batch normalization, ReLU, and fully connected layers for each node type\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            \n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            \n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "            \n",
    "    def forward(self, node_feature, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_feature: Dictionary of node features for each node type.\n",
    "        :param edge_index: Dictionary of edge indices for each message type.\n",
    "        :return: The output embeddings for each node type after passing through the model.\n",
    "        \"\"\"\n",
    "        x = node_feature\n",
    "\n",
    "        # Apply graph convolutional, batch normalization, and ReLU layers\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        x = forward_op(x, self.fc)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        \"\"\"\n",
    "        Computes the loss for the model.\n",
    "\n",
    "        :param preds: Predictions made by the model.\n",
    "        :param y: Ground truth target values.\n",
    "        :param indices: Indices of nodes for which loss should be calculated.\n",
    "        \n",
    "        :return: The computed loss value.\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = 0\n",
    "        loss_func = torch.nn.L1Loss() \n",
    "             \n",
    "        mask = y['event'][indices['event'], 0] != -1\n",
    "        non_zero_idx = torch.masked_select(indices['event'], mask)\n",
    "                \n",
    "        loss += loss_func(preds['event'][non_zero_idx], y['event'][non_zero_idx, 0])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    \"\"\"\n",
    "    Trains the model on the given heterogeneous graph using the specified indices.\n",
    "\n",
    "    :param model: The graph neural network model to train.\n",
    "    :param optimizer: The optimizer used for training the model.\n",
    "    :param hetero_graph: The heterogeneous graph data.\n",
    "    :param train_idx: Indices for training nodes.\n",
    "\n",
    "    :return: The training loss as a float.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "    optimizer.zero_grad() # Zero out any existing gradients \n",
    "    \n",
    "    # Compute predictions using the model\n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    # Compute the loss using model's loss function\n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "\n",
    "    loss.backward() # Backward pass: compute gradient of the loss\n",
    "    optimizer.step() # Perform a single optimization step, updates parameters\n",
    "    \n",
    "    return loss.item() \n",
    "\n",
    "def test(model, graph, indices, best_model=None, best_val=0):\n",
    "    \"\"\"\n",
    "    Tests the model on given indices and updates the best model based on validation loss.\n",
    "\n",
    "    :param model: The trained graph neural network model.\n",
    "    :param graph: The heterogeneous graph data.\n",
    "    :param indices: List of indices for training, validation, and testing nodes.\n",
    "    :param best_model: The current best model based on validation loss.\n",
    "    :param best_val: The current best validation loss.\n",
    "    \n",
    "    :return: A tuple containing the list of losses for each dataset, the best model, and the best validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    accs = []\n",
    "    \n",
    "    # Evaluate the model on each set of indices\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        \n",
    "        idx = index['event']\n",
    "\n",
    "        L1 = torch.sum(torch.abs(preds['event'][idx] - graph.node_target['event'][idx]))\n",
    "        \n",
    "        accs.append(L1)\n",
    "    \n",
    "    # Update the best model and validation loss if the current model performs better\n",
    "    if accs[1] < best_val:\n",
    "        best_val = accs[1]\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    return accs, best_model, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 64,\n",
    "    'epochs': 200,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.201,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell creates a small heterogeneous graph, primarily for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [1, 1, 1],   # event 0\n",
    "                [2, 2, 2]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([[50], [2000]], dtype=torch.float32),\n",
    "    # \"concept\": torch.tensor([[0], [0]], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}\n",
    "\n",
    "# Testing\n",
    "hetero_graph = HeteroGraph(\n",
    "    node_feature=S_node_feature,\n",
    "    node_target=S_node_targets,\n",
    "    edge_index=S_edge_index\n",
    ")\n",
    "\n",
    "train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./1_concepts_similar.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "    # Convert to directed graph for compatibility with Deepsnap\n",
    "    G = G.to_directed()\n",
    "    \n",
    "hetero_graph = HeteroGraph(G, netlib=nx, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 2\n",
      "\t Feature 3\n",
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 2\n",
      "\t Feature 2\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 3\n",
      "\t Feature 2\n",
      "KEY ('event', 'related', 'concept') <class 'tuple'>\n",
      "KEY NUMS ('event', 'related', 'concept') 8487 8729\n",
      "MAX EDGES tensor(8448) tensor(8728) 8487 8729\n",
      "KEY ('event', 'similar', 'event') <class 'tuple'>\n",
      "KEY NUMS ('event', 'similar', 'event') 8487 8487\n",
      "MAX EDGES tensor(8486) tensor(8486) 8487 8487\n",
      "KEY ('concept', 'related', 'event') <class 'tuple'>\n",
      "KEY NUMS ('concept', 'related', 'event') 8729 8487\n",
      "MAX EDGES tensor(8728) tensor(8448) 8729 8487\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block ensures that all tensors in a heterogeneous graph are transferred to the same \n",
    "computational device, as specified in the 'args' dictionary.\n",
    "\"\"\"\n",
    "\n",
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "# Send node features to device\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "# Create a torch.SparseTensor from edge_index and send it to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key, type(key))\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    \n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max(), hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
    "    \n",
    "# Send node targets to device\n",
    "for key in hetero_graph.node_target:\n",
    "    hetero_graph.node_target[key] = hetero_graph.node_target[key].to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5940])\n",
      "torch.Size([1698])\n",
      "torch.Size([849])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block creates a basic split of a graph's nodes into training, validation, and testing sets. \n",
    "It uses predefined ratios to divide 'event' and 'concept' nodes in the heterogeneous graph for a simple \n",
    "dataset split, mainly for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "nEvents = hetero_graph.num_nodes(\"event\")\n",
    "nConcepts = hetero_graph.num_nodes(\"concept\")\n",
    "\n",
    "s1 = 0.7\n",
    "s2 = 0.8\n",
    "\n",
    "train_idx = {   \"event\": torch.tensor(range(0, int(nEvents * s1))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(0, int(nConcepts * s1))).to(args[\"device\"])\n",
    "            }\n",
    "val_idx = {   \"event\": torch.tensor(range(int(nEvents * s1), int(nEvents * s2))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s1), int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "test_idx = {   \"event\": torch.tensor(range(int(nEvents * s2), nEvents)).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s2), nConcepts)).to(args[\"device\"])\n",
    "            }\n",
    "\n",
    "print(train_idx[\"event\"].shape)\n",
    "print(test_idx[\"event\"].shape)\n",
    "print(val_idx[\"event\"].shape)\n",
    "\n",
    "\n",
    "# TODO: Add node labels to the nodes and try to make the deepsnap split work even for regression!\n",
    "\n",
    "# dataset = deepsnap.dataset.GraphDataset([hetero_graph], task='node')\n",
    "\n",
    "# dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
    "# datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([421])) that is different to the input size (torch.Size([421, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 1984.8907470703125 Accs [tensor(2.8063e+09, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.0359e+08, device='cuda:0', grad_fn=<SumBackward0>), tensor(7.7820e+08, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 1 Loss 1884.4149169921875 Accs [tensor(5024216., device='cuda:0', grad_fn=<SumBackward0>), tensor(726604.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(1428664.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 2 Loss 1976.921142578125 Accs [tensor(6965416., device='cuda:0', grad_fn=<SumBackward0>), tensor(1004241.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(1961118.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 3 Loss 1799.135009765625 Accs [tensor(5828627., device='cuda:0', grad_fn=<SumBackward0>), tensor(839708.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(1637365.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 4 Loss 1764.792236328125 Accs [tensor(2790167.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(401487.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(782146.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 5 Loss 1725.3070068359375 Accs [tensor(1716395.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(246753.3438, device='cuda:0', grad_fn=<SumBackward0>), tensor(480422.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 6 Loss 1733.2386474609375 Accs [tensor(1432404.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(205883.9844, device='cuda:0', grad_fn=<SumBackward0>), tensor(400817.1250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 7 Loss 1728.369140625 Accs [tensor(1352120., device='cuda:0', grad_fn=<SumBackward0>), tensor(194308.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(378621.0625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 8 Loss 1717.5296630859375 Accs [tensor(1332205.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(191481.3438, device='cuda:0', grad_fn=<SumBackward0>), tensor(373377.6250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 9 Loss 1716.536865234375 Accs [tensor(1277216.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(183604.6562, device='cuda:0', grad_fn=<SumBackward0>), tensor(358194.0312, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 10 Loss 1716.928466796875 Accs [tensor(1160104., device='cuda:0', grad_fn=<SumBackward0>), tensor(166770.5312, device='cuda:0', grad_fn=<SumBackward0>), tensor(325600.4062, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 11 Loss 1717.88671875 Accs [tensor(996048., device='cuda:0', grad_fn=<SumBackward0>), tensor(143114.6719, device='cuda:0', grad_fn=<SumBackward0>), tensor(279828.2812, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 12 Loss 1716.4339599609375 Accs [tensor(846562.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(121537.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(238033.9688, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 13 Loss 1715.75830078125 Accs [tensor(717756.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(102969.9844, device='cuda:0', grad_fn=<SumBackward0>), tensor(201870.6875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 14 Loss 1715.8004150390625 Accs [tensor(599204.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(85886.4844, device='cuda:0', grad_fn=<SumBackward0>), tensor(168573.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 15 Loss 1716.65380859375 Accs [tensor(501804.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(71803.8281, device='cuda:0', grad_fn=<SumBackward0>), tensor(141242.4688, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 16 Loss 1716.0738525390625 Accs [tensor(425858.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(60809.0273, device='cuda:0', grad_fn=<SumBackward0>), tensor(120030.6641, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 17 Loss 1715.4647216796875 Accs [tensor(365521.4062, device='cuda:0', grad_fn=<SumBackward0>), tensor(52102.3281, device='cuda:0', grad_fn=<SumBackward0>), tensor(103290.0547, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 18 Loss 1715.4268798828125 Accs [tensor(319750.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(45610.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(90540.6562, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 19 Loss 1716.120849609375 Accs [tensor(285383.3438, device='cuda:0', grad_fn=<SumBackward0>), tensor(40878.9844, device='cuda:0', grad_fn=<SumBackward0>), tensor(80984.6094, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 20 Loss 1715.717529296875 Accs [tensor(261109.2344, device='cuda:0', grad_fn=<SumBackward0>), tensor(37533.6094, device='cuda:0', grad_fn=<SumBackward0>), tensor(74207.8984, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 21 Loss 1715.3236083984375 Accs [tensor(243119.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(35087.3047, device='cuda:0', grad_fn=<SumBackward0>), tensor(69157.5625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 22 Loss 1715.3587646484375 Accs [tensor(227509.1719, device='cuda:0', grad_fn=<SumBackward0>), tensor(32923.1758, device='cuda:0', grad_fn=<SumBackward0>), tensor(64759.3516, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 23 Loss 1715.6968994140625 Accs [tensor(212781., device='cuda:0', grad_fn=<SumBackward0>), tensor(30846.2148, device='cuda:0', grad_fn=<SumBackward0>), tensor(60606.0547, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 24 Loss 1715.5181884765625 Accs [tensor(197226.5938, device='cuda:0', grad_fn=<SumBackward0>), tensor(28652.6211, device='cuda:0', grad_fn=<SumBackward0>), tensor(56177.9141, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 25 Loss 1715.1175537109375 Accs [tensor(183983.4688, device='cuda:0', grad_fn=<SumBackward0>), tensor(26797.7383, device='cuda:0', grad_fn=<SumBackward0>), tensor(52403.4297, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 26 Loss 1715.35888671875 Accs [tensor(174568.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(25467.2969, device='cuda:0', grad_fn=<SumBackward0>), tensor(49724.9648, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 27 Loss 1715.587890625 Accs [tensor(168753.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(24673.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(48033.8594, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 28 Loss 1715.271240234375 Accs [tensor(165197.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(24178.3262, device='cuda:0', grad_fn=<SumBackward0>), tensor(47018.0938, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 29 Loss 1715.1112060546875 Accs [tensor(161580.2969, device='cuda:0', grad_fn=<SumBackward0>), tensor(23670.2773, device='cuda:0', grad_fn=<SumBackward0>), tensor(45986.6484, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 30 Loss 1715.361328125 Accs [tensor(156322.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(22935.0332, device='cuda:0', grad_fn=<SumBackward0>), tensor(44488.8906, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 31 Loss 1715.3807373046875 Accs [tensor(150087.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(22028.6445, device='cuda:0', grad_fn=<SumBackward0>), tensor(42736.1289, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 32 Loss 1715.150634765625 Accs [tensor(143823.5938, device='cuda:0', grad_fn=<SumBackward0>), tensor(21136.5352, device='cuda:0', grad_fn=<SumBackward0>), tensor(40963.3984, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 33 Loss 1715.1827392578125 Accs [tensor(139395.5312, device='cuda:0', grad_fn=<SumBackward0>), tensor(20500.5234, device='cuda:0', grad_fn=<SumBackward0>), tensor(39700.7734, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 34 Loss 1715.29541015625 Accs [tensor(137553.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(20222.7930, device='cuda:0', grad_fn=<SumBackward0>), tensor(39184.3750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 35 Loss 1715.2008056640625 Accs [tensor(136219.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(20042.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(38810.7344, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 36 Loss 1715.17041015625 Accs [tensor(134502.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(19781.8984, device='cuda:0', grad_fn=<SumBackward0>), tensor(38343.4453, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 37 Loss 1715.228759765625 Accs [tensor(131292.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(19314.2773, device='cuda:0', grad_fn=<SumBackward0>), tensor(37444.4492, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 38 Loss 1715.1466064453125 Accs [tensor(127061.8125, device='cuda:0', grad_fn=<SumBackward0>), tensor(18699.3125, device='cuda:0', grad_fn=<SumBackward0>), tensor(36260.8594, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 39 Loss 1715.103271484375 Accs [tensor(123637.9688, device='cuda:0', grad_fn=<SumBackward0>), tensor(18185.4180, device='cuda:0', grad_fn=<SumBackward0>), tensor(35307.3125, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 40 Loss 1715.211669921875 Accs [tensor(121490.9219, device='cuda:0', grad_fn=<SumBackward0>), tensor(17882.6523, device='cuda:0', grad_fn=<SumBackward0>), tensor(34700.7422, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 41 Loss 1715.201904296875 Accs [tensor(121025.6172, device='cuda:0', grad_fn=<SumBackward0>), tensor(17802.9551, device='cuda:0', grad_fn=<SumBackward0>), tensor(34575.3047, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 42 Loss 1715.096923828125 Accs [tensor(120469.7266, device='cuda:0', grad_fn=<SumBackward0>), tensor(17724.9180, device='cuda:0', grad_fn=<SumBackward0>), tensor(34416.1875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 43 Loss 1715.1160888671875 Accs [tensor(118472.7969, device='cuda:0', grad_fn=<SumBackward0>), tensor(17422.7305, device='cuda:0', grad_fn=<SumBackward0>), tensor(33843.2188, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 44 Loss 1715.137939453125 Accs [tensor(115596.3125, device='cuda:0', grad_fn=<SumBackward0>), tensor(16993.4727, device='cuda:0', grad_fn=<SumBackward0>), tensor(33019.1094, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 45 Loss 1715.09619140625 Accs [tensor(112278.4844, device='cuda:0', grad_fn=<SumBackward0>), tensor(16514.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(32063.5742, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 46 Loss 1715.13330078125 Accs [tensor(110295.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(16212.2588, device='cuda:0', grad_fn=<SumBackward0>), tensor(31498.5312, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 47 Loss 1715.1729736328125 Accs [tensor(109084.9141, device='cuda:0', grad_fn=<SumBackward0>), tensor(16047.4580, device='cuda:0', grad_fn=<SumBackward0>), tensor(31150.2852, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 48 Loss 1715.125732421875 Accs [tensor(108981.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(16024.3057, device='cuda:0', grad_fn=<SumBackward0>), tensor(31124.7695, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 49 Loss 1715.1129150390625 Accs [tensor(107877.1406, device='cuda:0', grad_fn=<SumBackward0>), tensor(15878.6172, device='cuda:0', grad_fn=<SumBackward0>), tensor(30810.9668, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 50 Loss 1715.1224365234375 Accs [tensor(106288.1094, device='cuda:0', grad_fn=<SumBackward0>), tensor(15650.3867, device='cuda:0', grad_fn=<SumBackward0>), tensor(30359.8027, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 51 Loss 1715.0792236328125 Accs [tensor(104098.5781, device='cuda:0', grad_fn=<SumBackward0>), tensor(15357.3652, device='cuda:0', grad_fn=<SumBackward0>), tensor(29738.5938, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 52 Loss 1715.06787109375 Accs [tensor(102633.3438, device='cuda:0', grad_fn=<SumBackward0>), tensor(15156.3213, device='cuda:0', grad_fn=<SumBackward0>), tensor(29302.9766, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 53 Loss 1715.089599609375 Accs [tensor(102286.2109, device='cuda:0', grad_fn=<SumBackward0>), tensor(15112.1191, device='cuda:0', grad_fn=<SumBackward0>), tensor(29183.9102, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 54 Loss 1715.07373046875 Accs [tensor(102339.7188, device='cuda:0', grad_fn=<SumBackward0>), tensor(15134.7178, device='cuda:0', grad_fn=<SumBackward0>), tensor(29189.1445, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 55 Loss 1715.076171875 Accs [tensor(102502.9141, device='cuda:0', grad_fn=<SumBackward0>), tensor(15156.0752, device='cuda:0', grad_fn=<SumBackward0>), tensor(29233.5605, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 56 Loss 1715.106689453125 Accs [tensor(101934.4531, device='cuda:0', grad_fn=<SumBackward0>), tensor(15093.2031, device='cuda:0', grad_fn=<SumBackward0>), tensor(29051.2305, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 57 Loss 1715.114990234375 Accs [tensor(101785.0391, device='cuda:0', grad_fn=<SumBackward0>), tensor(15061.3223, device='cuda:0', grad_fn=<SumBackward0>), tensor(29012.9180, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 58 Loss 1715.1488037109375 Accs [tensor(101345.0703, device='cuda:0', grad_fn=<SumBackward0>), tensor(15013.8896, device='cuda:0', grad_fn=<SumBackward0>), tensor(28878.2695, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 59 Loss 1715.2279052734375 Accs [tensor(102066.5781, device='cuda:0', grad_fn=<SumBackward0>), tensor(15098.2207, device='cuda:0', grad_fn=<SumBackward0>), tensor(29089.7715, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 60 Loss 1715.31689453125 Accs [tensor(102115.0156, device='cuda:0', grad_fn=<SumBackward0>), tensor(15133.1328, device='cuda:0', grad_fn=<SumBackward0>), tensor(29090.8730, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 61 Loss 1715.42578125 Accs [tensor(103175.4297, device='cuda:0', grad_fn=<SumBackward0>), tensor(15258.9902, device='cuda:0', grad_fn=<SumBackward0>), tensor(29402.6680, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 62 Loss 1715.48046875 Accs [tensor(102658.4844, device='cuda:0', grad_fn=<SumBackward0>), tensor(15211.0889, device='cuda:0', grad_fn=<SumBackward0>), tensor(29240.2832, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 63 Loss 1715.367431640625 Accs [tensor(103134.7266, device='cuda:0', grad_fn=<SumBackward0>), tensor(15259.8623, device='cuda:0', grad_fn=<SumBackward0>), tensor(29381.6582, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 64 Loss 1715.161376953125 Accs [tensor(103070.4766, device='cuda:0', grad_fn=<SumBackward0>), tensor(15258.2549, device='cuda:0', grad_fn=<SumBackward0>), tensor(29356.7754, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 65 Loss 1715.0606689453125 Accs [tensor(103333.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(15302.2900, device='cuda:0', grad_fn=<SumBackward0>), tensor(29426.9062, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 66 Loss 1715.1239013671875 Accs [tensor(104243.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(15418.8066, device='cuda:0', grad_fn=<SumBackward0>), tensor(29691.2266, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 67 Loss 1715.2445068359375 Accs [tensor(104267.6719, device='cuda:0', grad_fn=<SumBackward0>), tensor(15442.2041, device='cuda:0', grad_fn=<SumBackward0>), tensor(29685.5820, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 68 Loss 1715.2578125 Accs [tensor(105291.7344, device='cuda:0', grad_fn=<SumBackward0>), tensor(15573.3848, device='cuda:0', grad_fn=<SumBackward0>), tensor(29978.5918, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 69 Loss 1715.1390380859375 Accs [tensor(105536.0469, device='cuda:0', grad_fn=<SumBackward0>), tensor(15615.7969, device='cuda:0', grad_fn=<SumBackward0>), tensor(30038.4062, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 70 Loss 1715.0531005859375 Accs [tensor(105951.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(15678.7627, device='cuda:0', grad_fn=<SumBackward0>), tensor(30150.4102, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 71 Loss 1715.0899658203125 Accs [tensor(106839.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(15795.4502, device='cuda:0', grad_fn=<SumBackward0>), tensor(30405.4648, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 72 Loss 1715.16357421875 Accs [tensor(107022.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(15837.1592, device='cuda:0', grad_fn=<SumBackward0>), tensor(30447.5645, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 73 Loss 1715.168701171875 Accs [tensor(107732.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(15927.4004, device='cuda:0', grad_fn=<SumBackward0>), tensor(30653.1855, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 74 Loss 1715.097900390625 Accs [tensor(107843.7031, device='cuda:0', grad_fn=<SumBackward0>), tensor(15948.4180, device='cuda:0', grad_fn=<SumBackward0>), tensor(30678.8164, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 75 Loss 1715.0462646484375 Accs [tensor(107970.2344, device='cuda:0', grad_fn=<SumBackward0>), tensor(15969.5078, device='cuda:0', grad_fn=<SumBackward0>), tensor(30710.1523, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 76 Loss 1715.0745849609375 Accs [tensor(108484.6406, device='cuda:0', grad_fn=<SumBackward0>), tensor(16034.1045, device='cuda:0', grad_fn=<SumBackward0>), tensor(30859.1426, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 77 Loss 1715.121337890625 Accs [tensor(108510.0703, device='cuda:0', grad_fn=<SumBackward0>), tensor(16049.9463, device='cuda:0', grad_fn=<SumBackward0>), tensor(30859.5156, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 78 Loss 1715.117919921875 Accs [tensor(109036.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(16116.5781, device='cuda:0', grad_fn=<SumBackward0>), tensor(31013.4746, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 79 Loss 1715.07470703125 Accs [tensor(109087.6953, device='cuda:0', grad_fn=<SumBackward0>), tensor(16128.1348, device='cuda:0', grad_fn=<SumBackward0>), tensor(31024.5742, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 80 Loss 1715.04443359375 Accs [tensor(109111.6641, device='cuda:0', grad_fn=<SumBackward0>), tensor(16133.5850, device='cuda:0', grad_fn=<SumBackward0>), tensor(31028.4355, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 81 Loss 1715.0576171875 Accs [tensor(109386.5938, device='cuda:0', grad_fn=<SumBackward0>), tensor(16165.8271, device='cuda:0', grad_fn=<SumBackward0>), tensor(31108.8359, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 82 Loss 1715.0882568359375 Accs [tensor(109295.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(16162.3594, device='cuda:0', grad_fn=<SumBackward0>), tensor(31077.7617, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 83 Loss 1715.0919189453125 Accs [tensor(109685.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(16210.4453, device='cuda:0', grad_fn=<SumBackward0>), tensor(31192.8223, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 84 Loss 1715.06787109375 Accs [tensor(109676.2188, device='cuda:0', grad_fn=<SumBackward0>), tensor(16214.2051, device='cuda:0', grad_fn=<SumBackward0>), tensor(31187.5762, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 85 Loss 1715.0447998046875 Accs [tensor(109726.9453, device='cuda:0', grad_fn=<SumBackward0>), tensor(16221.2861, device='cuda:0', grad_fn=<SumBackward0>), tensor(31201.0664, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 86 Loss 1715.0426025390625 Accs [tensor(109862.1484, device='cuda:0', grad_fn=<SumBackward0>), tensor(16236.5010, device='cuda:0', grad_fn=<SumBackward0>), tensor(31240.4414, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 87 Loss 1715.05908203125 Accs [tensor(109749.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(16227.2461, device='cuda:0', grad_fn=<SumBackward0>), tensor(31204.4121, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 88 Loss 1715.0716552734375 Accs [tensor(110063.6562, device='cuda:0', grad_fn=<SumBackward0>), tensor(16265.1602, device='cuda:0', grad_fn=<SumBackward0>), tensor(31297.6387, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 89 Loss 1715.0675048828125 Accs [tensor(110004.4531, device='cuda:0', grad_fn=<SumBackward0>), tensor(16263.2207, device='cuda:0', grad_fn=<SumBackward0>), tensor(31277.6504, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 90 Loss 1715.0531005859375 Accs [tensor(110158.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(16281.7520, device='cuda:0', grad_fn=<SumBackward0>), tensor(31322.8438, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 91 Loss 1715.0399169921875 Accs [tensor(110166.0156, device='cuda:0', grad_fn=<SumBackward0>), tensor(16283.1914, device='cuda:0', grad_fn=<SumBackward0>), tensor(31323.8457, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 92 Loss 1715.03759765625 Accs [tensor(110137.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(16281.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(31313.9258, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 93 Loss 1715.0447998046875 Accs [tensor(110339.8594, device='cuda:0', grad_fn=<SumBackward0>), tensor(16306.0879, device='cuda:0', grad_fn=<SumBackward0>), tensor(31373.8203, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 94 Loss 1715.0531005859375 Accs [tensor(110272.7031, device='cuda:0', grad_fn=<SumBackward0>), tensor(16302.5977, device='cuda:0', grad_fn=<SumBackward0>), tensor(31351.8574, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 95 Loss 1715.056884765625 Accs [tensor(110501.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(16329.4912, device='cuda:0', grad_fn=<SumBackward0>), tensor(31419.7539, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 96 Loss 1715.0538330078125 Accs [tensor(110401.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(16320.3643, device='cuda:0', grad_fn=<SumBackward0>), tensor(31388.0703, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 97 Loss 1715.0462646484375 Accs [tensor(110548.4219, device='cuda:0', grad_fn=<SumBackward0>), tensor(16337.5898, device='cuda:0', grad_fn=<SumBackward0>), tensor(31431.1328, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 98 Loss 1715.0390625 Accs [tensor(110564.2109, device='cuda:0', grad_fn=<SumBackward0>), tensor(16342.0234, device='cuda:0', grad_fn=<SumBackward0>), tensor(31434.3516, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 99 Loss 1715.0343017578125 Accs [tensor(110663.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(16356.1777, device='cuda:0', grad_fn=<SumBackward0>), tensor(31462.7109, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 100 Loss 1715.033203125 Accs [tensor(110785.3594, device='cuda:0', grad_fn=<SumBackward0>), tensor(16372.4688, device='cuda:0', grad_fn=<SumBackward0>), tensor(31498.0312, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 101 Loss 1715.035400390625 Accs [tensor(110769.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(16373.0469, device='cuda:0', grad_fn=<SumBackward0>), tensor(31491.6250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 102 Loss 1715.0386962890625 Accs [tensor(110928.6797, device='cuda:0', grad_fn=<SumBackward0>), tensor(16391.8926, device='cuda:0', grad_fn=<SumBackward0>), tensor(31538.1699, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 103 Loss 1715.0426025390625 Accs [tensor(110871.1953, device='cuda:0', grad_fn=<SumBackward0>), tensor(16388.5352, device='cuda:0', grad_fn=<SumBackward0>), tensor(31518.9609, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 104 Loss 1715.0462646484375 Accs [tensor(111110.6406, device='cuda:0', grad_fn=<SumBackward0>), tensor(16417.5742, device='cuda:0', grad_fn=<SumBackward0>), tensor(31589.8633, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 105 Loss 1715.0494384765625 Accs [tensor(111027.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(16411.9414, device='cuda:0', grad_fn=<SumBackward0>), tensor(31562.9316, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 106 Loss 1715.053466796875 Accs [tensor(111276.2734, device='cuda:0', grad_fn=<SumBackward0>), tensor(16440.9551, device='cuda:0', grad_fn=<SumBackward0>), tensor(31636.7500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 107 Loss 1715.0577392578125 Accs [tensor(111137.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(16428.4453, device='cuda:0', grad_fn=<SumBackward0>), tensor(31592.6289, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 108 Loss 1715.063720703125 Accs [tensor(111434.8516, device='cuda:0', grad_fn=<SumBackward0>), tensor(16462.8984, device='cuda:0', grad_fn=<SumBackward0>), tensor(31681.2812, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 109 Loss 1715.0731201171875 Accs [tensor(111260.3047, device='cuda:0', grad_fn=<SumBackward0>), tensor(16447.4766, device='cuda:0', grad_fn=<SumBackward0>), tensor(31626.3047, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 110 Loss 1715.08740234375 Accs [tensor(111645.7734, device='cuda:0', grad_fn=<SumBackward0>), tensor(16491.9102, device='cuda:0', grad_fn=<SumBackward0>), tensor(31741.9023, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 111 Loss 1715.1097412109375 Accs [tensor(111337.3516, device='cuda:0', grad_fn=<SumBackward0>), tensor(16460.7891, device='cuda:0', grad_fn=<SumBackward0>), tensor(31646.5625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 112 Loss 1715.1451416015625 Accs [tensor(111802.6328, device='cuda:0', grad_fn=<SumBackward0>), tensor(16511.8281, device='cuda:0', grad_fn=<SumBackward0>), tensor(31787.4492, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 113 Loss 1715.1986083984375 Accs [tensor(111331.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(16463.1602, device='cuda:0', grad_fn=<SumBackward0>), tensor(31642.2402, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 114 Loss 1715.275146484375 Accs [tensor(112053.1016, device='cuda:0', grad_fn=<SumBackward0>), tensor(16544.3516, device='cuda:0', grad_fn=<SumBackward0>), tensor(31860.1406, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 115 Loss 1715.37158203125 Accs [tensor(111430.7969, device='cuda:0', grad_fn=<SumBackward0>), tensor(16480.9551, device='cuda:0', grad_fn=<SumBackward0>), tensor(31667.5156, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 116 Loss 1715.465087890625 Accs [tensor(112377.6797, device='cuda:0', grad_fn=<SumBackward0>), tensor(16589.0547, device='cuda:0', grad_fn=<SumBackward0>), tensor(31952.4609, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 117 Loss 1715.50341796875 Accs [tensor(111655.1641, device='cuda:0', grad_fn=<SumBackward0>), tensor(16512.7305, device='cuda:0', grad_fn=<SumBackward0>), tensor(31729.9238, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 118 Loss 1715.432373046875 Accs [tensor(112476.8906, device='cuda:0', grad_fn=<SumBackward0>), tensor(16607.8086, device='cuda:0', grad_fn=<SumBackward0>), tensor(31976.7539, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 119 Loss 1715.2574462890625 Accs [tensor(112114.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(16570.9688, device='cuda:0', grad_fn=<SumBackward0>), tensor(31864.7461, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 120 Loss 1715.083251953125 Accs [tensor(112373.6328, device='cuda:0', grad_fn=<SumBackward0>), tensor(16603.9219, device='cuda:0', grad_fn=<SumBackward0>), tensor(31940.5117, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 121 Loss 1715.0252685546875 Accs [tensor(112680.6562, device='cuda:0', grad_fn=<SumBackward0>), tensor(16641.8594, device='cuda:0', grad_fn=<SumBackward0>), tensor(32030.6836, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 122 Loss 1715.095703125 Accs [tensor(112341.5781, device='cuda:0', grad_fn=<SumBackward0>), tensor(16607.1094, device='cuda:0', grad_fn=<SumBackward0>), tensor(31925.1016, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 123 Loss 1715.1951904296875 Accs [tensor(112997.2109, device='cuda:0', grad_fn=<SumBackward0>), tensor(16683.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(32121.4863, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 124 Loss 1715.2139892578125 Accs [tensor(112585.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(16640.6816, device='cuda:0', grad_fn=<SumBackward0>), tensor(31994.7305, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 125 Loss 1715.134521484375 Accs [tensor(113009.5547, device='cuda:0', grad_fn=<SumBackward0>), tensor(16691.9434, device='cuda:0', grad_fn=<SumBackward0>), tensor(32120.8945, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 126 Loss 1715.04296875 Accs [tensor(113065.9141, device='cuda:0', grad_fn=<SumBackward0>), tensor(16701.5117, device='cuda:0', grad_fn=<SumBackward0>), tensor(32135.6953, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 127 Loss 1715.02783203125 Accs [tensor(112935.1484, device='cuda:0', grad_fn=<SumBackward0>), tensor(16689.2812, device='cuda:0', grad_fn=<SumBackward0>), tensor(32094.0527, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 128 Loss 1715.0823974609375 Accs [tensor(113406.3047, device='cuda:0', grad_fn=<SumBackward0>), tensor(16745.2969, device='cuda:0', grad_fn=<SumBackward0>), tensor(32234.5039, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 129 Loss 1715.1263427734375 Accs [tensor(113078.5469, device='cuda:0', grad_fn=<SumBackward0>), tensor(16710.8672, device='cuda:0', grad_fn=<SumBackward0>), tensor(32133.4180, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 130 Loss 1715.1055908203125 Accs [tensor(113485.6328, device='cuda:0', grad_fn=<SumBackward0>), tensor(16760.1406, device='cuda:0', grad_fn=<SumBackward0>), tensor(32254.5938, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 131 Loss 1715.0487060546875 Accs [tensor(113479.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(16762.7344, device='cuda:0', grad_fn=<SumBackward0>), tensor(32250.2422, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 132 Loss 1715.0206298828125 Accs [tensor(113477.5156, device='cuda:0', grad_fn=<SumBackward0>), tensor(16765.6445, device='cuda:0', grad_fn=<SumBackward0>), tensor(32247.4043, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 133 Loss 1715.0426025390625 Accs [tensor(113826.5312, device='cuda:0', grad_fn=<SumBackward0>), tensor(16807.8770, device='cuda:0', grad_fn=<SumBackward0>), tensor(32350.8047, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 134 Loss 1715.0758056640625 Accs [tensor(113596.7656, device='cuda:0', grad_fn=<SumBackward0>), tensor(16784.6582, device='cuda:0', grad_fn=<SumBackward0>), tensor(32279.2129, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 135 Loss 1715.076171875 Accs [tensor(113925.9219, device='cuda:0', grad_fn=<SumBackward0>), tensor(16823.8887, device='cuda:0', grad_fn=<SumBackward0>), tensor(32377.7422, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 136 Loss 1715.0458984375 Accs [tensor(113830.0078, device='cuda:0', grad_fn=<SumBackward0>), tensor(16814.4082, device='cuda:0', grad_fn=<SumBackward0>), tensor(32347.9531, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 137 Loss 1715.02099609375 Accs [tensor(113843.9297, device='cuda:0', grad_fn=<SumBackward0>), tensor(16817.2305, device='cuda:0', grad_fn=<SumBackward0>), tensor(32351.3242, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 138 Loss 1715.0252685546875 Accs [tensor(114042.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(16840.7656, device='cuda:0', grad_fn=<SumBackward0>), tensor(32410.6797, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 139 Loss 1715.0455322265625 Accs [tensor(113857.4531, device='cuda:0', grad_fn=<SumBackward0>), tensor(16821.5312, device='cuda:0', grad_fn=<SumBackward0>), tensor(32353.5391, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 140 Loss 1715.0545654296875 Accs [tensor(114130.1094, device='cuda:0', grad_fn=<SumBackward0>), tensor(16853.6914, device='cuda:0', grad_fn=<SumBackward0>), tensor(32435.4648, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 141 Loss 1715.04248046875 Accs [tensor(114023.9219, device='cuda:0', grad_fn=<SumBackward0>), tensor(16843.3555, device='cuda:0', grad_fn=<SumBackward0>), tensor(32402.2812, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 142 Loss 1715.0238037109375 Accs [tensor(114117.5156, device='cuda:0', grad_fn=<SumBackward0>), tensor(16855.4922, device='cuda:0', grad_fn=<SumBackward0>), tensor(32429.5254, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 143 Loss 1715.0177001953125 Accs [tensor(114237.7109, device='cuda:0', grad_fn=<SumBackward0>), tensor(16870.6035, device='cuda:0', grad_fn=<SumBackward0>), tensor(32464.8906, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 144 Loss 1715.02587890625 Accs [tensor(114117.5703, device='cuda:0', grad_fn=<SumBackward0>), tensor(16858.2070, device='cuda:0', grad_fn=<SumBackward0>), tensor(32427.8984, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 145 Loss 1715.0362548828125 Accs [tensor(114320.6484, device='cuda:0', grad_fn=<SumBackward0>), tensor(16881.6074, device='cuda:0', grad_fn=<SumBackward0>), tensor(32489.3594, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 146 Loss 1715.0372314453125 Accs [tensor(114133.3594, device='cuda:0', grad_fn=<SumBackward0>), tensor(16859.9727, device='cuda:0', grad_fn=<SumBackward0>), tensor(32433.3398, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 147 Loss 1715.0283203125 Accs [tensor(114221.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(16869.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(32460.8730, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 148 Loss 1715.0185546875 Accs [tensor(114186.7812, device='cuda:0', grad_fn=<SumBackward0>), tensor(16865.1680, device='cuda:0', grad_fn=<SumBackward0>), tensor(32450.6777, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 149 Loss 1715.0157470703125 Accs [tensor(114130.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(16858.7969, device='cuda:0', grad_fn=<SumBackward0>), tensor(32433.9141, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 150 Loss 1715.020263671875 Accs [tensor(114250.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(16872.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(32470.5547, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 151 Loss 1715.02587890625 Accs [tensor(114122.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(16858.6113, device='cuda:0', grad_fn=<SumBackward0>), tensor(32431.3320, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 152 Loss 1715.027587890625 Accs [tensor(114274.0938, device='cuda:0', grad_fn=<SumBackward0>), tensor(16876.1465, device='cuda:0', grad_fn=<SumBackward0>), tensor(32477.2500, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 153 Loss 1715.024169921875 Accs [tensor(114200.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(16868.9141, device='cuda:0', grad_fn=<SumBackward0>), tensor(32454.1152, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 154 Loss 1715.0185546875 Accs [tensor(114302.5781, device='cuda:0', grad_fn=<SumBackward0>), tensor(16881.8672, device='cuda:0', grad_fn=<SumBackward0>), tensor(32484.3008, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 155 Loss 1715.0146484375 Accs [tensor(114349.1016, device='cuda:0', grad_fn=<SumBackward0>), tensor(16888.7344, device='cuda:0', grad_fn=<SumBackward0>), tensor(32497.3066, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 156 Loss 1715.0142822265625 Accs [tensor(114373.1016, device='cuda:0', grad_fn=<SumBackward0>), tensor(16893.6758, device='cuda:0', grad_fn=<SumBackward0>), tensor(32502.9082, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 157 Loss 1715.0162353515625 Accs [tensor(114533.2656, device='cuda:0', grad_fn=<SumBackward0>), tensor(16914.1035, device='cuda:0', grad_fn=<SumBackward0>), tensor(32549.6836, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 158 Loss 1715.0191650390625 Accs [tensor(114513.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(16914.7090, device='cuda:0', grad_fn=<SumBackward0>), tensor(32541.7246, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 159 Loss 1715.0203857421875 Accs [tensor(114704.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(16938.6641, device='cuda:0', grad_fn=<SumBackward0>), tensor(32597.6875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 160 Loss 1715.0198974609375 Accs [tensor(114682.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(16938.6973, device='cuda:0', grad_fn=<SumBackward0>), tensor(32588.9922, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 161 Loss 1715.0177001953125 Accs [tensor(114839.5469, device='cuda:0', grad_fn=<SumBackward0>), tensor(16958.8281, device='cuda:0', grad_fn=<SumBackward0>), tensor(32634.9727, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 162 Loss 1715.01513671875 Accs [tensor(114858.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(16963.3477, device='cuda:0', grad_fn=<SumBackward0>), tensor(32639.0430, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 163 Loss 1715.0130615234375 Accs [tensor(114951.4844, device='cuda:0', grad_fn=<SumBackward0>), tensor(16975.9590, device='cuda:0', grad_fn=<SumBackward0>), tensor(32665.6172, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 164 Loss 1715.011962890625 Accs [tensor(115019.2422, device='cuda:0', grad_fn=<SumBackward0>), tensor(16985.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(32684.7344, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 165 Loss 1715.011962890625 Accs [tensor(115052.8828, device='cuda:0', grad_fn=<SumBackward0>), tensor(16991.4531, device='cuda:0', grad_fn=<SumBackward0>), tensor(32693.4648, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 166 Loss 1715.0123291015625 Accs [tensor(115168.8359, device='cuda:0', grad_fn=<SumBackward0>), tensor(17006.5469, device='cuda:0', grad_fn=<SumBackward0>), tensor(32727.1602, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 167 Loss 1715.013427734375 Accs [tensor(115180.1719, device='cuda:0', grad_fn=<SumBackward0>), tensor(17010.3867, device='cuda:0', grad_fn=<SumBackward0>), tensor(32728.6914, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 168 Loss 1715.014404296875 Accs [tensor(115331.3672, device='cuda:0', grad_fn=<SumBackward0>), tensor(17029.6133, device='cuda:0', grad_fn=<SumBackward0>), tensor(32772.9844, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 169 Loss 1715.01513671875 Accs [tensor(115348.5859, device='cuda:0', grad_fn=<SumBackward0>), tensor(17035.0195, device='cuda:0', grad_fn=<SumBackward0>), tensor(32775.5625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 170 Loss 1715.015869140625 Accs [tensor(115524.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(17057.3125, device='cuda:0', grad_fn=<SumBackward0>), tensor(32827.2539, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 171 Loss 1715.0162353515625 Accs [tensor(115569.9531, device='cuda:0', grad_fn=<SumBackward0>), tensor(17067.1582, device='cuda:0', grad_fn=<SumBackward0>), tensor(32837.1406, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 172 Loss 1715.0169677734375 Accs [tensor(115810.3984, device='cuda:0', grad_fn=<SumBackward0>), tensor(17098.2520, device='cuda:0', grad_fn=<SumBackward0>), tensor(32906.9844, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 173 Loss 1715.0177001953125 Accs [tensor(115808.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(17101.7969, device='cuda:0', grad_fn=<SumBackward0>), tensor(32903.3828, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 174 Loss 1715.0189208984375 Accs [tensor(116047.1016, device='cuda:0', grad_fn=<SumBackward0>), tensor(17132.0391, device='cuda:0', grad_fn=<SumBackward0>), tensor(32973.3281, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 175 Loss 1715.020751953125 Accs [tensor(116091.3906, device='cuda:0', grad_fn=<SumBackward0>), tensor(17143.1328, device='cuda:0', grad_fn=<SumBackward0>), tensor(32981.8516, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 176 Loss 1715.0238037109375 Accs [tensor(116373.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(17178.4141, device='cuda:0', grad_fn=<SumBackward0>), tensor(33064.8438, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 177 Loss 1715.0286865234375 Accs [tensor(116267.3984, device='cuda:0', grad_fn=<SumBackward0>), tensor(17169.4922, device='cuda:0', grad_fn=<SumBackward0>), tensor(33030.1758, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 178 Loss 1715.0369873046875 Accs [tensor(116510.3359, device='cuda:0', grad_fn=<SumBackward0>), tensor(17196.8359, device='cuda:0', grad_fn=<SumBackward0>), tensor(33104.2539, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 179 Loss 1715.05078125 Accs [tensor(116431.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(17194.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(33074.3125, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 180 Loss 1715.07421875 Accs [tensor(116828.2031, device='cuda:0', grad_fn=<SumBackward0>), tensor(17240.1895, device='cuda:0', grad_fn=<SumBackward0>), tensor(33194.8125, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 181 Loss 1715.1138916015625 Accs [tensor(116782.6406, device='cuda:0', grad_fn=<SumBackward0>), tensor(17248.6406, device='cuda:0', grad_fn=<SumBackward0>), tensor(33168.8984, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 182 Loss 1715.1815185546875 Accs [tensor(117472.0859, device='cuda:0', grad_fn=<SumBackward0>), tensor(17328.5938, device='cuda:0', grad_fn=<SumBackward0>), tensor(33377.2891, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 183 Loss 1715.2930908203125 Accs [tensor(117200.8672, device='cuda:0', grad_fn=<SumBackward0>), tensor(17313.9512, device='cuda:0', grad_fn=<SumBackward0>), tensor(33279.9180, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 184 Loss 1715.4676513671875 Accs [tensor(117618.4062, device='cuda:0', grad_fn=<SumBackward0>), tensor(17343.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(33423.4375, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 185 Loss 1715.698486328125 Accs [tensor(116434.6875, device='cuda:0', grad_fn=<SumBackward0>), tensor(17207.5273, device='cuda:0', grad_fn=<SumBackward0>), tensor(33063.0664, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 186 Loss 1715.7752685546875 Accs [tensor(116814.1094, device='cuda:0', grad_fn=<SumBackward0>), tensor(17230.7305, device='cuda:0', grad_fn=<SumBackward0>), tensor(33198.8984, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 187 Loss 1715.504638671875 Accs [tensor(116367.6562, device='cuda:0', grad_fn=<SumBackward0>), tensor(17187.7051, device='cuda:0', grad_fn=<SumBackward0>), tensor(33054.7422, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 188 Loss 1715.13037109375 Accs [tensor(116448.0156, device='cuda:0', grad_fn=<SumBackward0>), tensor(17193.9688, device='cuda:0', grad_fn=<SumBackward0>), tensor(33082.1406, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 189 Loss 1715.013427734375 Accs [tensor(116635.4062, device='cuda:0', grad_fn=<SumBackward0>), tensor(17210.4531, device='cuda:0', grad_fn=<SumBackward0>), tensor(33143.8867, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 190 Loss 1715.2000732421875 Accs [tensor(116501.9531, device='cuda:0', grad_fn=<SumBackward0>), tensor(17211.8223, device='cuda:0', grad_fn=<SumBackward0>), tensor(33086.7539, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 191 Loss 1715.361083984375 Accs [tensor(116200.9688, device='cuda:0', grad_fn=<SumBackward0>), tensor(17147.0156, device='cuda:0', grad_fn=<SumBackward0>), tensor(33024.5391, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 192 Loss 1715.2452392578125 Accs [tensor(115725.6094, device='cuda:0', grad_fn=<SumBackward0>), tensor(17091.7246, device='cuda:0', grad_fn=<SumBackward0>), tensor(32879.3984, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 193 Loss 1715.038330078125 Accs [tensor(115737.1094, device='cuda:0', grad_fn=<SumBackward0>), tensor(17093.3262, device='cuda:0', grad_fn=<SumBackward0>), tensor(32882.7695, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 194 Loss 1715.038330078125 Accs [tensor(115842.0469, device='cuda:0', grad_fn=<SumBackward0>), tensor(17096.5430, device='cuda:0', grad_fn=<SumBackward0>), tensor(32923.8750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 195 Loss 1715.178955078125 Accs [tensor(115553.6953, device='cuda:0', grad_fn=<SumBackward0>), tensor(17071.5547, device='cuda:0', grad_fn=<SumBackward0>), tensor(32827.6562, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 196 Loss 1715.18212890625 Accs [tensor(116295.7969, device='cuda:0', grad_fn=<SumBackward0>), tensor(17166.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(33046.1328, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 197 Loss 1715.046630859375 Accs [tensor(117307.8281, device='cuda:0', grad_fn=<SumBackward0>), tensor(17313.5938, device='cuda:0', grad_fn=<SumBackward0>), tensor(33324.9297, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 198 Loss 1715.01513671875 Accs [tensor(118570.0234, device='cuda:0', grad_fn=<SumBackward0>), tensor(17503.3125, device='cuda:0', grad_fn=<SumBackward0>), tensor(33666.8867, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 199 Loss 1715.1058349609375 Accs [tensor(115714.7188, device='cuda:0', grad_fn=<SumBackward0>), tensor(17079.8594, device='cuda:0', grad_fn=<SumBackward0>), tensor(32889.1250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Best accs [tensor(101345.0703, device='cuda:0', grad_fn=<SumBackward0>), tensor(15013.8896, device='cuda:0', grad_fn=<SumBackward0>), tensor(28878.2695, device='cuda:0', grad_fn=<SumBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creates a HeteroGNN model from the previously constructed hetero graph and trains it.\n",
    "\"\"\"\n",
    "\n",
    "best_model = None\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    # Train\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    # Test for the accuracy of the model\n",
    "    accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n",
    "    print(f\"Epoch {epoch} Loss {loss} Accs {accs}\")\n",
    "\n",
    "# Get the accuracy of the best model\n",
    "best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx])\n",
    "\n",
    "print(\"Best accs\", best_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.8535], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([-6.1119], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([-6.0239], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([-5.7486], device='cuda:0', grad_fn=<SelectBackward0>) tensor([12.], device='cuda:0')\n",
      "tensor([-5.3623], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-6.2780], device='cuda:0', grad_fn=<SelectBackward0>) tensor([56.], device='cuda:0')\n",
      "tensor([-5.7104], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([-5.1195], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([-5.4131], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([15.9232], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([15.9233], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([15.9242], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([15.9233], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([15.9232], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([15.9231], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([15.9233], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([15.9232], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([15.9239], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([-5.6978], device='cuda:0', grad_fn=<SelectBackward0>) tensor([94.], device='cuda:0')\n",
      "tensor([15.9992], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([15.9234], device='cuda:0', grad_fn=<SelectBackward0>) tensor([11.], device='cuda:0')\n",
      "tensor([15.9234], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([15.9232], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([-5.6164], device='cuda:0', grad_fn=<SelectBackward0>) tensor([48.], device='cuda:0')\n",
      "tensor([15.9235], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([15.9230], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([15.9232], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([15.9239], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([-6.0307], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([-6.0895], device='cuda:0', grad_fn=<SelectBackward0>) tensor([111.], device='cuda:0')\n",
      "tensor([-6.2685], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([-5.1953], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([15.9233], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-5.8028], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-5.4910], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([15.9230], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([-5.9152], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([-5.2584], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([-5.2553], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([-6.2898], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([15.9231], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([-6.2536], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([15.9231], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([-6.0771], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([15.9992], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([-6.1353], device='cuda:0', grad_fn=<SelectBackward0>) tensor([58.], device='cuda:0')\n",
      "tensor([-5.4461], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-4.9768], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-5.5091], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([-4.4866], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([-5.7485], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-4.6532], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-5.0627], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([-5.2561], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([15.9992], device='cuda:0', grad_fn=<SelectBackward0>) tensor([14.], device='cuda:0')\n",
      "tensor([-5.6200], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([15.9230], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([15.9231], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([15.9992], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([-5.6964], device='cuda:0', grad_fn=<SelectBackward0>) tensor([32.], device='cuda:0')\n",
      "tensor([15.9232], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-6.0559], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([-6.0487], device='cuda:0', grad_fn=<SelectBackward0>) tensor([19.], device='cuda:0')\n",
      "tensor([-5.8773], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([-6.2793], device='cuda:0', grad_fn=<SelectBackward0>) tensor([144.], device='cuda:0')\n",
      "tensor([-5.5874], device='cuda:0', grad_fn=<SelectBackward0>) tensor([79.], device='cuda:0')\n",
      "tensor([-5.1279], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([-6.0790], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([15.9236], device='cuda:0', grad_fn=<SelectBackward0>) tensor([12.], device='cuda:0')\n",
      "tensor([-5.5607], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([15.9992], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([-5.7571], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([15.9992], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([15.9992], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([15.9239], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-5.9871], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([-5.5091], device='cuda:0', grad_fn=<SelectBackward0>) tensor([51.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "preds = best_model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "# mask = preds['event'] > 0\n",
    "# preds['event'][preds['event'] > 0].shape\n",
    "\n",
    "# print(preds['event'][0], hetero_graph.node_target['event'][0]) \n",
    "\n",
    "\n",
    "#print(hetero_graph.node_feature['event'])\n",
    "\n",
    "# for i in range(3000):\n",
    "#     if hetero_graph.node_target['event'][i] != -1: # concepts have node target -1\n",
    "#         print(preds['event'][i], hetero_graph.node_target['event'][i])\n",
    "        \n",
    "    \n",
    "for i in range(1000):    \n",
    "    if hetero_graph.node_target['event'][test_idx['event']][i] != -1:\n",
    "        print(preds['event'][test_idx['event']][i], hetero_graph.node_target['event'][test_idx['event']][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
