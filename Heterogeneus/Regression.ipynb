{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        self.lin_dst = nn.Linear(in_channels_dst, out_channels)\n",
    "        self.lin_src = nn.Linear(in_channels_src, out_channels)\n",
    "        self.lin_update = nn.Linear(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "        ):\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        \"\"\"\n",
    "        Initializes the HeteroGNNWrapperConv instance.\n",
    "\n",
    "        :param convs: Dictionary of convolution layers for each message type.\n",
    "        :param args: Arguments dictionary containing hyperparameters like hidden_size and attn_size.\n",
    "        :param aggr: Aggregation method, defaults to 'mean'.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_features: Dictionary of node features for each node type.\n",
    "        :param edge_indices: Dictionary of edge indices for each message type.\n",
    "        :return: Aggregated node embeddings for each node type.\n",
    "        \"\"\"\n",
    "        \n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        \n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        \n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "                \n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        \"\"\"\n",
    "        Aggregates node embeddings using the specified aggregation method.\n",
    "\n",
    "        :param xs: List of node embeddings to aggregate.\n",
    "        :return: Aggregated node embeddings as a torch.Tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates convolutional layers for each message type in a heterogeneous graph.\n",
    "\n",
    "    :param hetero_graph: The heterogeneous graph for which convolutions are to be created.\n",
    "    :param conv: The convolutional layer class or constructor.\n",
    "    :param hidden_size: The number of features in the hidden layer.\n",
    "    :param first_layer: Boolean indicating if this is the first layer in the network.\n",
    "    \n",
    "    :return: A dictionary of convolutional layers, keyed by message type.\n",
    "    \"\"\"\n",
    "\n",
    "    convs = {}\n",
    "    \n",
    "    # Extracting all types of messages/edges in the heterogeneous graph.\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        \n",
    "        # Determine the input feature size for source and destination nodes.\n",
    "        # If it's the first layer, use the feature size of the nodes.\n",
    "        # Otherwise, use the hidden size, since from there on the size of embeddings\n",
    "        # is the same for all nodes.\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        \n",
    "        # Create a convolutional layer for this message type and add it to the dictionary.\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, num_layers, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "        \n",
    "        # Initialize the graph convolutional layers\n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        # Initialize batch normalization, ReLU, and fully connected layers for each node type\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            \n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            \n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "            \n",
    "    def forward(self, node_feature, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_feature: Dictionary of node features for each node type.\n",
    "        :param edge_index: Dictionary of edge indices for each message type.\n",
    "        :return: The output embeddings for each node type after passing through the model.\n",
    "        \"\"\"\n",
    "        x = node_feature\n",
    "\n",
    "        # Apply graph convolutional, batch normalization, and ReLU layers\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        x = forward_op(x, self.fc)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        \"\"\"\n",
    "        Computes the loss for the model.\n",
    "\n",
    "        :param preds: Predictions made by the model.\n",
    "        :param y: Ground truth target values.\n",
    "        :param indices: Indices of nodes for which loss should be calculated.\n",
    "        \n",
    "        :return: The computed loss value.\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = 0\n",
    "        loss_func = torch.nn.MSELoss() \n",
    "             \n",
    "        mask = y['event'][indices['event'], 0] != -1\n",
    "        non_zero_idx = torch.masked_select(indices['event'], mask)\n",
    "                \n",
    "        loss += loss_func(preds['event'][non_zero_idx], y['event'][non_zero_idx])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    \"\"\"\n",
    "    Trains the model on the given heterogeneous graph using the specified indices.\n",
    "\n",
    "    :param model: The graph neural network model to train.\n",
    "    :param optimizer: The optimizer used for training the model.\n",
    "    :param hetero_graph: The heterogeneous graph data.\n",
    "    :param train_idx: Indices for training nodes.\n",
    "\n",
    "    :return: The training loss as a float.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "    optimizer.zero_grad() # Zero out any existing gradients \n",
    "    \n",
    "    # Compute predictions using the model\n",
    "    # TODO: Use only train_idx instead of edge_index\n",
    "    # TODO: Train only on events not on concepts\n",
    "    \n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    # Compute the loss using model's loss function\n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "\n",
    "    loss.backward() # Backward pass: compute gradient of the loss\n",
    "    optimizer.step() # Perform a single optimization step, updates parameters\n",
    "    \n",
    "    return loss.item() \n",
    "\n",
    "def test(model, graph, indices, best_model, best_tvt_scores):\n",
    "    \"\"\"\n",
    "    Tests the model on given indices and updates the best model based on validation loss.\n",
    "\n",
    "    :param model: The trained graph neural network model.\n",
    "    :param graph: The heterogeneous graph data.\n",
    "    :param indices: List of indices for training, validation, and testing nodes.\n",
    "    :param best_model: The current best model based on validation loss.\n",
    "    :param best_val: The current best validation loss.\n",
    "    \n",
    "    :return: A tuple containing the list of losses for each dataset, the best model, and the best validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    tvt_scores = []\n",
    "    \n",
    "    # Evaluate the model on each set of indices\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        \n",
    "        idx = index['event']\n",
    "\n",
    "        L1 = torch.sum(torch.abs(preds['event'][idx] - graph.node_target['event'][idx])) / idx.shape[0]\n",
    "        \n",
    "        tvt_scores.append(L1)\n",
    "    \n",
    "    # Update the best model and validation loss if the current model performs better\n",
    "    if tvt_scores[1] < best_tvt_scores[1]:\n",
    "        best_tvt_scores = tvt_scores\n",
    "        # torch.to_pickle(model, 'best_model.pkl')\n",
    "        # model.to_pickle('best_model.pkl')\n",
    "        # best_model = copy.deepcopy(model)\n",
    "        torch.save(model.state_dict(), './best_model.pkl')\n",
    "    \n",
    "    return tvt_scores, best_tvt_scores, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 48,\n",
    "    'epochs': 500,\n",
    "    'weight_decay': 0.0002930387278908051,\n",
    "    'lr': 0.05091434725288385,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell creates a small heterogeneous graph, primarily for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [1, 1, 1],   # event 0\n",
    "                [2, 2, 2]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([[50], [2000]], dtype=torch.float32),\n",
    "    # \"concept\": torch.tensor([[0], [0]], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}\n",
    "\n",
    "# Testing\n",
    "hetero_graph = HeteroGraph(\n",
    "    node_feature=S_node_feature,\n",
    "    node_target=S_node_targets,\n",
    "    edge_index=S_edge_index\n",
    ")\n",
    "\n",
    "train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./1_concepts_similar_llm (4).pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "    # Convert to directed graph for compatibility with Deepsnap\n",
    "    # G = G.to_directed()\n",
    "   \n",
    "        \n",
    "hetero_graph = HeteroGraph(G, netlib=nx, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 769\n",
      "\t Feature 769\n",
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 769\n",
      "\t Feature 1\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 1\n",
      "\t Feature 769\n",
      "KEY ('event', 'similar', 'event') <class 'tuple'>\n",
      "KEY NUMS ('event', 'similar', 'event') 8487 8487\n",
      "MAX EDGES tensor(8283) tensor(8283) 8487 8487\n",
      "KEY ('event', 'related', 'concept') <class 'tuple'>\n",
      "KEY NUMS ('event', 'related', 'concept') 8487 8729\n",
      "MAX EDGES tensor(8265) tensor(8728) 8487 8729\n",
      "KEY ('concept', 'related', 'event') <class 'tuple'>\n",
      "KEY NUMS ('concept', 'related', 'event') 8729 8487\n",
      "MAX EDGES tensor(8728) tensor(8265) 8729 8487\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block ensures that all tensors in a heterogeneous graph are transferred to the same \n",
    "computational device, as specified in the 'args' dictionary.\n",
    "\"\"\"\n",
    "\n",
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "# Send node features to device\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "# Create a torch.SparseTensor from edge_index and send it to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key, type(key))\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    \n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max(), hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
    "    \n",
    "# Send node targets to device\n",
    "for key in hetero_graph.node_target:\n",
    "    hetero_graph.node_target[key] = hetero_graph.node_target[key].to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5940])\n",
      "torch.Size([1698])\n",
      "torch.Size([849])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block creates a basic split of a graph's nodes into training, validation, and testing sets. \n",
    "It uses predefined ratios to divide 'event' and 'concept' nodes in the heterogeneous graph for a simple \n",
    "dataset split, mainly for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "nEvents = hetero_graph.num_nodes(\"event\")\n",
    "nConcepts = hetero_graph.num_nodes(\"concept\")\n",
    "\n",
    "s1 = 0.7\n",
    "s2 = 0.8\n",
    "\n",
    "train_idx = {   \"event\": torch.tensor(range(0, int(nEvents * s1))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(0, int(nConcepts * s1))).to(args[\"device\"])\n",
    "            }\n",
    "val_idx = {   \"event\": torch.tensor(range(int(nEvents * s1), int(nEvents * s2))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s1), int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "test_idx = {   \"event\": torch.tensor(range(int(nEvents * s2), nEvents)).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s2), nConcepts)).to(args[\"device\"])\n",
    "            }\n",
    "\n",
    "print(train_idx[\"event\"].shape)\n",
    "print(test_idx[\"event\"].shape)\n",
    "print(val_idx[\"event\"].shape)\n",
    "\n",
    "\n",
    "# TODO: Add node labels to the nodes and try to make the deepsnap split work even for regression!\n",
    "\n",
    "# dataset = deepsnap.dataset.GraphDataset([hetero_graph], task='node')\n",
    "\n",
    "# dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
    "# datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2817.4351 Current Train,Val,Test Scores [73.37296295166016, 72.3178939819336, 67.2185287475586]\n",
      "Epoch 1 Loss 2661.3945 Current Train,Val,Test Scores [127.10586547851562, 124.55751037597656, 116.45291137695312]\n",
      "Epoch 2 Loss 2209.1316 Current Train,Val,Test Scores [239.5946807861328, 237.00051879882812, 212.3839111328125]\n",
      "Epoch 3 Loss 1911.5636 Current Train,Val,Test Scores [406.8420715332031, 405.4908752441406, 366.10260009765625]\n",
      "Epoch 4 Loss 1713.6218 Current Train,Val,Test Scores [292.3856506347656, 291.435546875, 265.078125]\n",
      "Epoch 5 Loss 1686.1168 Current Train,Val,Test Scores [150.5601348876953, 150.25732421875, 138.033203125]\n",
      "Epoch 6 Loss 1711.5646 Current Train,Val,Test Scores [100.28548431396484, 100.97725677490234, 94.2756576538086]\n",
      "Epoch 7 Loss 1652.0020 Current Train,Val,Test Scores [231.53900146484375, 233.34344482421875, 208.87338256835938]\n",
      "Epoch 8 Loss 1536.9677 Current Train,Val,Test Scores [366.8499755859375, 369.4120788574219, 324.70355224609375]\n",
      "Epoch 9 Loss 1508.4781 Current Train,Val,Test Scores [427.4873046875, 429.90826416015625, 379.227783203125]\n",
      "Epoch 10 Loss 1512.9110 Current Train,Val,Test Scores [284.72613525390625, 287.1332092285156, 253.26487731933594]\n",
      "Epoch 11 Loss 1419.3145 Current Train,Val,Test Scores [214.67013549804688, 216.85147094726562, 190.89422607421875]\n",
      "Epoch 12 Loss 1558.8210 Current Train,Val,Test Scores [213.90040588378906, 214.929443359375, 192.86190795898438]\n",
      "Epoch 13 Loss 1453.8207 Current Train,Val,Test Scores [209.68234252929688, 210.13575744628906, 189.38265991210938]\n",
      "Epoch 14 Loss 1499.9211 Current Train,Val,Test Scores [192.6766357421875, 193.16709899902344, 173.46975708007812]\n",
      "Epoch 15 Loss 1300.7819 Current Train,Val,Test Scores [179.41748046875, 179.81752014160156, 160.89598083496094]\n",
      "Epoch 16 Loss 1458.3015 Current Train,Val,Test Scores [193.4753875732422, 193.64300537109375, 173.97970581054688]\n",
      "Epoch 17 Loss 1258.4001 Current Train,Val,Test Scores [211.468505859375, 211.35626220703125, 189.9114227294922]\n",
      "Epoch 18 Loss 1302.9410 Current Train,Val,Test Scores [204.57330322265625, 204.19667053222656, 182.50636291503906]\n",
      "Epoch 19 Loss 1248.3381 Current Train,Val,Test Scores [184.00790405273438, 183.4595947265625, 162.9982452392578]\n",
      "Epoch 20 Loss 1148.2173 Current Train,Val,Test Scores [175.15359497070312, 174.35780334472656, 154.8311004638672]\n",
      "Epoch 21 Loss 1202.9104 Current Train,Val,Test Scores [185.5456085205078, 184.3415985107422, 163.73565673828125]\n",
      "Epoch 22 Loss 1086.9722 Current Train,Val,Test Scores [201.05226135253906, 199.3916473388672, 177.20657348632812]\n",
      "Epoch 23 Loss 1068.0522 Current Train,Val,Test Scores [203.2283935546875, 201.39892578125, 178.8828125]\n",
      "Epoch 24 Loss 1044.1125 Current Train,Val,Test Scores [173.21511840820312, 171.6844940185547, 152.46128845214844]\n",
      "Epoch 25 Loss 942.4184 Current Train,Val,Test Scores [135.00311279296875, 133.87689208984375, 118.98545837402344]\n",
      "Epoch 26 Loss 974.2896 Current Train,Val,Test Scores [110.5043716430664, 109.70452880859375, 97.39164733886719]\n",
      "Epoch 27 Loss 858.2549 Current Train,Val,Test Scores [62.706443786621094, 62.522525787353516, 55.64469528198242]\n",
      "Epoch 28 Loss 865.7735 Current Train,Val,Test Scores [46.20429229736328, 47.27740478515625, 41.42970657348633]\n",
      "Epoch 29 Loss 793.2958 Current Train,Val,Test Scores [44.49304962158203, 45.500953674316406, 40.04599380493164]\n",
      "Epoch 30 Loss 801.1843 Current Train,Val,Test Scores [47.916324615478516, 48.82624816894531, 43.17118835449219]\n",
      "Epoch 31 Loss 740.7065 Current Train,Val,Test Scores [49.60201644897461, 50.53074264526367, 44.77244567871094]\n",
      "Epoch 32 Loss 739.1008 Current Train,Val,Test Scores [51.22884750366211, 52.092342376708984, 46.43313217163086]\n",
      "Epoch 33 Loss 698.5811 Current Train,Val,Test Scores [60.970218658447266, 61.83058547973633, 55.07559585571289]\n",
      "Epoch 34 Loss 663.5514 Current Train,Val,Test Scores [71.82908630371094, 72.74333190917969, 64.55904388427734]\n",
      "Epoch 35 Loss 656.0414 Current Train,Val,Test Scores [85.48555755615234, 86.41974639892578, 76.5440673828125]\n",
      "Epoch 36 Loss 635.5615 Current Train,Val,Test Scores [79.27751922607422, 80.28225708007812, 71.02307891845703]\n",
      "Epoch 37 Loss 584.8050 Current Train,Val,Test Scores [92.99726104736328, 92.81708526611328, 83.13216400146484]\n",
      "Epoch 38 Loss 579.3993 Current Train,Val,Test Scores [113.15225982666016, 112.42780303955078, 100.62416076660156]\n",
      "Epoch 39 Loss 574.5250 Current Train,Val,Test Scores [123.96345520019531, 123.0696029663086, 110.00666046142578]\n",
      "Epoch 40 Loss 542.0509 Current Train,Val,Test Scores [118.4700927734375, 117.58453369140625, 105.04025268554688]\n",
      "Epoch 41 Loss 527.2870 Current Train,Val,Test Scores [109.1740493774414, 108.33087921142578, 96.76203918457031]\n",
      "Epoch 42 Loss 525.0867 Current Train,Val,Test Scores [127.31063079833984, 126.14640045166016, 112.62519836425781]\n",
      "Epoch 43 Loss 516.2646 Current Train,Val,Test Scores [153.6361541748047, 152.01832580566406, 135.680419921875]\n",
      "Epoch 44 Loss 484.3944 Current Train,Val,Test Scores [152.46646118164062, 150.80587768554688, 134.63467407226562]\n",
      "Epoch 45 Loss 463.5772 Current Train,Val,Test Scores [127.1084976196289, 125.73833465576172, 112.38111114501953]\n",
      "Epoch 46 Loss 449.8712 Current Train,Val,Test Scores [98.43254089355469, 97.37432098388672, 87.21746826171875]\n",
      "Epoch 47 Loss 446.4398 Current Train,Val,Test Scores [72.2835693359375, 71.53691101074219, 64.316650390625]\n",
      "Epoch 48 Loss 434.0978 Current Train,Val,Test Scores [59.781768798828125, 59.151329040527344, 53.52499008178711]\n",
      "Epoch 49 Loss 430.0506 Current Train,Val,Test Scores [58.50347900390625, 57.8432502746582, 52.413761138916016]\n",
      "Epoch 50 Loss 401.6313 Current Train,Val,Test Scores [57.065185546875, 56.34421157836914, 51.08613204956055]\n",
      "Epoch 51 Loss 394.4077 Current Train,Val,Test Scores [47.86699676513672, 47.43281936645508, 43.088478088378906]\n",
      "Epoch 52 Loss 390.8250 Current Train,Val,Test Scores [35.54085159301758, 35.888187408447266, 32.297183990478516]\n",
      "Epoch 53 Loss 400.0349 Current Train,Val,Test Scores [24.449199676513672, 25.219139099121094, 22.428152084350586]\n",
      "Epoch 54 Loss 419.6214 Current Train,Val,Test Scores [27.172136306762695, 27.66839027404785, 24.87925910949707]\n",
      "Epoch 55 Loss 605.1671 Current Train,Val,Test Scores [20.44049072265625, 21.087940216064453, 18.924617767333984]\n",
      "Epoch 56 Loss 475.3458 Current Train,Val,Test Scores [19.66714096069336, 20.316429138183594, 18.222166061401367]\n",
      "Epoch 57 Loss 587.4189 Current Train,Val,Test Scores [14.98160171508789, 15.641518592834473, 14.14271354675293]\n",
      "Epoch 58 Loss 578.0588 Current Train,Val,Test Scores [11.277451515197754, 11.915264129638672, 10.875027656555176]\n",
      "Epoch 59 Loss 536.6075 Current Train,Val,Test Scores [165.41380310058594, 163.4066925048828, 145.33782958984375]\n",
      "Epoch 60 Loss 519.9350 Current Train,Val,Test Scores [30.575746536254883, 30.291969299316406, 26.985429763793945]\n",
      "Epoch 61 Loss 502.5680 Current Train,Val,Test Scores [192.21334838867188, 190.07789611816406, 168.64614868164062]\n",
      "Epoch 62 Loss 446.4952 Current Train,Val,Test Scores [351.32171630859375, 347.0372619628906, 308.1650695800781]\n",
      "Epoch 63 Loss 485.7120 Current Train,Val,Test Scores [260.0285949707031, 257.1939392089844, 228.0261993408203]\n",
      "Epoch 64 Loss 549.4623 Current Train,Val,Test Scores [398.96771240234375, 394.09014892578125, 349.9309387207031]\n",
      "Epoch 65 Loss 456.6870 Current Train,Val,Test Scores [401.1166076660156, 396.1463928222656, 351.8315734863281]\n",
      "Epoch 66 Loss 490.4123 Current Train,Val,Test Scores [47.172576904296875, 46.859413146972656, 41.199989318847656]\n",
      "Epoch 67 Loss 552.1865 Current Train,Val,Test Scores [8.378314971923828, 8.858603477478027, 8.276416778564453]\n",
      "Epoch 68 Loss 378.0286 Current Train,Val,Test Scores [7.136768341064453, 7.583789348602295, 7.158238887786865]\n",
      "Epoch 69 Loss 472.5925 Current Train,Val,Test Scores [12.794046401977539, 12.81478214263916, 12.221038818359375]\n",
      "Epoch 70 Loss 633.7975 Current Train,Val,Test Scores [28.671531677246094, 27.654191970825195, 25.90264320373535]\n",
      "Epoch 71 Loss 332.3725 Current Train,Val,Test Scores [31.22191047668457, 30.16705894470215, 28.078908920288086]\n",
      "Epoch 72 Loss 876.9219 Current Train,Val,Test Scores [19.205453872680664, 18.533973693847656, 17.746856689453125]\n",
      "Epoch 73 Loss 575.5768 Current Train,Val,Test Scores [5.113570213317871, 5.649619102478027, 5.262200832366943]\n",
      "Epoch 74 Loss 841.4124 Current Train,Val,Test Scores [8.269989013671875, 8.533129692077637, 8.143596649169922]\n",
      "Epoch 75 Loss 427.0474 Current Train,Val,Test Scores [5.991649150848389, 6.399333953857422, 6.133448600769043]\n",
      "Epoch 76 Loss 544.2921 Current Train,Val,Test Scores [5.637557506561279, 6.171398639678955, 5.7628703117370605]\n",
      "Epoch 77 Loss 486.7156 Current Train,Val,Test Scores [9.269993782043457, 9.761466979980469, 8.9110689163208]\n",
      "Epoch 78 Loss 349.0553 Current Train,Val,Test Scores [4.757222652435303, 5.313779830932617, 4.952979564666748]\n",
      "Epoch 79 Loss 529.7986 Current Train,Val,Test Scores [6.865468978881836, 7.192753314971924, 6.938514709472656]\n",
      "Epoch 80 Loss 439.7986 Current Train,Val,Test Scores [9.177711486816406, 9.339604377746582, 9.092494010925293]\n",
      "Epoch 81 Loss 287.3556 Current Train,Val,Test Scores [8.593029975891113, 9.048157691955566, 8.552069664001465]\n",
      "Epoch 82 Loss 456.1417 Current Train,Val,Test Scores [9.446609497070312, 9.914899826049805, 9.277166366577148]\n",
      "Epoch 83 Loss 354.5083 Current Train,Val,Test Scores [6.281686305999756, 6.802001953125, 6.445346355438232]\n",
      "Epoch 84 Loss 301.0218 Current Train,Val,Test Scores [6.995687484741211, 7.420651435852051, 6.958474159240723]\n",
      "Epoch 85 Loss 378.2097 Current Train,Val,Test Scores [26.7846736907959, 25.999895095825195, 23.995361328125]\n",
      "Epoch 86 Loss 340.2624 Current Train,Val,Test Scores [61.428070068359375, 60.432403564453125, 54.15232467651367]\n",
      "Epoch 87 Loss 272.5543 Current Train,Val,Test Scores [125.0480728149414, 123.36885833740234, 109.9251937866211]\n",
      "Epoch 88 Loss 327.6220 Current Train,Val,Test Scores [100.98433685302734, 99.5470199584961, 88.84099578857422]\n",
      "Epoch 89 Loss 297.3059 Current Train,Val,Test Scores [45.299560546875, 44.335655212402344, 40.0698356628418]\n",
      "Epoch 90 Loss 257.0841 Current Train,Val,Test Scores [36.197547912597656, 35.20075607299805, 32.161434173583984]\n",
      "Epoch 91 Loss 296.1903 Current Train,Val,Test Scores [54.47861862182617, 53.47536087036133, 48.069541931152344]\n",
      "Epoch 92 Loss 277.0656 Current Train,Val,Test Scores [133.97068786621094, 132.1529998779297, 117.70560455322266]\n",
      "Epoch 93 Loss 234.9935 Current Train,Val,Test Scores [226.46141052246094, 223.44007873535156, 198.82672119140625]\n",
      "Epoch 94 Loss 264.9939 Current Train,Val,Test Scores [204.99346923828125, 202.25222778320312, 179.97952270507812]\n",
      "Epoch 95 Loss 245.2771 Current Train,Val,Test Scores [115.28619384765625, 113.69793701171875, 101.26310729980469]\n",
      "Epoch 96 Loss 218.3187 Current Train,Val,Test Scores [79.87837982177734, 78.64909362792969, 70.2455062866211]\n",
      "Epoch 97 Loss 236.8840 Current Train,Val,Test Scores [127.56088256835938, 125.79985809326172, 112.04100799560547]\n",
      "Epoch 98 Loss 216.7448 Current Train,Val,Test Scores [245.78028869628906, 242.3895263671875, 215.76971435546875]\n",
      "Epoch 99 Loss 206.3663 Current Train,Val,Test Scores [303.55133056640625, 299.33245849609375, 266.4333801269531]\n",
      "Epoch 100 Loss 212.6917 Current Train,Val,Test Scores [237.2132110595703, 233.93426513671875, 208.20545959472656]\n",
      "Epoch 101 Loss 184.3048 Current Train,Val,Test Scores [168.44261169433594, 166.1269989013672, 147.85833740234375]\n",
      "Epoch 102 Loss 194.9598 Current Train,Val,Test Scores [206.43914794921875, 203.49908447265625, 181.20201110839844]\n",
      "Epoch 103 Loss 181.2452 Current Train,Val,Test Scores [323.6426086425781, 318.96185302734375, 284.02532958984375]\n",
      "Epoch 104 Loss 167.6904 Current Train,Val,Test Scores [364.3244934082031, 359.0570983886719, 319.6986999511719]\n",
      "Epoch 105 Loss 173.6480 Current Train,Val,Test Scores [203.8739013671875, 201.02267456054688, 178.75791931152344]\n",
      "Epoch 106 Loss 153.0741 Current Train,Val,Test Scores [55.92715072631836, 55.034610748291016, 48.8510856628418]\n",
      "Epoch 107 Loss 161.3123 Current Train,Val,Test Scores [52.88395309448242, 52.21150207519531, 46.106502532958984]\n",
      "Epoch 108 Loss 141.4437 Current Train,Val,Test Scores [121.64588928222656, 120.0655746459961, 106.42475128173828]\n",
      "Epoch 109 Loss 144.2267 Current Train,Val,Test Scores [33.592960357666016, 33.291481018066406, 29.14084243774414]\n",
      "Epoch 110 Loss 132.3146 Current Train,Val,Test Scores [4.28529691696167, 4.7115325927734375, 4.50398063659668]\n",
      "Epoch 111 Loss 130.7500 Current Train,Val,Test Scores [5.1175971031188965, 5.520065784454346, 5.2232465744018555]\n",
      "Epoch 112 Loss 123.9922 Current Train,Val,Test Scores [19.111881256103516, 18.789188385009766, 16.628957748413086]\n",
      "Epoch 113 Loss 120.2887 Current Train,Val,Test Scores [11.573474884033203, 11.583134651184082, 10.791213035583496]\n",
      "Epoch 114 Loss 115.7404 Current Train,Val,Test Scores [5.5998992919921875, 5.983950614929199, 5.675912380218506]\n",
      "Epoch 115 Loss 114.3689 Current Train,Val,Test Scores [11.50961685180664, 11.271059036254883, 10.597923278808594]\n",
      "Epoch 116 Loss 106.9003 Current Train,Val,Test Scores [182.87030029296875, 180.58116149902344, 160.09239196777344]\n",
      "Epoch 117 Loss 110.4768 Current Train,Val,Test Scores [100.34497833251953, 99.3311996459961, 87.70171356201172]\n",
      "Epoch 118 Loss 98.9155 Current Train,Val,Test Scores [73.4014663696289, 72.56810760498047, 64.07662200927734]\n",
      "Epoch 119 Loss 101.0022 Current Train,Val,Test Scores [191.18690490722656, 188.8401641845703, 167.45510864257812]\n",
      "Epoch 120 Loss 96.1675 Current Train,Val,Test Scores [162.81370544433594, 160.9051513671875, 142.54637145996094]\n",
      "Epoch 121 Loss 91.2778 Current Train,Val,Test Scores [68.54763793945312, 67.7919692993164, 59.795127868652344]\n",
      "Epoch 122 Loss 92.6100 Current Train,Val,Test Scores [130.2774200439453, 128.59242248535156, 113.97334289550781]\n",
      "Epoch 123 Loss 87.9943 Current Train,Val,Test Scores [84.06004333496094, 82.80680084228516, 73.38130950927734]\n",
      "Epoch 124 Loss 85.5241 Current Train,Val,Test Scores [4.695432662963867, 5.182046890258789, 4.85129451751709]\n",
      "Epoch 125 Loss 85.9477 Current Train,Val,Test Scores [12.131624221801758, 11.631329536437988, 11.1171293258667]\n",
      "Epoch 126 Loss 82.4016 Current Train,Val,Test Scores [4.823246479034424, 5.284610748291016, 4.986151695251465]\n",
      "Epoch 127 Loss 80.5090 Current Train,Val,Test Scores [4.202903747558594, 4.609560012817383, 4.473581314086914]\n",
      "Epoch 128 Loss 80.7993 Current Train,Val,Test Scores [4.307466506958008, 4.706658363342285, 4.572707653045654]\n",
      "Epoch 129 Loss 78.9807 Current Train,Val,Test Scores [4.950535297393799, 5.295324802398682, 5.097407341003418]\n",
      "Epoch 130 Loss 76.4923 Current Train,Val,Test Scores [9.351555824279785, 9.68372917175293, 9.026930809020996]\n",
      "Epoch 131 Loss 76.4076 Current Train,Val,Test Scores [10.1854887008667, 10.503435134887695, 9.75373649597168]\n",
      "Epoch 132 Loss 76.5340 Current Train,Val,Test Scores [9.9153413772583, 10.229043960571289, 9.498848915100098]\n",
      "Epoch 133 Loss 75.5250 Current Train,Val,Test Scores [4.814908027648926, 5.128978252410889, 4.958675861358643]\n",
      "Epoch 134 Loss 72.9278 Current Train,Val,Test Scores [3.1829452514648438, 3.6223344802856445, 3.565375804901123]\n",
      "Epoch 135 Loss 71.8069 Current Train,Val,Test Scores [2.950563430786133, 3.410642147064209, 3.383578300476074]\n",
      "Epoch 136 Loss 72.0281 Current Train,Val,Test Scores [35.41365051269531, 34.70175552368164, 31.16010284423828]\n",
      "Epoch 137 Loss 72.6339 Current Train,Val,Test Scores [77.44957733154297, 77.83330535888672, 69.20886993408203]\n",
      "Epoch 138 Loss 73.7842 Current Train,Val,Test Scores [87.66178131103516, 87.69092559814453, 78.07916259765625]\n",
      "Epoch 139 Loss 70.2490 Current Train,Val,Test Scores [85.73030853271484, 86.10614776611328, 76.11090850830078]\n",
      "Epoch 140 Loss 68.2635 Current Train,Val,Test Scores [85.22695922851562, 85.58918762207031, 75.59711456298828]\n",
      "Epoch 141 Loss 67.6071 Current Train,Val,Test Scores [77.38623046875, 77.81597900390625, 68.78297424316406]\n",
      "Epoch 142 Loss 68.3726 Current Train,Val,Test Scores [65.8577651977539, 66.22271728515625, 58.60784149169922]\n",
      "Epoch 143 Loss 68.5108 Current Train,Val,Test Scores [48.03453063964844, 48.409751892089844, 42.916419982910156]\n",
      "Epoch 144 Loss 66.8592 Current Train,Val,Test Scores [34.39189910888672, 34.73024368286133, 30.894868850708008]\n",
      "Epoch 145 Loss 64.5452 Current Train,Val,Test Scores [19.87652587890625, 20.225374221801758, 18.09987449645996]\n",
      "Epoch 146 Loss 63.0904 Current Train,Val,Test Scores [6.849736213684082, 7.202635765075684, 6.653525352478027]\n",
      "Epoch 147 Loss 62.1926 Current Train,Val,Test Scores [2.586268424987793, 3.038581132888794, 2.950869560241699]\n",
      "Epoch 148 Loss 62.5784 Current Train,Val,Test Scores [4.751064777374268, 5.311954021453857, 4.964046955108643]\n",
      "Epoch 149 Loss 63.0614 Current Train,Val,Test Scores [4.7143049240112305, 5.288911819458008, 4.877498149871826]\n",
      "Epoch 150 Loss 63.7074 Current Train,Val,Test Scores [4.86447286605835, 5.409974575042725, 5.0152268409729]\n",
      "Epoch 151 Loss 63.7940 Current Train,Val,Test Scores [5.105491638183594, 5.7007880210876465, 5.220213413238525]\n",
      "Epoch 152 Loss 63.7102 Current Train,Val,Test Scores [5.22518253326416, 5.773910045623779, 5.350308895111084]\n",
      "Epoch 153 Loss 63.1983 Current Train,Val,Test Scores [4.976083278656006, 5.5381178855896, 5.107401371002197]\n",
      "Epoch 154 Loss 62.7720 Current Train,Val,Test Scores [4.9278435707092285, 5.495412826538086, 5.06071138381958]\n",
      "Epoch 155 Loss 61.7765 Current Train,Val,Test Scores [5.50825309753418, 6.100777626037598, 5.561248302459717]\n",
      "Epoch 156 Loss 61.1807 Current Train,Val,Test Scores [5.106497287750244, 5.683400630950928, 5.21596097946167]\n",
      "Epoch 157 Loss 60.1554 Current Train,Val,Test Scores [5.092991352081299, 5.677572250366211, 5.19295072555542]\n",
      "Epoch 158 Loss 59.4542 Current Train,Val,Test Scores [4.95041561126709, 5.511145114898682, 5.079360485076904]\n",
      "Epoch 159 Loss 58.7920 Current Train,Val,Test Scores [5.587255001068115, 6.167887210845947, 5.629716396331787]\n",
      "Epoch 160 Loss 58.2538 Current Train,Val,Test Scores [5.987381935119629, 6.555455684661865, 5.979055404663086]\n",
      "Epoch 161 Loss 57.7666 Current Train,Val,Test Scores [6.092803478240967, 6.663604259490967, 6.068978309631348]\n",
      "Epoch 162 Loss 57.4450 Current Train,Val,Test Scores [6.242127418518066, 6.799509525299072, 6.2084479331970215]\n",
      "Epoch 163 Loss 57.0846 Current Train,Val,Test Scores [6.322908401489258, 6.899386882781982, 6.280760765075684]\n",
      "Epoch 164 Loss 56.9671 Current Train,Val,Test Scores [6.179246425628662, 6.772187232971191, 6.150317668914795]\n",
      "Epoch 165 Loss 56.8300 Current Train,Val,Test Scores [5.85783576965332, 6.444360256195068, 5.846942901611328]\n",
      "Epoch 166 Loss 57.1617 Current Train,Val,Test Scores [6.020009517669678, 6.591405868530273, 5.988349437713623]\n",
      "Epoch 167 Loss 57.8918 Current Train,Val,Test Scores [6.334481239318848, 6.923567771911621, 6.2664947509765625]\n",
      "Epoch 168 Loss 60.3577 Current Train,Val,Test Scores [7.252720832824707, 7.815730094909668, 7.105618476867676]\n",
      "Epoch 169 Loss 65.0668 Current Train,Val,Test Scores [7.172416687011719, 7.445880889892578, 7.044856548309326]\n",
      "Epoch 170 Loss 77.6865 Current Train,Val,Test Scores [13.603705406188965, 11.838240623474121, 12.038402557373047]\n",
      "Epoch 171 Loss 94.4529 Current Train,Val,Test Scores [7.812945365905762, 7.857081890106201, 7.388944625854492]\n",
      "Epoch 172 Loss 136.7651 Current Train,Val,Test Scores [53.64079666137695, 50.894737243652344, 46.741817474365234]\n",
      "Epoch 173 Loss 125.1186 Current Train,Val,Test Scores [108.31716918945312, 108.67809295654297, 96.10289001464844]\n",
      "Epoch 174 Loss 115.2525 Current Train,Val,Test Scores [147.25830078125, 147.6300048828125, 130.41262817382812]\n",
      "Epoch 175 Loss 62.2229 Current Train,Val,Test Scores [177.7735595703125, 178.1826629638672, 157.28050231933594]\n",
      "Epoch 176 Loss 62.0473 Current Train,Val,Test Scores [195.34048461914062, 195.74298095703125, 173.0773468017578]\n",
      "Epoch 177 Loss 90.7025 Current Train,Val,Test Scores [192.7560577392578, 193.2432098388672, 170.9356689453125]\n",
      "Epoch 178 Loss 72.7424 Current Train,Val,Test Scores [193.12606811523438, 193.63467407226562, 171.38369750976562]\n",
      "Epoch 179 Loss 60.2657 Current Train,Val,Test Scores [183.0506591796875, 183.5395050048828, 162.27854919433594]\n",
      "Epoch 180 Loss 61.4394 Current Train,Val,Test Scores [168.54965209960938, 169.0608367919922, 149.24288940429688]\n",
      "Epoch 181 Loss 75.6605 Current Train,Val,Test Scores [160.45281982421875, 160.99917602539062, 142.0514373779297]\n",
      "Epoch 182 Loss 94.2908 Current Train,Val,Test Scores [139.39279174804688, 139.96463012695312, 123.57023620605469]\n",
      "Epoch 183 Loss 65.1261 Current Train,Val,Test Scores [119.4343032836914, 119.97992706298828, 106.03506469726562]\n",
      "Epoch 184 Loss 108.9546 Current Train,Val,Test Scores [115.70162963867188, 116.2308120727539, 102.74617767333984]\n",
      "Epoch 185 Loss 98.1777 Current Train,Val,Test Scores [96.42613220214844, 96.99230194091797, 85.74109649658203]\n",
      "Epoch 186 Loss 85.3390 Current Train,Val,Test Scores [75.2288589477539, 75.78812408447266, 67.06127166748047]\n",
      "Epoch 187 Loss 80.0054 Current Train,Val,Test Scores [63.592491149902344, 64.1119613647461, 56.78474044799805]\n",
      "Epoch 188 Loss 96.7510 Current Train,Val,Test Scores [55.77424621582031, 56.31086730957031, 49.88309097290039]\n",
      "Epoch 189 Loss 62.9627 Current Train,Val,Test Scores [303.3185119628906, 297.71435546875, 266.1985778808594]\n",
      "Epoch 190 Loss 95.4875 Current Train,Val,Test Scores [97.20298767089844, 94.2926025390625, 85.2857437133789]\n",
      "Epoch 191 Loss 75.3631 Current Train,Val,Test Scores [101.33868408203125, 97.91819763183594, 88.95246887207031]\n",
      "Epoch 192 Loss 62.7740 Current Train,Val,Test Scores [23.409374237060547, 23.926279067993164, 21.352685928344727]\n",
      "Epoch 193 Loss 72.2568 Current Train,Val,Test Scores [32.413204193115234, 32.80625915527344, 29.296489715576172]\n",
      "Epoch 194 Loss 73.3626 Current Train,Val,Test Scores [33.65873336791992, 33.89842987060547, 30.34349822998047]\n",
      "Epoch 195 Loss 65.8640 Current Train,Val,Test Scores [23.73029899597168, 24.031715393066406, 21.547739028930664]\n",
      "Epoch 196 Loss 72.9633 Current Train,Val,Test Scores [24.645784378051758, 24.886701583862305, 22.336973190307617]\n",
      "Epoch 197 Loss 65.5361 Current Train,Val,Test Scores [17.078697204589844, 17.4227294921875, 15.691522598266602]\n",
      "Epoch 198 Loss 57.4049 Current Train,Val,Test Scores [10.081048011779785, 10.502195358276367, 9.58228874206543]\n",
      "Epoch 199 Loss 68.7970 Current Train,Val,Test Scores [8.537193298339844, 8.995479583740234, 8.267938613891602]\n",
      "Epoch 200 Loss 50.8977 Current Train,Val,Test Scores [5.378879547119141, 5.887589931488037, 5.513399600982666]\n",
      "Epoch 201 Loss 59.4300 Current Train,Val,Test Scores [178.4668731689453, 175.9495849609375, 156.25]\n",
      "Epoch 202 Loss 57.8401 Current Train,Val,Test Scores [95.88349914550781, 94.34716796875, 83.95296478271484]\n",
      "Epoch 203 Loss 52.0247 Current Train,Val,Test Scores [6.058241844177246, 6.557613372802734, 6.115500450134277]\n",
      "Epoch 204 Loss 50.6955 Current Train,Val,Test Scores [9.068488121032715, 9.550280570983887, 8.769874572753906]\n",
      "Epoch 205 Loss 53.4247 Current Train,Val,Test Scores [10.524230003356934, 10.988168716430664, 10.0479154586792]\n",
      "Epoch 206 Loss 52.6545 Current Train,Val,Test Scores [12.951050758361816, 13.387251853942871, 12.175796508789062]\n",
      "Epoch 207 Loss 48.6070 Current Train,Val,Test Scores [12.96325969696045, 13.400550842285156, 12.187111854553223]\n",
      "Epoch 208 Loss 56.7386 Current Train,Val,Test Scores [8.193684577941895, 8.694517135620117, 8.011091232299805]\n",
      "Epoch 209 Loss 47.6135 Current Train,Val,Test Scores [4.598519325256348, 5.138352394104004, 4.893855571746826]\n",
      "Epoch 210 Loss 48.4801 Current Train,Val,Test Scores [124.253173828125, 120.84796905517578, 109.406982421875]\n",
      "Epoch 211 Loss 46.6861 Current Train,Val,Test Scores [206.35472106933594, 203.08905029296875, 180.8734130859375]\n",
      "Epoch 212 Loss 49.7891 Current Train,Val,Test Scores [335.8674011230469, 330.9208984375, 294.5508117675781]\n",
      "Epoch 213 Loss 44.7126 Current Train,Val,Test Scores [396.5693359375, 391.19427490234375, 347.63153076171875]\n",
      "Epoch 214 Loss 49.7482 Current Train,Val,Test Scores [231.5579376220703, 227.93641662597656, 202.96414184570312]\n",
      "Epoch 215 Loss 47.2198 Current Train,Val,Test Scores [153.87899780273438, 150.19000244140625, 135.3761444091797]\n",
      "Epoch 216 Loss 45.0516 Current Train,Val,Test Scores [84.46483612060547, 81.44377899169922, 74.63868713378906]\n",
      "Epoch 217 Loss 45.2280 Current Train,Val,Test Scores [27.54222869873047, 28.097965240478516, 25.169540405273438]\n",
      "Epoch 218 Loss 45.5520 Current Train,Val,Test Scores [26.363008499145508, 26.931257247924805, 24.080074310302734]\n",
      "Epoch 219 Loss 44.1476 Current Train,Val,Test Scores [30.018722534179688, 30.608375549316406, 27.30878257751465]\n",
      "Epoch 220 Loss 43.7886 Current Train,Val,Test Scores [35.8632926940918, 36.39141845703125, 32.494895935058594]\n",
      "Epoch 221 Loss 45.2548 Current Train,Val,Test Scores [26.549367904663086, 27.089879989624023, 24.264570236206055]\n",
      "Epoch 222 Loss 41.8622 Current Train,Val,Test Scores [15.247832298278809, 15.824485778808594, 14.241257667541504]\n",
      "Epoch 223 Loss 43.4270 Current Train,Val,Test Scores [17.381160736083984, 17.95675277709961, 16.108999252319336]\n",
      "Epoch 224 Loss 42.9587 Current Train,Val,Test Scores [13.99661636352539, 14.531968116760254, 13.131013870239258]\n",
      "Epoch 225 Loss 42.6071 Current Train,Val,Test Scores [3.312295436859131, 3.813560724258423, 3.576432466506958]\n",
      "Epoch 226 Loss 41.6353 Current Train,Val,Test Scores [5.809267997741699, 6.372588634490967, 5.926399230957031]\n",
      "Epoch 227 Loss 42.5859 Current Train,Val,Test Scores [4.764732360839844, 5.2713446617126465, 5.0400543212890625]\n",
      "Epoch 228 Loss 41.2305 Current Train,Val,Test Scores [4.968014240264893, 5.467514991760254, 5.2269182205200195]\n",
      "Epoch 229 Loss 40.6117 Current Train,Val,Test Scores [5.008723735809326, 5.573219299316406, 5.203039646148682]\n",
      "Epoch 230 Loss 41.3235 Current Train,Val,Test Scores [4.169974327087402, 4.663697242736816, 4.504363536834717]\n",
      "Epoch 231 Loss 40.5458 Current Train,Val,Test Scores [4.418071746826172, 4.908130168914795, 4.734692573547363]\n",
      "Epoch 232 Loss 40.3760 Current Train,Val,Test Scores [5.244644641876221, 5.814257621765137, 5.411240100860596]\n",
      "Epoch 233 Loss 39.8305 Current Train,Val,Test Scores [4.442878246307373, 5.013454914093018, 4.718613147735596]\n",
      "Epoch 234 Loss 39.8791 Current Train,Val,Test Scores [4.058084011077881, 4.576708793640137, 4.411879539489746]\n",
      "Epoch 235 Loss 39.6954 Current Train,Val,Test Scores [4.44339656829834, 4.98027229309082, 4.7078704833984375]\n",
      "Epoch 236 Loss 38.9338 Current Train,Val,Test Scores [4.349173069000244, 4.89729642868042, 4.627150058746338]\n",
      "Epoch 237 Loss 38.8403 Current Train,Val,Test Scores [3.9852442741394043, 4.494779586791992, 4.377917766571045]\n",
      "Epoch 238 Loss 39.5199 Current Train,Val,Test Scores [5.459268093109131, 6.041922092437744, 5.601199150085449]\n",
      "Epoch 239 Loss 40.9179 Current Train,Val,Test Scores [80.21564483642578, 80.80181884765625, 71.59893035888672]\n",
      "Epoch 240 Loss 43.5204 Current Train,Val,Test Scores [158.177001953125, 158.9173126220703, 140.85787963867188]\n",
      "Epoch 241 Loss 41.4762 Current Train,Val,Test Scores [183.42637634277344, 184.1554412841797, 163.12872314453125]\n",
      "Epoch 242 Loss 40.8418 Current Train,Val,Test Scores [178.90928649902344, 179.68997192382812, 158.79898071289062]\n",
      "Epoch 243 Loss 41.0251 Current Train,Val,Test Scores [168.04730224609375, 168.72193908691406, 148.9292755126953]\n",
      "Epoch 244 Loss 41.9033 Current Train,Val,Test Scores [153.7036590576172, 154.4394989013672, 136.0409393310547]\n",
      "Epoch 245 Loss 40.1628 Current Train,Val,Test Scores [139.9596710205078, 140.67796325683594, 123.95760345458984]\n",
      "Epoch 246 Loss 39.6896 Current Train,Val,Test Scores [126.05601501464844, 126.77297973632812, 111.82816314697266]\n",
      "Epoch 247 Loss 39.8969 Current Train,Val,Test Scores [111.13983154296875, 111.850341796875, 98.67736053466797]\n",
      "Epoch 248 Loss 40.4338 Current Train,Val,Test Scores [96.9793930053711, 97.65969848632812, 86.17507934570312]\n",
      "Epoch 249 Loss 39.1144 Current Train,Val,Test Scores [84.17926788330078, 84.85757446289062, 74.89622497558594]\n",
      "Epoch 250 Loss 38.7012 Current Train,Val,Test Scores [72.8204116821289, 73.49339294433594, 64.9221420288086]\n",
      "Epoch 251 Loss 39.0246 Current Train,Val,Test Scores [64.08653259277344, 64.75390625, 57.231754302978516]\n",
      "Epoch 252 Loss 38.9354 Current Train,Val,Test Scores [57.91312026977539, 58.56278610229492, 51.78044128417969]\n",
      "Epoch 253 Loss 38.5404 Current Train,Val,Test Scores [51.7745246887207, 52.42253494262695, 46.384273529052734]\n",
      "Epoch 254 Loss 37.9223 Current Train,Val,Test Scores [45.59601593017578, 46.25068283081055, 40.93288040161133]\n",
      "Epoch 255 Loss 38.2606 Current Train,Val,Test Scores [38.844974517822266, 39.49655532836914, 34.969688415527344]\n",
      "Epoch 256 Loss 37.8715 Current Train,Val,Test Scores [32.4750862121582, 33.12506103515625, 29.34480094909668]\n",
      "Epoch 257 Loss 37.5926 Current Train,Val,Test Scores [27.138402938842773, 27.79366683959961, 24.638124465942383]\n",
      "Epoch 258 Loss 37.5417 Current Train,Val,Test Scores [23.348636627197266, 24.004568099975586, 21.286409378051758]\n",
      "Epoch 259 Loss 37.1296 Current Train,Val,Test Scores [20.46992301940918, 21.118181228637695, 18.733659744262695]\n",
      "Epoch 260 Loss 37.1528 Current Train,Val,Test Scores [38.94171142578125, 39.58651351928711, 35.0241813659668]\n",
      "Epoch 261 Loss 38.5190 Current Train,Val,Test Scores [70.11457061767578, 70.78722381591797, 62.45048522949219]\n",
      "Epoch 262 Loss 37.2731 Current Train,Val,Test Scores [89.39404296875, 90.07964324951172, 79.55374145507812]\n",
      "Epoch 263 Loss 38.2060 Current Train,Val,Test Scores [96.25922393798828, 96.94216918945312, 85.39762115478516]\n",
      "Epoch 264 Loss 37.7587 Current Train,Val,Test Scores [98.06761169433594, 98.75747680664062, 86.96373748779297]\n",
      "Epoch 265 Loss 37.4629 Current Train,Val,Test Scores [97.4328842163086, 98.12175750732422, 86.42005920410156]\n",
      "Epoch 266 Loss 37.7939 Current Train,Val,Test Scores [89.59059143066406, 90.26988983154297, 79.5016098022461]\n",
      "Epoch 267 Loss 36.3699 Current Train,Val,Test Scores [76.81672668457031, 77.53209686279297, 68.25325012207031]\n",
      "Epoch 268 Loss 36.4960 Current Train,Val,Test Scores [62.00439453125, 62.696189880371094, 55.334163665771484]\n",
      "Epoch 269 Loss 36.6295 Current Train,Val,Test Scores [45.686058044433594, 46.37079620361328, 41.071537017822266]\n",
      "Epoch 270 Loss 35.7914 Current Train,Val,Test Scores [37.8626708984375, 38.52987289428711, 34.20484161376953]\n",
      "Epoch 271 Loss 35.1903 Current Train,Val,Test Scores [33.112667083740234, 33.7690315246582, 29.92717742919922]\n",
      "Epoch 272 Loss 35.5413 Current Train,Val,Test Scores [25.037384033203125, 25.6976318359375, 22.774118423461914]\n",
      "Epoch 273 Loss 35.8937 Current Train,Val,Test Scores [19.780202865600586, 20.43624496459961, 18.047155380249023]\n",
      "Epoch 274 Loss 34.2789 Current Train,Val,Test Scores [14.218356132507324, 14.865849494934082, 13.128355026245117]\n",
      "Epoch 275 Loss 34.0547 Current Train,Val,Test Scores [7.816756248474121, 8.447176933288574, 7.4669342041015625]\n",
      "Epoch 276 Loss 35.3052 Current Train,Val,Test Scores [6.114633083343506, 6.726850509643555, 5.9368414878845215]\n",
      "Epoch 277 Loss 35.3989 Current Train,Val,Test Scores [1.3992111682891846, 2.0111474990844727, 1.803909420967102]\n",
      "Epoch 278 Loss 39.7158 Current Train,Val,Test Scores [5.982632637023926, 6.577944755554199, 5.820144176483154]\n",
      "Epoch 279 Loss 44.7978 Current Train,Val,Test Scores [5.096020698547363, 5.687861919403076, 5.087435722351074]\n",
      "Epoch 280 Loss 62.8038 Current Train,Val,Test Scores [8.118439674377441, 8.611701965332031, 7.871119976043701]\n",
      "Epoch 281 Loss 104.9964 Current Train,Val,Test Scores [3.776118755340576, 4.320174217224121, 4.008137226104736]\n",
      "Epoch 282 Loss 73.0162 Current Train,Val,Test Scores [8.710615158081055, 9.223713874816895, 8.357382774353027]\n",
      "Epoch 283 Loss 49.3242 Current Train,Val,Test Scores [8.130876541137695, 8.624174118041992, 7.886058807373047]\n",
      "Epoch 284 Loss 32.9065 Current Train,Val,Test Scores [6.400607585906982, 7.01261043548584, 6.532674312591553]\n",
      "Epoch 285 Loss 51.0418 Current Train,Val,Test Scores [29.870361328125, 30.04997444152832, 25.48201560974121]\n",
      "Epoch 286 Loss 92.7351 Current Train,Val,Test Scores [157.78109741210938, 156.9383087158203, 137.75833129882812]\n",
      "Epoch 287 Loss 50.7243 Current Train,Val,Test Scores [171.4827117919922, 170.6802215576172, 149.6734161376953]\n",
      "Epoch 288 Loss 56.8931 Current Train,Val,Test Scores [77.09999084472656, 77.44367218017578, 69.33677673339844]\n",
      "Epoch 289 Loss 88.2458 Current Train,Val,Test Scores [195.95033264160156, 195.26422119140625, 171.46371459960938]\n",
      "Epoch 290 Loss 51.8097 Current Train,Val,Test Scores [184.38360595703125, 184.186767578125, 161.5441436767578]\n",
      "Epoch 291 Loss 54.8452 Current Train,Val,Test Scores [170.94512939453125, 171.5281524658203, 151.42686462402344]\n",
      "Epoch 292 Loss 79.2537 Current Train,Val,Test Scores [183.19363403320312, 183.73646545410156, 162.17115783691406]\n",
      "Epoch 293 Loss 47.4422 Current Train,Val,Test Scores [182.5369415283203, 183.05462646484375, 161.51681518554688]\n",
      "Epoch 294 Loss 79.6778 Current Train,Val,Test Scores [168.9845428466797, 169.50405883789062, 149.52342224121094]\n",
      "Epoch 295 Loss 110.7603 Current Train,Val,Test Scores [158.63624572753906, 159.14694213867188, 140.44009399414062]\n",
      "Epoch 296 Loss 80.4929 Current Train,Val,Test Scores [146.8901824951172, 147.39312744140625, 130.19561767578125]\n",
      "Epoch 297 Loss 86.0731 Current Train,Val,Test Scores [134.824462890625, 135.29324340820312, 119.74457550048828]\n",
      "Epoch 298 Loss 76.1083 Current Train,Val,Test Scores [127.37358856201172, 127.87025451660156, 111.54521942138672]\n",
      "Epoch 299 Loss 55.6478 Current Train,Val,Test Scores [495.9132385253906, 491.287353515625, 434.7597961425781]\n",
      "Epoch 300 Loss 82.4557 Current Train,Val,Test Scores [356.8478698730469, 353.9413757324219, 312.6750793457031]\n",
      "Epoch 301 Loss 82.7563 Current Train,Val,Test Scores [604.0027465820312, 597.0663452148438, 529.8865356445312]\n",
      "Epoch 302 Loss 67.0822 Current Train,Val,Test Scores [617.4398193359375, 610.3440551757812, 541.4581298828125]\n",
      "Epoch 303 Loss 60.0798 Current Train,Val,Test Scores [298.4759216308594, 295.7350158691406, 261.3016052246094]\n",
      "Epoch 304 Loss 84.5824 Current Train,Val,Test Scores [133.16310119628906, 130.98861694335938, 116.75366973876953]\n",
      "Epoch 305 Loss 43.7304 Current Train,Val,Test Scores [59.27256774902344, 59.812042236328125, 52.96552276611328]\n",
      "Epoch 306 Loss 77.9856 Current Train,Val,Test Scores [46.65570068359375, 47.259857177734375, 41.878936767578125]\n",
      "Epoch 307 Loss 61.8363 Current Train,Val,Test Scores [40.85166549682617, 41.45008087158203, 36.75705337524414]\n",
      "Epoch 308 Loss 50.0718 Current Train,Val,Test Scores [37.9203987121582, 38.48731231689453, 34.16173553466797]\n",
      "Epoch 309 Loss 69.7594 Current Train,Val,Test Scores [32.742305755615234, 33.30516052246094, 29.595687866210938]\n",
      "Epoch 310 Loss 63.8573 Current Train,Val,Test Scores [21.135141372680664, 21.788558959960938, 19.393901824951172]\n",
      "Epoch 311 Loss 47.2488 Current Train,Val,Test Scores [17.543319702148438, 18.127933502197266, 16.251636505126953]\n",
      "Epoch 312 Loss 71.6745 Current Train,Val,Test Scores [12.13617992401123, 12.7985258102417, 11.450078010559082]\n",
      "Epoch 313 Loss 47.7903 Current Train,Val,Test Scores [14.296056747436523, 14.903857231140137, 13.327201843261719]\n",
      "Epoch 314 Loss 48.2826 Current Train,Val,Test Scores [13.73535442352295, 14.30811882019043, 12.827596664428711]\n",
      "Epoch 315 Loss 55.3978 Current Train,Val,Test Scores [9.727927207946777, 10.298904418945312, 9.260910034179688]\n",
      "Epoch 316 Loss 42.3029 Current Train,Val,Test Scores [8.664632797241211, 9.235774993896484, 8.334988594055176]\n",
      "Epoch 317 Loss 49.1189 Current Train,Val,Test Scores [9.33648681640625, 9.864266395568848, 8.930797576904297]\n",
      "Epoch 318 Loss 43.0336 Current Train,Val,Test Scores [9.878157615661621, 10.40549373626709, 9.410122871398926]\n",
      "Epoch 319 Loss 40.6174 Current Train,Val,Test Scores [9.03316593170166, 9.621724128723145, 8.683067321777344]\n",
      "Epoch 320 Loss 45.5582 Current Train,Val,Test Scores [9.97614574432373, 10.530856132507324, 9.486969947814941]\n",
      "Epoch 321 Loss 38.1261 Current Train,Val,Test Scores [9.563746452331543, 10.110024452209473, 9.110823631286621]\n",
      "Epoch 322 Loss 42.4703 Current Train,Val,Test Scores [9.858662605285645, 10.422760009765625, 9.369391441345215]\n",
      "Epoch 323 Loss 39.1112 Current Train,Val,Test Scores [9.375814437866211, 9.965267181396484, 8.950404167175293]\n",
      "Epoch 324 Loss 37.5205 Current Train,Val,Test Scores [6.502735614776611, 7.150299072265625, 6.476296424865723]\n",
      "Epoch 325 Loss 40.9458 Current Train,Val,Test Scores [7.701748371124268, 7.0617194175720215, 7.401228904724121]\n",
      "Epoch 326 Loss 34.6562 Current Train,Val,Test Scores [101.66830444335938, 97.99823760986328, 89.84928131103516]\n",
      "Epoch 327 Loss 38.1324 Current Train,Val,Test Scores [201.33050537109375, 197.43067932128906, 176.63693237304688]\n",
      "Epoch 328 Loss 34.5530 Current Train,Val,Test Scores [226.022216796875, 222.87939453125, 197.68511962890625]\n",
      "Epoch 329 Loss 34.8703 Current Train,Val,Test Scores [186.5494384765625, 183.9261474609375, 163.0801239013672]\n",
      "Epoch 330 Loss 35.1676 Current Train,Val,Test Scores [181.3335723876953, 178.79226684570312, 158.50869750976562]\n",
      "Epoch 331 Loss 32.8208 Current Train,Val,Test Scores [171.46267700195312, 169.14266967773438, 149.84088134765625]\n",
      "Epoch 332 Loss 34.0232 Current Train,Val,Test Scores [97.05016326904297, 96.10066223144531, 84.40441131591797]\n",
      "Epoch 333 Loss 32.2361 Current Train,Val,Test Scores [13.699581146240234, 13.945403099060059, 12.700254440307617]\n",
      "Epoch 334 Loss 31.4657 Current Train,Val,Test Scores [11.807461738586426, 12.392019271850586, 11.006733894348145]\n",
      "Epoch 335 Loss 31.8056 Current Train,Val,Test Scores [10.978142738342285, 11.558935165405273, 10.42619800567627]\n",
      "Epoch 336 Loss 30.2125 Current Train,Val,Test Scores [12.404668807983398, 12.912318229675293, 11.691441535949707]\n",
      "Epoch 337 Loss 30.3438 Current Train,Val,Test Scores [12.493894577026367, 13.094643592834473, 11.774388313293457]\n",
      "Epoch 338 Loss 29.2385 Current Train,Val,Test Scores [20.099531173706055, 20.075130462646484, 17.13800621032715]\n",
      "Epoch 339 Loss 28.3093 Current Train,Val,Test Scores [16.995628356933594, 17.06722068786621, 15.248144149780273]\n",
      "Epoch 340 Loss 30.2334 Current Train,Val,Test Scores [41.476444244384766, 41.3785400390625, 35.28669738769531]\n",
      "Epoch 341 Loss 37.2851 Current Train,Val,Test Scores [16.00812530517578, 16.50211524963379, 14.906939506530762]\n",
      "Epoch 342 Loss 33.2537 Current Train,Val,Test Scores [18.415943145751953, 18.874853134155273, 16.910856246948242]\n",
      "Epoch 343 Loss 34.2692 Current Train,Val,Test Scores [16.74414825439453, 17.194427490234375, 15.546974182128906]\n",
      "Epoch 344 Loss 33.5371 Current Train,Val,Test Scores [16.978679656982422, 17.424238204956055, 15.817842483520508]\n",
      "Epoch 345 Loss 32.7414 Current Train,Val,Test Scores [19.657690048217773, 20.093257904052734, 18.211631774902344]\n",
      "Epoch 346 Loss 31.2510 Current Train,Val,Test Scores [18.681665420532227, 19.225536346435547, 17.342079162597656]\n",
      "Epoch 347 Loss 31.3765 Current Train,Val,Test Scores [17.26374626159668, 17.81473159790039, 16.0881404876709]\n",
      "Epoch 348 Loss 32.0975 Current Train,Val,Test Scores [19.068653106689453, 19.630273818969727, 17.8519287109375]\n",
      "Epoch 349 Loss 31.5571 Current Train,Val,Test Scores [29.133563995361328, 29.041210174560547, 24.922090530395508]\n",
      "Epoch 350 Loss 30.1460 Current Train,Val,Test Scores [79.51364135742188, 79.0821762084961, 68.74735260009766]\n",
      "Epoch 351 Loss 28.5055 Current Train,Val,Test Scores [38.9073600769043, 38.952110290527344, 33.06397247314453]\n",
      "Epoch 352 Loss 37.6945 Current Train,Val,Test Scores [74.98545837402344, 74.5379409790039, 65.00190734863281]\n",
      "Epoch 353 Loss 51.0353 Current Train,Val,Test Scores [96.26888275146484, 96.79027557373047, 86.47470092773438]\n",
      "Epoch 354 Loss 38.3181 Current Train,Val,Test Scores [148.29727172851562, 148.7388153076172, 131.9300994873047]\n",
      "Epoch 355 Loss 44.8379 Current Train,Val,Test Scores [178.04100036621094, 178.4478759765625, 157.79446411132812]\n",
      "Epoch 356 Loss 39.9473 Current Train,Val,Test Scores [191.1153564453125, 191.55203247070312, 169.11962890625]\n",
      "Epoch 357 Loss 37.5855 Current Train,Val,Test Scores [197.61343383789062, 198.0787353515625, 174.7304229736328]\n",
      "Epoch 358 Loss 45.6149 Current Train,Val,Test Scores [200.09738159179688, 200.52293395996094, 176.8716278076172]\n",
      "Epoch 359 Loss 37.2122 Current Train,Val,Test Scores [198.05364990234375, 198.4559326171875, 175.10552978515625]\n",
      "Epoch 360 Loss 34.4210 Current Train,Val,Test Scores [188.59115600585938, 188.99868774414062, 166.87338256835938]\n",
      "Epoch 361 Loss 37.5079 Current Train,Val,Test Scores [169.7408447265625, 170.3669891357422, 150.4295654296875]\n",
      "Epoch 362 Loss 34.5430 Current Train,Val,Test Scores [174.73435974121094, 174.0894012451172, 153.1149139404297]\n",
      "Epoch 363 Loss 37.3406 Current Train,Val,Test Scores [209.9109344482422, 208.8373565673828, 184.05628967285156]\n",
      "Epoch 364 Loss 35.8908 Current Train,Val,Test Scores [162.10231018066406, 159.3806610107422, 142.9583740234375]\n",
      "Epoch 365 Loss 32.0797 Current Train,Val,Test Scores [115.26950073242188, 115.85135650634766, 102.6014175415039]\n",
      "Epoch 366 Loss 34.0434 Current Train,Val,Test Scores [102.11506652832031, 102.72027587890625, 91.05946350097656]\n",
      "Epoch 367 Loss 33.4583 Current Train,Val,Test Scores [89.17782592773438, 89.76749420166016, 79.65289306640625]\n",
      "Epoch 368 Loss 32.9017 Current Train,Val,Test Scores [76.35235595703125, 76.96771240234375, 68.37763214111328]\n",
      "Epoch 369 Loss 33.9672 Current Train,Val,Test Scores [66.52779388427734, 67.1461410522461, 59.758514404296875]\n",
      "Epoch 370 Loss 31.4617 Current Train,Val,Test Scores [59.21958541870117, 59.815364837646484, 53.348209381103516]\n",
      "Epoch 371 Loss 30.8258 Current Train,Val,Test Scores [50.49568557739258, 51.08112716674805, 45.68682861328125]\n",
      "Epoch 372 Loss 31.7349 Current Train,Val,Test Scores [39.23184585571289, 39.83137512207031, 35.780391693115234]\n",
      "Epoch 373 Loss 30.4475 Current Train,Val,Test Scores [27.188722610473633, 27.80961799621582, 25.234451293945312]\n",
      "Epoch 374 Loss 30.6506 Current Train,Val,Test Scores [17.29286766052246, 17.877826690673828, 16.527149200439453]\n",
      "Epoch 375 Loss 29.7035 Current Train,Val,Test Scores [13.1961088180542, 13.763904571533203, 12.937810897827148]\n",
      "Epoch 376 Loss 28.0827 Current Train,Val,Test Scores [11.590682029724121, 12.115416526794434, 11.515049934387207]\n",
      "Epoch 377 Loss 28.7842 Current Train,Val,Test Scores [9.951468467712402, 10.439566612243652, 10.093853950500488]\n",
      "Epoch 378 Loss 28.0679 Current Train,Val,Test Scores [8.255060195922852, 8.836776733398438, 8.560408592224121]\n",
      "Epoch 379 Loss 27.1634 Current Train,Val,Test Scores [6.3274922370910645, 6.9150776863098145, 6.765224456787109]\n",
      "Epoch 380 Loss 26.8925 Current Train,Val,Test Scores [4.887720108032227, 5.532228469848633, 5.603246688842773]\n",
      "Epoch 381 Loss 25.7673 Current Train,Val,Test Scores [4.074334144592285, 4.9427170753479, 5.112530708312988]\n",
      "Epoch 382 Loss 25.6030 Current Train,Val,Test Scores [10.963371276855469, 10.072046279907227, 9.786028861999512]\n",
      "Epoch 383 Loss 24.4110 Current Train,Val,Test Scores [49.655433654785156, 49.19148635864258, 42.50130081176758]\n",
      "Epoch 384 Loss 23.8243 Current Train,Val,Test Scores [74.5599594116211, 73.86459350585938, 64.32962036132812]\n",
      "Epoch 385 Loss 23.3266 Current Train,Val,Test Scores [76.5425033569336, 75.84928131103516, 65.98506164550781]\n",
      "Epoch 386 Loss 22.1552 Current Train,Val,Test Scores [60.109214782714844, 59.62464904785156, 51.43490219116211]\n",
      "Epoch 387 Loss 22.3393 Current Train,Val,Test Scores [36.57440948486328, 36.36748123168945, 30.864023208618164]\n",
      "Epoch 388 Loss 21.3156 Current Train,Val,Test Scores [6.687079429626465, 6.525882720947266, 6.0108323097229]\n",
      "Epoch 389 Loss 21.1569 Current Train,Val,Test Scores [10.28105640411377, 10.727685928344727, 9.678417205810547]\n",
      "Epoch 390 Loss 20.6674 Current Train,Val,Test Scores [13.369051933288574, 13.760499954223633, 12.406259536743164]\n",
      "Epoch 391 Loss 20.3158 Current Train,Val,Test Scores [15.531057357788086, 15.887713432312012, 14.30292797088623]\n",
      "Epoch 392 Loss 26.3752 Current Train,Val,Test Scores [19.447908401489258, 19.765249252319336, 17.724695205688477]\n",
      "Epoch 393 Loss 31.3431 Current Train,Val,Test Scores [20.034189224243164, 20.348234176635742, 18.259462356567383]\n",
      "Epoch 394 Loss 22.5180 Current Train,Val,Test Scores [24.079256057739258, 24.401411056518555, 21.789316177368164]\n",
      "Epoch 395 Loss 25.8126 Current Train,Val,Test Scores [31.22283935546875, 31.517513275146484, 28.066421508789062]\n",
      "Epoch 396 Loss 23.0893 Current Train,Val,Test Scores [28.65850830078125, 28.968088150024414, 25.825807571411133]\n",
      "Epoch 397 Loss 26.3012 Current Train,Val,Test Scores [35.97416305541992, 36.294776916503906, 32.306640625]\n",
      "Epoch 398 Loss 23.6003 Current Train,Val,Test Scores [41.60910415649414, 41.949398040771484, 37.34828186035156]\n",
      "Epoch 399 Loss 23.0969 Current Train,Val,Test Scores [45.224220275878906, 45.55085372924805, 40.55783462524414]\n",
      "Epoch 400 Loss 22.5129 Current Train,Val,Test Scores [40.7503547668457, 41.08211898803711, 36.61329650878906]\n",
      "Epoch 401 Loss 23.7138 Current Train,Val,Test Scores [35.03385543823242, 35.429866790771484, 31.57387924194336]\n",
      "Epoch 402 Loss 19.5221 Current Train,Val,Test Scores [35.333290100097656, 35.69680404663086, 31.84672737121582]\n",
      "Epoch 403 Loss 18.6329 Current Train,Val,Test Scores [18.094614028930664, 18.44023323059082, 16.68853187561035]\n",
      "Epoch 404 Loss 19.1545 Current Train,Val,Test Scores [16.186115264892578, 16.39395523071289, 14.847735404968262]\n",
      "Epoch 405 Loss 18.9645 Current Train,Val,Test Scores [73.2852783203125, 73.7542953491211, 66.9083480834961]\n",
      "Epoch 406 Loss 37.0088 Current Train,Val,Test Scores [246.0780792236328, 245.68014526367188, 220.33010864257812]\n",
      "Epoch 407 Loss 30.8073 Current Train,Val,Test Scores [396.1484680175781, 395.7166442871094, 358.8282775878906]\n",
      "Epoch 408 Loss 73.3979 Current Train,Val,Test Scores [448.9187927246094, 445.4176330566406, 395.4230041503906]\n",
      "Epoch 409 Loss 215.5116 Current Train,Val,Test Scores [63.47040557861328, 64.01212310791016, 56.90069580078125]\n",
      "Epoch 410 Loss 276.2612 Current Train,Val,Test Scores [146.4904022216797, 146.5089111328125, 130.8314666748047]\n",
      "Epoch 411 Loss 97.8811 Current Train,Val,Test Scores [199.66058349609375, 199.14736938476562, 176.7811279296875]\n",
      "Epoch 412 Loss 459.6508 Current Train,Val,Test Scores [145.96299743652344, 145.94142150878906, 130.97744750976562]\n",
      "Epoch 413 Loss 293.0546 Current Train,Val,Test Scores [122.61246490478516, 122.60992431640625, 110.89846801757812]\n",
      "Epoch 414 Loss 436.4886 Current Train,Val,Test Scores [416.9007263183594, 412.3346862792969, 366.795166015625]\n",
      "Epoch 415 Loss 149.6821 Current Train,Val,Test Scores [873.8954467773438, 861.9859008789062, 767.5098266601562]\n",
      "Epoch 416 Loss 425.8850 Current Train,Val,Test Scores [208.2665252685547, 206.06863403320312, 185.47515869140625]\n",
      "Epoch 417 Loss 401.8441 Current Train,Val,Test Scores [295.8218688964844, 292.271240234375, 262.4532165527344]\n",
      "Epoch 418 Loss 660.4551 Current Train,Val,Test Scores [67.27367401123047, 66.92431640625, 61.160797119140625]\n",
      "Epoch 419 Loss 741.5166 Current Train,Val,Test Scores [97.00736236572266, 95.1084976196289, 86.53524780273438]\n",
      "Epoch 420 Loss 637.8299 Current Train,Val,Test Scores [67.2119369506836, 65.8526840209961, 59.95505905151367]\n",
      "Epoch 421 Loss 829.6405 Current Train,Val,Test Scores [37.54657745361328, 37.7034912109375, 34.080326080322266]\n",
      "Epoch 422 Loss 720.4273 Current Train,Val,Test Scores [119.37801361083984, 118.27374267578125, 105.93653106689453]\n",
      "Epoch 423 Loss 617.6275 Current Train,Val,Test Scores [142.3108367919922, 140.8422393798828, 125.9620590209961]\n",
      "Epoch 424 Loss 725.2808 Current Train,Val,Test Scores [93.60743713378906, 92.7230453491211, 83.07735443115234]\n",
      "Epoch 425 Loss 623.2327 Current Train,Val,Test Scores [52.420936584472656, 52.43601989746094, 46.9102897644043]\n",
      "Epoch 426 Loss 510.9142 Current Train,Val,Test Scores [40.637245178222656, 41.05371856689453, 36.5037727355957]\n",
      "Epoch 427 Loss 536.7726 Current Train,Val,Test Scores [205.12612915039062, 203.85935974121094, 179.73904418945312]\n",
      "Epoch 428 Loss 460.2344 Current Train,Val,Test Scores [374.6802062988281, 370.57275390625, 328.6252746582031]\n",
      "Epoch 429 Loss 484.4711 Current Train,Val,Test Scores [576.6156616210938, 569.8451538085938, 505.71807861328125]\n",
      "Epoch 430 Loss 406.3506 Current Train,Val,Test Scores [823.2239379882812, 812.851318359375, 722.1010131835938]\n",
      "Epoch 431 Loss 334.6047 Current Train,Val,Test Scores [1057.6656494140625, 1043.7598876953125, 927.8681640625]\n",
      "Epoch 432 Loss 251.3223 Current Train,Val,Test Scores [1318.670654296875, 1300.7391357421875, 1156.9769287109375]\n",
      "Epoch 433 Loss 239.0669 Current Train,Val,Test Scores [1474.515625, 1454.15771484375, 1293.7276611328125]\n",
      "Epoch 434 Loss 174.2591 Current Train,Val,Test Scores [1627.378662109375, 1604.689453125, 1427.823486328125]\n",
      "Epoch 435 Loss 300.7629 Current Train,Val,Test Scores [1505.135009765625, 1484.2088623046875, 1320.5416259765625]\n",
      "Epoch 436 Loss 248.2179 Current Train,Val,Test Scores [1164.072998046875, 1148.2252197265625, 1021.1543579101562]\n",
      "Epoch 437 Loss 183.4132 Current Train,Val,Test Scores [709.8306274414062, 700.8015747070312, 622.3809204101562]\n",
      "Epoch 438 Loss 268.3484 Current Train,Val,Test Scores [469.5164794921875, 463.5462951660156, 411.715576171875]\n",
      "Epoch 439 Loss 160.3759 Current Train,Val,Test Scores [103.6004867553711, 102.32283020019531, 90.69011688232422]\n",
      "Epoch 440 Loss 210.2581 Current Train,Val,Test Scores [58.41465377807617, 58.8162841796875, 52.168731689453125]\n",
      "Epoch 441 Loss 123.1311 Current Train,Val,Test Scores [59.758399963378906, 60.015708923339844, 53.3630485534668]\n",
      "Epoch 442 Loss 171.0652 Current Train,Val,Test Scores [58.18326950073242, 58.61563491821289, 51.918052673339844]\n",
      "Epoch 443 Loss 124.5473 Current Train,Val,Test Scores [63.11609649658203, 63.139225006103516, 56.33331298828125]\n",
      "Epoch 444 Loss 158.2374 Current Train,Val,Test Scores [74.0578384399414, 73.1747817993164, 65.72817993164062]\n",
      "Epoch 445 Loss 122.0366 Current Train,Val,Test Scores [82.51625061035156, 81.46361541748047, 72.99766540527344]\n",
      "Epoch 446 Loss 113.9823 Current Train,Val,Test Scores [84.79578399658203, 83.65052795410156, 74.99256896972656]\n",
      "Epoch 447 Loss 123.8673 Current Train,Val,Test Scores [81.78079223632812, 80.6438217163086, 72.35224914550781]\n",
      "Epoch 448 Loss 111.3593 Current Train,Val,Test Scores [71.1027603149414, 70.07278442382812, 62.981468200683594]\n",
      "Epoch 449 Loss 118.2250 Current Train,Val,Test Scores [55.202144622802734, 54.33297348022461, 49.213199615478516]\n",
      "Epoch 450 Loss 89.3978 Current Train,Val,Test Scores [40.67660903930664, 40.691715240478516, 36.58222579956055]\n",
      "Epoch 451 Loss 99.7673 Current Train,Val,Test Scores [31.749441146850586, 32.19603729248047, 28.631677627563477]\n",
      "Epoch 452 Loss 90.6175 Current Train,Val,Test Scores [26.765718460083008, 27.292861938476562, 24.295150756835938]\n",
      "Epoch 453 Loss 86.0275 Current Train,Val,Test Scores [49.327877044677734, 47.09550857543945, 44.04039764404297]\n",
      "Epoch 454 Loss 79.8322 Current Train,Val,Test Scores [273.2713623046875, 269.4100646972656, 239.87960815429688]\n",
      "Epoch 455 Loss 73.2290 Current Train,Val,Test Scores [384.4864807128906, 379.50628662109375, 337.13201904296875]\n",
      "Epoch 456 Loss 78.9040 Current Train,Val,Test Scores [393.610595703125, 388.3570556640625, 345.1958312988281]\n",
      "Epoch 457 Loss 66.5562 Current Train,Val,Test Scores [375.5452575683594, 370.4623107910156, 329.3861999511719]\n",
      "Epoch 458 Loss 76.5365 Current Train,Val,Test Scores [369.36907958984375, 364.3892822265625, 323.9380187988281]\n",
      "Epoch 459 Loss 65.9394 Current Train,Val,Test Scores [362.75714111328125, 357.99639892578125, 318.0072021484375]\n",
      "Epoch 460 Loss 67.5534 Current Train,Val,Test Scores [287.65313720703125, 283.9942932128906, 252.08139038085938]\n",
      "Epoch 461 Loss 65.4053 Current Train,Val,Test Scores [172.366455078125, 170.17620849609375, 151.00665283203125]\n",
      "Epoch 462 Loss 63.3691 Current Train,Val,Test Scores [116.38089752197266, 114.95601654052734, 101.90164947509766]\n",
      "Epoch 463 Loss 61.7994 Current Train,Val,Test Scores [138.3519744873047, 136.56312561035156, 121.17607879638672]\n",
      "Epoch 464 Loss 59.6031 Current Train,Val,Test Scores [215.93121337890625, 213.22726440429688, 189.05825805664062]\n",
      "Epoch 465 Loss 58.5720 Current Train,Val,Test Scores [303.9535827636719, 300.05755615234375, 266.22186279296875]\n",
      "Epoch 466 Loss 54.2340 Current Train,Val,Test Scores [370.6544494628906, 365.68902587890625, 324.8031005859375]\n",
      "Epoch 467 Loss 57.2080 Current Train,Val,Test Scores [392.113525390625, 386.84881591796875, 343.6349182128906]\n",
      "Epoch 468 Loss 52.8192 Current Train,Val,Test Scores [366.4250183105469, 361.45574951171875, 321.0729064941406]\n",
      "Epoch 469 Loss 54.0177 Current Train,Val,Test Scores [296.8192138671875, 292.9177551269531, 259.9401550292969]\n",
      "Epoch 470 Loss 51.2615 Current Train,Val,Test Scores [238.02157592773438, 234.9589385986328, 208.32778930664062]\n",
      "Epoch 471 Loss 53.2106 Current Train,Val,Test Scores [213.3444366455078, 210.66966247558594, 186.64476013183594]\n",
      "Epoch 472 Loss 49.6553 Current Train,Val,Test Scores [197.11276245117188, 194.72059631347656, 172.36151123046875]\n",
      "Epoch 473 Loss 50.1040 Current Train,Val,Test Scores [152.51559448242188, 150.77476501464844, 133.20350646972656]\n",
      "Epoch 474 Loss 48.0852 Current Train,Val,Test Scores [88.4827651977539, 87.62944030761719, 77.01587677001953]\n",
      "Epoch 475 Loss 47.8161 Current Train,Val,Test Scores [40.702877044677734, 40.40914535522461, 34.984886169433594]\n",
      "Epoch 476 Loss 46.4099 Current Train,Val,Test Scores [31.295000076293945, 31.152896881103516, 26.861228942871094]\n",
      "Epoch 477 Loss 46.5329 Current Train,Val,Test Scores [49.517921447753906, 49.12554168701172, 42.83841323852539]\n",
      "Epoch 478 Loss 44.8527 Current Train,Val,Test Scores [83.96381378173828, 83.07108306884766, 73.06983947753906]\n",
      "Epoch 479 Loss 44.4207 Current Train,Val,Test Scores [113.59933471679688, 112.35025024414062, 99.13093566894531]\n",
      "Epoch 480 Loss 43.9346 Current Train,Val,Test Scores [135.25637817382812, 133.6631622314453, 118.15261840820312]\n",
      "Epoch 481 Loss 43.4313 Current Train,Val,Test Scores [145.1953887939453, 143.41485595703125, 126.8836669921875]\n",
      "Epoch 482 Loss 42.4661 Current Train,Val,Test Scores [158.63018798828125, 156.63844299316406, 138.66978454589844]\n",
      "Epoch 483 Loss 42.6177 Current Train,Val,Test Scores [182.1267547607422, 179.7286834716797, 159.29818725585938]\n",
      "Epoch 484 Loss 41.8013 Current Train,Val,Test Scores [199.9596710205078, 197.20802307128906, 174.98208618164062]\n",
      "Epoch 485 Loss 41.9496 Current Train,Val,Test Scores [188.17234802246094, 185.5664520263672, 164.6495819091797]\n",
      "Epoch 486 Loss 41.3573 Current Train,Val,Test Scores [151.39683532714844, 149.302734375, 132.38937377929688]\n",
      "Epoch 487 Loss 41.1763 Current Train,Val,Test Scores [112.71144104003906, 111.13871765136719, 98.4473648071289]\n",
      "Epoch 488 Loss 40.5219 Current Train,Val,Test Scores [87.26145935058594, 86.0335922241211, 76.05569458007812]\n",
      "Epoch 489 Loss 40.7010 Current Train,Val,Test Scores [76.70710754394531, 75.62493896484375, 66.76390838623047]\n",
      "Epoch 490 Loss 39.9359 Current Train,Val,Test Scores [76.93241882324219, 75.86831665039062, 66.96001434326172]\n",
      "Epoch 491 Loss 40.0567 Current Train,Val,Test Scores [78.83747863769531, 77.75570678710938, 68.62303924560547]\n",
      "Epoch 492 Loss 39.5553 Current Train,Val,Test Scores [79.63318634033203, 78.51497650146484, 69.35201263427734]\n",
      "Epoch 493 Loss 39.4695 Current Train,Val,Test Scores [84.43218231201172, 83.30596923828125, 73.53085327148438]\n",
      "Epoch 494 Loss 39.0588 Current Train,Val,Test Scores [100.46001434326172, 99.1006851196289, 87.59284210205078]\n",
      "Epoch 495 Loss 38.9142 Current Train,Val,Test Scores [124.25666809082031, 122.53592681884766, 108.46231079101562]\n",
      "Epoch 496 Loss 38.4950 Current Train,Val,Test Scores [143.1465606689453, 141.1543731689453, 125.02928924560547]\n",
      "Epoch 497 Loss 38.3372 Current Train,Val,Test Scores [149.6645050048828, 147.5752410888672, 130.74502563476562]\n",
      "Epoch 498 Loss 38.1724 Current Train,Val,Test Scores [144.44088745117188, 142.4474334716797, 126.14073944091797]\n",
      "Epoch 499 Loss 37.7380 Current Train,Val,Test Scores [133.04266357421875, 131.1223602294922, 116.26640319824219]\n",
      "Best Train,Val,Test Scores [1.3992111682891846, 2.0111474990844727, 1.803909420967102]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creates a HeteroGNN model from the previously constructed hetero graph and trains it.\n",
    "\"\"\"\n",
    "\n",
    "best_model = None\n",
    "best_tvt_scores = (float(\"inf\"),float(\"inf\"),float(\"inf\"))\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    # Train\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    # Test for the accuracy of the model\n",
    "    cur_tvt_scores, best_tvt_scores, best_model = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_tvt_scores)\n",
    "    print(f\"Epoch {epoch} Loss {loss:.4f} Current Train,Val,Test Scores {[score.item() for score in cur_tvt_scores]}\")\n",
    "\n",
    "print(\"Best Train,Val,Test Scores\", [score.item() for score in best_tvt_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.8662], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([9.0660], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([126.0548], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([19.6090], device='cuda:0', grad_fn=<SelectBackward0>) tensor([94.], device='cuda:0')\n",
      "tensor([203.3217], device='cuda:0', grad_fn=<SelectBackward0>) tensor([48.], device='cuda:0')\n",
      "tensor([13.9356], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([170.8710], device='cuda:0', grad_fn=<SelectBackward0>) tensor([111.], device='cuda:0')\n",
      "tensor([10.1348], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([15.5526], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([4.1641], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([5.8842], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([11.1614], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([11.3835], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([14.1907], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([6.1016], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([7.5009], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([5.6754], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([10.1697], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([22.4297], device='cuda:0', grad_fn=<SelectBackward0>) tensor([58.], device='cuda:0')\n",
      "tensor([6.7624], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([3.3222], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([8.5024], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([3.9212], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([21.1564], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.0782], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([9.1948], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([13.7462], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([13.3861], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([178.7333], device='cuda:0', grad_fn=<SelectBackward0>) tensor([32.], device='cuda:0')\n",
      "tensor([13.3117], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([14.4331], device='cuda:0', grad_fn=<SelectBackward0>) tensor([19.], device='cuda:0')\n",
      "tensor([15.3503], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([21.5679], device='cuda:0', grad_fn=<SelectBackward0>) tensor([144.], device='cuda:0')\n",
      "tensor([95.1850], device='cuda:0', grad_fn=<SelectBackward0>) tensor([79.], device='cuda:0')\n",
      "tensor([4.8015], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([8.1912], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([12.8766], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([8.8815], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([7.5661], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([14.9038], device='cuda:0', grad_fn=<SelectBackward0>) tensor([51.], device='cuda:0')\n",
      "tensor([5.9410], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([7.2047], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([86.8079], device='cuda:0', grad_fn=<SelectBackward0>) tensor([40.], device='cuda:0')\n",
      "tensor([11.0570], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([6.4944], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([223.4184], device='cuda:0', grad_fn=<SelectBackward0>) tensor([607.], device='cuda:0')\n",
      "tensor([9.3704], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([10.1209], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([19.7471], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "model.load_state_dict(torch.load('./best_model.pkl'))\n",
    "preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "# mask = preds['event'] > 0\n",
    "# preds['event'][preds['event'] > 0].shape\n",
    "\n",
    "# print(preds['event'][0], hetero_graph.node_target['event'][0]) \n",
    "\n",
    "\n",
    "#print(hetero_graph.node_feature['event'])\n",
    "\n",
    "# for i in range(3000):\n",
    "#     if hetero_graph.node_target['event'][i] != -1: # concepts have node target -1\n",
    "#         print(preds['event'][i], hetero_graph.node_target['event'][i])\n",
    "        \n",
    "    \n",
    "for i in range(1000):    \n",
    "    if hetero_graph.node_target['event'][test_idx['event']][i] != -1:\n",
    "        print(preds['event'][test_idx['event']][i], hetero_graph.node_target['event'][test_idx['event']][i])\n",
    "\n",
    "\n",
    "# for i in range(1000):\n",
    "#     # Extract the target value and check if it is not equal to -1\n",
    "#     target = hetero_graph.node_target['event'][test_idx['event']][i].item()\n",
    "#     if target != -1:\n",
    "#         # Extract the prediction value\n",
    "#         pred = preds['event'][test_idx['event']][i].item()\n",
    "#         print(f\"Prediction: {pred:.4f}, Target: {target:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
