{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        self.lin_dst = nn.Linear(in_channels_dst, out_channels)\n",
    "        self.lin_src = nn.Linear(in_channels_src, out_channels)\n",
    "        self.lin_update = nn.Linear(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "        ):\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        \"\"\"\n",
    "        Initializes the HeteroGNNWrapperConv instance.\n",
    "\n",
    "        :param convs: Dictionary of convolution layers for each message type.\n",
    "        :param args: Arguments dictionary containing hyperparameters like hidden_size and attn_size.\n",
    "        :param aggr: Aggregation method, defaults to 'mean'.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_features: Dictionary of node features for each node type.\n",
    "        :param edge_indices: Dictionary of edge indices for each message type.\n",
    "        :return: Aggregated node embeddings for each node type.\n",
    "        \"\"\"\n",
    "        \n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        \n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        \n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "                \n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        \"\"\"\n",
    "        Aggregates node embeddings using the specified aggregation method.\n",
    "\n",
    "        :param xs: List of node embeddings to aggregate.\n",
    "        :return: Aggregated node embeddings as a torch.Tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates convolutional layers for each message type in a heterogeneous graph.\n",
    "\n",
    "    :param hetero_graph: The heterogeneous graph for which convolutions are to be created.\n",
    "    :param conv: The convolutional layer class or constructor.\n",
    "    :param hidden_size: The number of features in the hidden layer.\n",
    "    :param first_layer: Boolean indicating if this is the first layer in the network.\n",
    "    \n",
    "    :return: A dictionary of convolutional layers, keyed by message type.\n",
    "    \"\"\"\n",
    "\n",
    "    convs = {}\n",
    "    \n",
    "    # Extracting all types of messages/edges in the heterogeneous graph.\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        \n",
    "        # Determine the input feature size for source and destination nodes.\n",
    "        # If it's the first layer, use the feature size of the nodes.\n",
    "        # Otherwise, use the hidden size, since from there on the size of embeddings\n",
    "        # is the same for all nodes.\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        \n",
    "        # Create a convolutional layer for this message type and add it to the dictionary.\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, num_layers, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "        \n",
    "        # Initialize the graph convolutional layers\n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        # Initialize batch normalization, ReLU, and fully connected layers for each node type\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            \n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            \n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "            \n",
    "    def forward(self, node_feature, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_feature: Dictionary of node features for each node type.\n",
    "        :param edge_index: Dictionary of edge indices for each message type.\n",
    "        :return: The output embeddings for each node type after passing through the model.\n",
    "        \"\"\"\n",
    "        x = node_feature\n",
    "\n",
    "        # Apply graph convolutional, batch normalization, and ReLU layers\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        x = forward_op(x, self.fc)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        \"\"\"\n",
    "        Computes the loss for the model.\n",
    "\n",
    "        :param preds: Predictions made by the model.\n",
    "        :param y: Ground truth target values.\n",
    "        :param indices: Indices of nodes for which loss should be calculated.\n",
    "        \n",
    "        :return: The computed loss value.\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = 0\n",
    "        loss_func = torch.nn.MSELoss() \n",
    "             \n",
    "        mask = y['event'][indices['event'], 0] != -1\n",
    "        non_zero_idx = torch.masked_select(indices['event'], mask)\n",
    "                \n",
    "        loss += loss_func(preds['event'][non_zero_idx], y['event'][non_zero_idx])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    \"\"\"\n",
    "    Trains the model on the given heterogeneous graph using the specified indices.\n",
    "\n",
    "    :param model: The graph neural network model to train.\n",
    "    :param optimizer: The optimizer used for training the model.\n",
    "    :param hetero_graph: The heterogeneous graph data.\n",
    "    :param train_idx: Indices for training nodes.\n",
    "\n",
    "    :return: The training loss as a float.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "    optimizer.zero_grad() # Zero out any existing gradients \n",
    "    \n",
    "    # Compute predictions using the model\n",
    "    # TODO: Use only train_idx instead of edge_index\n",
    "    # TODO: Train only on events not on concepts\n",
    "    \n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    # Compute the loss using model's loss function\n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "\n",
    "    loss.backward() # Backward pass: compute gradient of the loss\n",
    "    optimizer.step() # Perform a single optimization step, updates parameters\n",
    "    \n",
    "    return loss.item() \n",
    "\n",
    "def test(model, graph, indices, best_model, best_tvt_scores):\n",
    "    \"\"\"\n",
    "    Tests the model on given indices and updates the best model based on validation loss.\n",
    "\n",
    "    :param model: The trained graph neural network model.\n",
    "    :param graph: The heterogeneous graph data.\n",
    "    :param indices: List of indices for training, validation, and testing nodes.\n",
    "    :param best_model: The current best model based on validation loss.\n",
    "    :param best_val: The current best validation loss.\n",
    "    \n",
    "    :return: A tuple containing the list of losses for each dataset, the best model, and the best validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    tvt_scores = []\n",
    "    \n",
    "    # Evaluate the model on each set of indices\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        \n",
    "        idx = index['event']\n",
    "\n",
    "        L1 = torch.sum(torch.abs(preds['event'][idx] - graph.node_target['event'][idx])) / idx.shape[0]\n",
    "        \n",
    "        tvt_scores.append(L1)\n",
    "    \n",
    "    # Update the best model and validation loss if the current model performs better\n",
    "    if tvt_scores[1] < best_tvt_scores[1]:\n",
    "        best_tvt_scores = tvt_scores\n",
    "        # torch.to_pickle(model, 'best_model.pkl')\n",
    "        # model.to_pickle('best_model.pkl')\n",
    "        # best_model = copy.deepcopy(model)\n",
    "        torch.save(model.state_dict(), './best_model.pkl')\n",
    "    \n",
    "    return tvt_scores, best_tvt_scores, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 48,\n",
    "    'epochs': 500,\n",
    "    'weight_decay': 0.0002930387278908051,\n",
    "    'lr': 0.05091434725288385,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell creates a small heterogeneous graph, primarily for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [1, 1, 1],   # event 0\n",
    "                [2, 2, 2]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([[50], [2000]], dtype=torch.float32),\n",
    "    # \"concept\": torch.tensor([[0], [0]], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}\n",
    "\n",
    "# Testing\n",
    "hetero_graph = HeteroGraph(\n",
    "    node_feature=S_node_feature,\n",
    "    node_target=S_node_targets,\n",
    "    edge_index=S_edge_index\n",
    ")\n",
    "\n",
    "train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./1_concepts_similar_llm (1).pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "    # Convert to directed graph for compatibility with Deepsnap\n",
    "    # G = G.to_directed()\n",
    "   \n",
    "        \n",
    "hetero_graph = HeteroGraph(G, netlib=nx, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 769\n",
      "\t Feature 769\n",
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 769\n",
      "\t Feature 1\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 1\n",
      "\t Feature 769\n",
      "KEY ('event', 'similar', 'event') <class 'tuple'>\n",
      "KEY NUMS ('event', 'similar', 'event') 8487 8487\n",
      "MAX EDGES tensor(8283) tensor(8283) 8487 8487\n",
      "KEY ('event', 'related', 'concept') <class 'tuple'>\n",
      "KEY NUMS ('event', 'related', 'concept') 8487 8729\n",
      "MAX EDGES tensor(8265) tensor(8728) 8487 8729\n",
      "KEY ('concept', 'related', 'event') <class 'tuple'>\n",
      "KEY NUMS ('concept', 'related', 'event') 8729 8487\n",
      "MAX EDGES tensor(8728) tensor(8265) 8729 8487\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block ensures that all tensors in a heterogeneous graph are transferred to the same \n",
    "computational device, as specified in the 'args' dictionary.\n",
    "\"\"\"\n",
    "\n",
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "# Send node features to device\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "# Create a torch.SparseTensor from edge_index and send it to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key, type(key))\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    \n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max(), hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
    "    \n",
    "# Send node targets to device\n",
    "for key in hetero_graph.node_target:\n",
    "    hetero_graph.node_target[key] = hetero_graph.node_target[key].to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5940])\n",
      "torch.Size([1698])\n",
      "torch.Size([849])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block creates a basic split of a graph's nodes into training, validation, and testing sets. \n",
    "It uses predefined ratios to divide 'event' and 'concept' nodes in the heterogeneous graph for a simple \n",
    "dataset split, mainly for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "nEvents = hetero_graph.num_nodes(\"event\")\n",
    "nConcepts = hetero_graph.num_nodes(\"concept\")\n",
    "\n",
    "s1 = 0.7\n",
    "s2 = 0.8\n",
    "\n",
    "train_idx = {   \"event\": torch.tensor(range(0, int(nEvents * s1))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(0, int(nConcepts * s1))).to(args[\"device\"])\n",
    "            }\n",
    "val_idx = {   \"event\": torch.tensor(range(int(nEvents * s1), int(nEvents * s2))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s1), int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "test_idx = {   \"event\": torch.tensor(range(int(nEvents * s2), nEvents)).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s2), nConcepts)).to(args[\"device\"])\n",
    "            }\n",
    "\n",
    "print(train_idx[\"event\"].shape)\n",
    "print(test_idx[\"event\"].shape)\n",
    "print(val_idx[\"event\"].shape)\n",
    "\n",
    "\n",
    "# TODO: Add node labels to the nodes and try to make the deepsnap split work even for regression!\n",
    "\n",
    "# dataset = deepsnap.dataset.GraphDataset([hetero_graph], task='node')\n",
    "\n",
    "# dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
    "# datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2816.9290 Current Train,Val,Test Scores [213.21290588378906, 212.5259246826172, 225.0499725341797]\n",
      "Epoch 1 Loss 2701.3721 Current Train,Val,Test Scores [110.56449127197266, 109.18181610107422, 109.03592681884766]\n",
      "Epoch 2 Loss 2340.1035 Current Train,Val,Test Scores [150.7607879638672, 148.78665161132812, 146.4342498779297]\n",
      "Epoch 3 Loss 2179.3923 Current Train,Val,Test Scores [503.9236755371094, 501.32208251953125, 444.88165283203125]\n",
      "Epoch 4 Loss 1842.8542 Current Train,Val,Test Scores [602.8342895507812, 599.501708984375, 533.1561889648438]\n",
      "Epoch 5 Loss 1715.8608 Current Train,Val,Test Scores [473.5594177246094, 471.395751953125, 419.12530517578125]\n",
      "Epoch 6 Loss 1689.0551 Current Train,Val,Test Scores [273.16107177734375, 272.9661865234375, 241.70230102539062]\n",
      "Epoch 7 Loss 1735.6801 Current Train,Val,Test Scores [241.61172485351562, 241.63662719726562, 213.93353271484375]\n",
      "Epoch 8 Loss 1763.9558 Current Train,Val,Test Scores [233.19325256347656, 233.31640625, 206.669677734375]\n",
      "Epoch 9 Loss 1733.0569 Current Train,Val,Test Scores [198.4501190185547, 198.58445739746094, 176.53416442871094]\n",
      "Epoch 10 Loss 1651.3087 Current Train,Val,Test Scores [186.64205932617188, 187.33782958984375, 167.0420379638672]\n",
      "Epoch 11 Loss 1566.5142 Current Train,Val,Test Scores [184.59034729003906, 185.60133361816406, 165.80905151367188]\n",
      "Epoch 12 Loss 1586.9474 Current Train,Val,Test Scores [221.02024841308594, 221.78659057617188, 198.65342712402344]\n",
      "Epoch 13 Loss 1536.0865 Current Train,Val,Test Scores [264.81182861328125, 264.6291809082031, 236.7377471923828]\n",
      "Epoch 14 Loss 1490.5157 Current Train,Val,Test Scores [261.3016357421875, 260.89794921875, 232.87840270996094]\n",
      "Epoch 15 Loss 1456.7344 Current Train,Val,Test Scores [226.48159790039062, 226.12876892089844, 201.48626708984375]\n",
      "Epoch 16 Loss 1401.4708 Current Train,Val,Test Scores [192.505126953125, 192.32748413085938, 170.9529266357422]\n",
      "Epoch 17 Loss 1316.3834 Current Train,Val,Test Scores [165.3593292236328, 166.07559204101562, 146.63758850097656]\n",
      "Epoch 18 Loss 1310.6567 Current Train,Val,Test Scores [196.95945739746094, 196.67979431152344, 173.73605346679688]\n",
      "Epoch 19 Loss 1178.4462 Current Train,Val,Test Scores [212.8738555908203, 212.33319091796875, 187.47561645507812]\n",
      "Epoch 20 Loss 1117.0049 Current Train,Val,Test Scores [207.5579071044922, 206.96360778808594, 183.06051635742188]\n",
      "Epoch 21 Loss 1052.2701 Current Train,Val,Test Scores [303.94049072265625, 302.1690979003906, 267.7042236328125]\n",
      "Epoch 22 Loss 941.9591 Current Train,Val,Test Scores [367.3906555175781, 364.7967529296875, 323.52203369140625]\n",
      "Epoch 23 Loss 903.4502 Current Train,Val,Test Scores [264.60150146484375, 263.5278015136719, 233.55551147460938]\n",
      "Epoch 24 Loss 900.4758 Current Train,Val,Test Scores [311.04473876953125, 309.7854309082031, 274.25140380859375]\n",
      "Epoch 25 Loss 904.0735 Current Train,Val,Test Scores [211.7061309814453, 212.76467895507812, 187.6239471435547]\n",
      "Epoch 26 Loss 750.9852 Current Train,Val,Test Scores [214.68475341796875, 215.8896026611328, 190.239990234375]\n",
      "Epoch 27 Loss 926.9747 Current Train,Val,Test Scores [243.35182189941406, 244.5345001220703, 215.40969848632812]\n",
      "Epoch 28 Loss 782.1042 Current Train,Val,Test Scores [245.9185791015625, 247.05233764648438, 217.58282470703125]\n",
      "Epoch 29 Loss 734.9659 Current Train,Val,Test Scores [233.4788360595703, 234.56884765625, 206.6593475341797]\n",
      "Epoch 30 Loss 760.0325 Current Train,Val,Test Scores [239.48049926757812, 240.47344970703125, 211.9380645751953]\n",
      "Epoch 31 Loss 663.0957 Current Train,Val,Test Scores [250.31541442871094, 251.1512908935547, 221.31661987304688]\n",
      "Epoch 32 Loss 682.1948 Current Train,Val,Test Scores [241.18125915527344, 241.89002990722656, 213.1698455810547]\n",
      "Epoch 33 Loss 636.1859 Current Train,Val,Test Scores [226.89695739746094, 227.54002380371094, 200.59544372558594]\n",
      "Epoch 34 Loss 628.4570 Current Train,Val,Test Scores [219.67286682128906, 220.2575225830078, 194.1904754638672]\n",
      "Epoch 35 Loss 605.3500 Current Train,Val,Test Scores [208.2826690673828, 208.95675659179688, 184.16262817382812]\n",
      "Epoch 36 Loss 592.5750 Current Train,Val,Test Scores [189.29115295410156, 190.1285400390625, 167.47882080078125]\n",
      "Epoch 37 Loss 582.3562 Current Train,Val,Test Scores [173.64102172851562, 174.50765991210938, 153.7063751220703]\n",
      "Epoch 38 Loss 573.1711 Current Train,Val,Test Scores [163.15878295898438, 163.9881591796875, 144.4532470703125]\n",
      "Epoch 39 Loss 587.7610 Current Train,Val,Test Scores [149.56399536132812, 150.49261474609375, 132.43138122558594]\n",
      "Epoch 40 Loss 530.2452 Current Train,Val,Test Scores [178.47848510742188, 178.912109375, 157.80458068847656]\n",
      "Epoch 41 Loss 523.8458 Current Train,Val,Test Scores [239.3955535888672, 239.4749755859375, 211.4385528564453]\n",
      "Epoch 42 Loss 507.6628 Current Train,Val,Test Scores [304.9281005859375, 305.19146728515625, 269.123046875]\n",
      "Epoch 43 Loss 510.3467 Current Train,Val,Test Scores [350.868408203125, 351.31658935546875, 309.55963134765625]\n",
      "Epoch 44 Loss 474.6384 Current Train,Val,Test Scores [366.159423828125, 366.7242736816406, 323.00140380859375]\n",
      "Epoch 45 Loss 472.8961 Current Train,Val,Test Scores [347.545654296875, 348.35247802734375, 306.5901184082031]\n",
      "Epoch 46 Loss 455.5127 Current Train,Val,Test Scores [327.26202392578125, 327.94244384765625, 288.7093811035156]\n",
      "Epoch 47 Loss 444.7293 Current Train,Val,Test Scores [304.7720031738281, 305.3858642578125, 268.94781494140625]\n",
      "Epoch 48 Loss 414.1877 Current Train,Val,Test Scores [273.7693786621094, 274.4773254394531, 241.76795959472656]\n",
      "Epoch 49 Loss 422.8055 Current Train,Val,Test Scores [248.72340393066406, 249.28970336914062, 219.80148315429688]\n",
      "Epoch 50 Loss 393.5930 Current Train,Val,Test Scores [222.54550170898438, 223.10594177246094, 196.77748107910156]\n",
      "Epoch 51 Loss 402.2521 Current Train,Val,Test Scores [198.40789794921875, 199.09765625, 175.52716064453125]\n",
      "Epoch 52 Loss 379.9034 Current Train,Val,Test Scores [172.35020446777344, 172.9991912841797, 152.59017944335938]\n",
      "Epoch 53 Loss 361.4485 Current Train,Val,Test Scores [150.509521484375, 151.25025939941406, 133.47337341308594]\n",
      "Epoch 54 Loss 365.1390 Current Train,Val,Test Scores [139.5320587158203, 140.2655792236328, 123.84451293945312]\n",
      "Epoch 55 Loss 345.0285 Current Train,Val,Test Scores [127.32756042480469, 128.03826904296875, 113.11970520019531]\n",
      "Epoch 56 Loss 317.2874 Current Train,Val,Test Scores [121.31830596923828, 121.90702056884766, 107.7100601196289]\n",
      "Epoch 57 Loss 308.3605 Current Train,Val,Test Scores [115.69606018066406, 116.17721557617188, 102.78592681884766]\n",
      "Epoch 58 Loss 291.1709 Current Train,Val,Test Scores [105.73365783691406, 106.15743255615234, 94.00360870361328]\n",
      "Epoch 59 Loss 321.4314 Current Train,Val,Test Scores [164.8357696533203, 163.76722717285156, 145.2082977294922]\n",
      "Epoch 60 Loss 337.5885 Current Train,Val,Test Scores [114.42335510253906, 114.98094177246094, 101.64017486572266]\n",
      "Epoch 61 Loss 337.2133 Current Train,Val,Test Scores [122.091552734375, 122.48944091796875, 108.44740295410156]\n",
      "Epoch 62 Loss 241.0170 Current Train,Val,Test Scores [124.33060455322266, 124.66683959960938, 110.4344482421875]\n",
      "Epoch 63 Loss 250.1504 Current Train,Val,Test Scores [119.74681854248047, 120.23291015625, 106.38427734375]\n",
      "Epoch 64 Loss 279.2129 Current Train,Val,Test Scores [131.7953338623047, 132.18666076660156, 117.06040954589844]\n",
      "Epoch 65 Loss 231.1340 Current Train,Val,Test Scores [404.1944274902344, 399.7745056152344, 355.32952880859375]\n",
      "Epoch 66 Loss 190.2068 Current Train,Val,Test Scores [365.4479675292969, 360.8256530761719, 321.9781799316406]\n",
      "Epoch 67 Loss 234.4501 Current Train,Val,Test Scores [200.95310974121094, 197.86424255371094, 178.02569580078125]\n",
      "Epoch 68 Loss 303.3311 Current Train,Val,Test Scores [111.45035552978516, 111.09483337402344, 99.14911651611328]\n",
      "Epoch 69 Loss 209.4632 Current Train,Val,Test Scores [78.52662658691406, 78.93577575683594, 70.12519073486328]\n",
      "Epoch 70 Loss 506.5248 Current Train,Val,Test Scores [72.46002197265625, 72.8235855102539, 64.70793151855469]\n",
      "Epoch 71 Loss 575.5869 Current Train,Val,Test Scores [50.749210357666016, 50.98993682861328, 45.53472900390625]\n",
      "Epoch 72 Loss 484.4129 Current Train,Val,Test Scores [32.10670471191406, 32.36081314086914, 29.034624099731445]\n",
      "Epoch 73 Loss 356.5992 Current Train,Val,Test Scores [33.99734115600586, 34.14751052856445, 30.632753372192383]\n",
      "Epoch 74 Loss 523.6421 Current Train,Val,Test Scores [23.692502975463867, 23.986345291137695, 21.51210594177246]\n",
      "Epoch 75 Loss 272.6129 Current Train,Val,Test Scores [25.326309204101562, 25.557193756103516, 22.9398193359375]\n",
      "Epoch 76 Loss 384.7624 Current Train,Val,Test Scores [22.048967361450195, 22.31961441040039, 20.11455726623535]\n",
      "Epoch 77 Loss 220.0699 Current Train,Val,Test Scores [21.951427459716797, 22.17482566833496, 20.102235794067383]\n",
      "Epoch 78 Loss 319.4835 Current Train,Val,Test Scores [15.976519584655762, 16.21165657043457, 14.865906715393066]\n",
      "Epoch 79 Loss 187.5520 Current Train,Val,Test Scores [10.414911270141602, 10.638872146606445, 9.967424392700195]\n",
      "Epoch 80 Loss 268.6034 Current Train,Val,Test Scores [14.2526216506958, 14.581148147583008, 13.277275085449219]\n",
      "Epoch 81 Loss 189.3107 Current Train,Val,Test Scores [335.5062255859375, 331.0495300292969, 294.595703125]\n",
      "Epoch 82 Loss 294.7022 Current Train,Val,Test Scores [385.2135314941406, 379.92718505859375, 338.1966857910156]\n",
      "Epoch 83 Loss 191.3838 Current Train,Val,Test Scores [564.3826904296875, 556.5269775390625, 495.3353271484375]\n",
      "Epoch 84 Loss 222.2518 Current Train,Val,Test Scores [890.4727172851562, 877.6549072265625, 781.3901977539062]\n",
      "Epoch 85 Loss 154.2274 Current Train,Val,Test Scores [1066.959716796875, 1051.7554931640625, 936.1774291992188]\n",
      "Epoch 86 Loss 209.3331 Current Train,Val,Test Scores [949.789794921875, 936.37060546875, 833.3692016601562]\n",
      "Epoch 87 Loss 162.2779 Current Train,Val,Test Scores [794.8522338867188, 781.8502807617188, 698.1361694335938]\n",
      "Epoch 88 Loss 189.3383 Current Train,Val,Test Scores [499.53179931640625, 490.4320983886719, 439.17431640625]\n",
      "Epoch 89 Loss 142.7492 Current Train,Val,Test Scores [105.51480865478516, 103.59770202636719, 93.27023315429688]\n",
      "Epoch 90 Loss 177.5511 Current Train,Val,Test Scores [63.339542388916016, 62.83613967895508, 56.60773468017578]\n",
      "Epoch 91 Loss 131.9766 Current Train,Val,Test Scores [65.89506530761719, 64.84947204589844, 58.76554870605469]\n",
      "Epoch 92 Loss 147.7046 Current Train,Val,Test Scores [65.69914245605469, 64.7193603515625, 58.60563278198242]\n",
      "Epoch 93 Loss 119.3139 Current Train,Val,Test Scores [61.91843032836914, 61.17985153198242, 55.32947540283203]\n",
      "Epoch 94 Loss 130.4819 Current Train,Val,Test Scores [55.669822692871094, 54.92221450805664, 49.82982635498047]\n",
      "Epoch 95 Loss 123.3580 Current Train,Val,Test Scores [54.833255767822266, 53.65273666381836, 48.97196960449219]\n",
      "Epoch 96 Loss 112.1519 Current Train,Val,Test Scores [53.45037841796875, 52.21757507324219, 47.63031768798828]\n",
      "Epoch 97 Loss 114.8133 Current Train,Val,Test Scores [44.19953536987305, 42.98112106323242, 39.54695129394531]\n",
      "Epoch 98 Loss 92.1845 Current Train,Val,Test Scores [29.796775817871094, 29.090147018432617, 27.087474822998047]\n",
      "Epoch 99 Loss 106.9814 Current Train,Val,Test Scores [20.055875778198242, 19.919227600097656, 18.573593139648438]\n",
      "Epoch 100 Loss 86.7525 Current Train,Val,Test Scores [13.366386413574219, 13.472097396850586, 12.666768074035645]\n",
      "Epoch 101 Loss 101.7471 Current Train,Val,Test Scores [7.918087959289551, 8.044059753417969, 7.916666030883789]\n",
      "Epoch 102 Loss 90.5048 Current Train,Val,Test Scores [8.43171215057373, 8.551621437072754, 8.513849258422852]\n",
      "Epoch 103 Loss 82.2758 Current Train,Val,Test Scores [7.520944595336914, 7.743785381317139, 7.727169036865234]\n",
      "Epoch 104 Loss 88.6772 Current Train,Val,Test Scores [5.307712554931641, 5.7324090003967285, 5.733473300933838]\n",
      "Epoch 105 Loss 73.6891 Current Train,Val,Test Scores [3.752856969833374, 4.223310470581055, 4.365743160247803]\n",
      "Epoch 106 Loss 77.3250 Current Train,Val,Test Scores [4.151780128479004, 4.618844509124756, 4.711655616760254]\n",
      "Epoch 107 Loss 81.8029 Current Train,Val,Test Scores [4.601184368133545, 5.038885593414307, 5.093626976013184]\n",
      "Epoch 108 Loss 68.0446 Current Train,Val,Test Scores [4.2936906814575195, 4.735148906707764, 4.823589324951172]\n",
      "Epoch 109 Loss 70.4539 Current Train,Val,Test Scores [3.237344264984131, 3.7040460109710693, 3.896071672439575]\n",
      "Epoch 110 Loss 72.7012 Current Train,Val,Test Scores [1.8586400747299194, 2.349137306213379, 2.6944174766540527]\n",
      "Epoch 111 Loss 62.1526 Current Train,Val,Test Scores [1.818671703338623, 2.401076555252075, 2.632563829421997]\n",
      "Epoch 112 Loss 64.9284 Current Train,Val,Test Scores [2.372955322265625, 2.856609582901001, 3.1389222145080566]\n",
      "Epoch 113 Loss 67.4342 Current Train,Val,Test Scores [2.5373592376708984, 3.026000499725342, 3.2682266235351562]\n",
      "Epoch 114 Loss 60.4584 Current Train,Val,Test Scores [2.231637477874756, 2.7217154502868652, 2.991795778274536]\n",
      "Epoch 115 Loss 58.1129 Current Train,Val,Test Scores [1.9236427545547485, 2.41733455657959, 2.703692674636841]\n",
      "Epoch 116 Loss 61.5068 Current Train,Val,Test Scores [1.9668056964874268, 2.5245628356933594, 2.7454676628112793]\n",
      "Epoch 117 Loss 59.6505 Current Train,Val,Test Scores [1.9586460590362549, 2.498497247695923, 2.728800058364868]\n",
      "Epoch 118 Loss 53.4863 Current Train,Val,Test Scores [1.9459385871887207, 2.4826719760894775, 2.7159135341644287]\n",
      "Epoch 119 Loss 53.4830 Current Train,Val,Test Scores [2.62770676612854, 3.1805458068847656, 3.322662830352783]\n",
      "Epoch 120 Loss 55.3448 Current Train,Val,Test Scores [2.837944507598877, 3.379452705383301, 3.5027544498443604]\n",
      "Epoch 121 Loss 52.8291 Current Train,Val,Test Scores [3.4935336112976074, 4.028294563293457, 4.0791335105896]\n",
      "Epoch 122 Loss 49.6090 Current Train,Val,Test Scores [3.139897584915161, 3.6758506298065186, 3.765789747238159]\n",
      "Epoch 123 Loss 50.3201 Current Train,Val,Test Scores [1.9199151992797852, 2.4389235973358154, 2.686030149459839]\n",
      "Epoch 124 Loss 51.3880 Current Train,Val,Test Scores [2.1133551597595215, 2.6464574337005615, 2.870798349380493]\n",
      "Epoch 125 Loss 49.8991 Current Train,Val,Test Scores [1.9564604759216309, 2.4634346961975098, 2.7313051223754883]\n",
      "Epoch 126 Loss 47.4264 Current Train,Val,Test Scores [2.1200335025787354, 2.646711587905884, 2.8777005672454834]\n",
      "Epoch 127 Loss 46.5676 Current Train,Val,Test Scores [2.0784459114074707, 2.60284686088562, 2.845018148422241]\n",
      "Epoch 128 Loss 47.2528 Current Train,Val,Test Scores [2.069167137145996, 2.559136390686035, 2.8353934288024902]\n",
      "Epoch 129 Loss 46.9193 Current Train,Val,Test Scores [2.55385684967041, 3.1065256595611572, 3.267817974090576]\n",
      "Epoch 130 Loss 45.2836 Current Train,Val,Test Scores [2.5544180870056152, 3.098494291305542, 3.2656848430633545]\n",
      "Epoch 131 Loss 43.2780 Current Train,Val,Test Scores [2.2066023349761963, 2.694925546646118, 2.9475739002227783]\n",
      "Epoch 132 Loss 42.4944 Current Train,Val,Test Scores [2.4823577404022217, 3.0458014011383057, 3.180654525756836]\n",
      "Epoch 133 Loss 42.1027 Current Train,Val,Test Scores [4.747622489929199, 5.2683892250061035, 5.168829441070557]\n",
      "Epoch 134 Loss 43.3366 Current Train,Val,Test Scores [5.5522332191467285, 6.057259559631348, 5.875610828399658]\n",
      "Epoch 135 Loss 48.2567 Current Train,Val,Test Scores [8.051091194152832, 8.544753074645996, 8.084843635559082]\n",
      "Epoch 136 Loss 60.8983 Current Train,Val,Test Scores [3.4836971759796143, 3.9302613735198975, 4.059295654296875]\n",
      "Epoch 137 Loss 107.5030 Current Train,Val,Test Scores [4.212190628051758, 4.715369701385498, 4.724900722503662]\n",
      "Epoch 138 Loss 144.1351 Current Train,Val,Test Scores [18.510860443115234, 18.856449127197266, 17.21609115600586]\n",
      "Epoch 139 Loss 209.1568 Current Train,Val,Test Scores [6.81350564956665, 7.309410572052002, 6.972644805908203]\n",
      "Epoch 140 Loss 75.2605 Current Train,Val,Test Scores [9.128405570983887, 9.58864688873291, 9.00416088104248]\n",
      "Epoch 141 Loss 115.4039 Current Train,Val,Test Scores [32.6224479675293, 32.7568473815918, 29.575448989868164]\n",
      "Epoch 142 Loss 153.7429 Current Train,Val,Test Scores [4.775745391845703, 5.204572677612305, 5.174139976501465]\n",
      "Epoch 143 Loss 81.0395 Current Train,Val,Test Scores [483.144775390625, 475.69134521484375, 423.9093322753906]\n",
      "Epoch 144 Loss 186.8120 Current Train,Val,Test Scores [701.559326171875, 691.230712890625, 615.189697265625]\n",
      "Epoch 145 Loss 150.2801 Current Train,Val,Test Scores [787.723876953125, 776.1557006835938, 690.7334594726562]\n",
      "Epoch 146 Loss 99.7093 Current Train,Val,Test Scores [498.1734924316406, 491.129150390625, 436.2488098144531]\n",
      "Epoch 147 Loss 165.6954 Current Train,Val,Test Scores [33.83342742919922, 33.320980072021484, 29.143600463867188]\n",
      "Epoch 148 Loss 49.5763 Current Train,Val,Test Scores [5.969131946563721, 6.474897384643555, 6.235314846038818]\n",
      "Epoch 149 Loss 120.0939 Current Train,Val,Test Scores [13.139026641845703, 13.543084144592285, 12.532508850097656]\n",
      "Epoch 150 Loss 64.1488 Current Train,Val,Test Scores [15.591889381408691, 15.95819091796875, 14.691202163696289]\n",
      "Epoch 151 Loss 88.0038 Current Train,Val,Test Scores [18.296077728271484, 18.622026443481445, 17.05032730102539]\n",
      "Epoch 152 Loss 76.0032 Current Train,Val,Test Scores [13.872169494628906, 14.265623092651367, 13.16220760345459]\n",
      "Epoch 153 Loss 85.4441 Current Train,Val,Test Scores [11.158480644226074, 11.615822792053223, 10.76677131652832]\n",
      "Epoch 154 Loss 84.2983 Current Train,Val,Test Scores [7.601526737213135, 8.128097534179688, 7.664920806884766]\n",
      "Epoch 155 Loss 65.0735 Current Train,Val,Test Scores [10.028032302856445, 10.494175910949707, 9.8052396774292]\n",
      "Epoch 156 Loss 62.0700 Current Train,Val,Test Scores [149.45504760742188, 147.94590759277344, 130.2213897705078]\n",
      "Epoch 157 Loss 86.1074 Current Train,Val,Test Scores [242.44398498535156, 239.46429443359375, 211.90419006347656]\n",
      "Epoch 158 Loss 58.0578 Current Train,Val,Test Scores [219.48760986328125, 216.59324645996094, 191.94137573242188]\n",
      "Epoch 159 Loss 67.0096 Current Train,Val,Test Scores [335.874267578125, 331.1467590332031, 294.1146240234375]\n",
      "Epoch 160 Loss 64.0739 Current Train,Val,Test Scores [471.6220397949219, 464.9119873046875, 413.1808166503906]\n",
      "Epoch 161 Loss 62.3622 Current Train,Val,Test Scores [307.85113525390625, 303.57440185546875, 269.3988952636719]\n",
      "Epoch 162 Loss 43.3035 Current Train,Val,Test Scores [109.57231140136719, 108.1624755859375, 95.41552734375]\n",
      "Epoch 163 Loss 61.0506 Current Train,Val,Test Scores [66.03738403320312, 65.10426330566406, 57.3333854675293]\n",
      "Epoch 164 Loss 43.0187 Current Train,Val,Test Scores [19.36421775817871, 18.896713256835938, 16.9227352142334]\n",
      "Epoch 165 Loss 57.8807 Current Train,Val,Test Scores [4.411313533782959, 5.025218486785889, 4.9364213943481445]\n",
      "Epoch 166 Loss 44.3500 Current Train,Val,Test Scores [5.360995769500732, 5.957352638244629, 5.786762237548828]\n",
      "Epoch 167 Loss 47.7680 Current Train,Val,Test Scores [5.803838729858398, 6.387331962585449, 6.161864757537842]\n",
      "Epoch 168 Loss 43.6980 Current Train,Val,Test Scores [6.359790325164795, 6.9385833740234375, 6.652464866638184]\n",
      "Epoch 169 Loss 45.5381 Current Train,Val,Test Scores [6.941116809844971, 7.528336524963379, 7.187262535095215]\n",
      "Epoch 170 Loss 42.1388 Current Train,Val,Test Scores [6.5496602058410645, 7.155472755432129, 6.8499932289123535]\n",
      "Epoch 171 Loss 46.4863 Current Train,Val,Test Scores [4.696502685546875, 5.308971405029297, 5.211421489715576]\n",
      "Epoch 172 Loss 37.6288 Current Train,Val,Test Scores [10.081422805786133, 9.676712036132812, 9.269577980041504]\n",
      "Epoch 173 Loss 41.2054 Current Train,Val,Test Scores [6.456404209136963, 6.550968647003174, 6.38995885848999]\n",
      "Epoch 174 Loss 37.5453 Current Train,Val,Test Scores [3.7108101844787598, 4.3336005210876465, 4.337864875793457]\n",
      "Epoch 175 Loss 38.2292 Current Train,Val,Test Scores [3.380105495452881, 4.004607677459717, 4.034091949462891]\n",
      "Epoch 176 Loss 39.4861 Current Train,Val,Test Scores [3.700031042098999, 4.317358493804932, 4.288476467132568]\n",
      "Epoch 177 Loss 35.7800 Current Train,Val,Test Scores [6.1617631912231445, 6.745898246765137, 6.441915512084961]\n",
      "Epoch 178 Loss 37.1395 Current Train,Val,Test Scores [7.814446926116943, 8.377248764038086, 7.904432773590088]\n",
      "Epoch 179 Loss 33.7994 Current Train,Val,Test Scores [6.967552661895752, 7.543089866638184, 7.17352819442749]\n",
      "Epoch 180 Loss 36.1657 Current Train,Val,Test Scores [6.04984712600708, 6.633692741394043, 6.370420455932617]\n",
      "Epoch 181 Loss 34.8887 Current Train,Val,Test Scores [6.030950546264648, 6.6067609786987305, 6.343109607696533]\n",
      "Epoch 182 Loss 34.5164 Current Train,Val,Test Scores [5.544445514678955, 6.118706226348877, 5.904730796813965]\n",
      "Epoch 183 Loss 34.4076 Current Train,Val,Test Scores [4.65458345413208, 5.241367816925049, 5.129299640655518]\n",
      "Epoch 184 Loss 32.7970 Current Train,Val,Test Scores [4.338393688201904, 4.933676719665527, 4.863194942474365]\n",
      "Epoch 185 Loss 32.8528 Current Train,Val,Test Scores [4.2046051025390625, 4.798274517059326, 4.740259647369385]\n",
      "Epoch 186 Loss 32.8498 Current Train,Val,Test Scores [3.662541389465332, 4.2649006843566895, 4.240917682647705]\n",
      "Epoch 187 Loss 32.3524 Current Train,Val,Test Scores [3.7192680835723877, 4.321702480316162, 4.283884048461914]\n",
      "Epoch 188 Loss 32.6282 Current Train,Val,Test Scores [4.305552005767822, 4.894285202026367, 4.8132710456848145]\n",
      "Epoch 189 Loss 31.1153 Current Train,Val,Test Scores [4.77054500579834, 5.350097179412842, 5.219669818878174]\n",
      "Epoch 190 Loss 31.6177 Current Train,Val,Test Scores [5.107626438140869, 5.681136131286621, 5.497170448303223]\n",
      "Epoch 191 Loss 31.0509 Current Train,Val,Test Scores [5.182867527008057, 5.742644309997559, 5.559698104858398]\n",
      "Epoch 192 Loss 30.9904 Current Train,Val,Test Scores [5.250652313232422, 5.801175594329834, 5.625578880310059]\n",
      "Epoch 193 Loss 30.9355 Current Train,Val,Test Scores [5.930602550506592, 6.462899684906006, 6.226757526397705]\n",
      "Epoch 194 Loss 30.0156 Current Train,Val,Test Scores [6.682303428649902, 7.198770046234131, 6.893906593322754]\n",
      "Epoch 195 Loss 30.4497 Current Train,Val,Test Scores [6.284707546234131, 6.808962821960449, 6.557516098022461]\n",
      "Epoch 196 Loss 29.7387 Current Train,Val,Test Scores [5.6180644035339355, 6.154245376586914, 5.985782623291016]\n",
      "Epoch 197 Loss 29.9700 Current Train,Val,Test Scores [5.70947265625, 6.245003700256348, 6.073810577392578]\n",
      "Epoch 198 Loss 29.5379 Current Train,Val,Test Scores [6.029509544372559, 6.562323093414307, 6.363558769226074]\n",
      "Epoch 199 Loss 29.3140 Current Train,Val,Test Scores [5.989137172698975, 6.521699905395508, 6.337340831756592]\n",
      "Epoch 200 Loss 29.1155 Current Train,Val,Test Scores [6.077848434448242, 6.610442638397217, 6.422088623046875]\n",
      "Epoch 201 Loss 28.9371 Current Train,Val,Test Scores [6.372933387756348, 6.905045986175537, 6.689429759979248]\n",
      "Epoch 202 Loss 28.9094 Current Train,Val,Test Scores [6.293268203735352, 6.827086448669434, 6.629757881164551]\n",
      "Epoch 203 Loss 28.6258 Current Train,Val,Test Scores [6.110198020935059, 6.647464752197266, 6.476941108703613]\n",
      "Epoch 204 Loss 28.4426 Current Train,Val,Test Scores [6.035892486572266, 6.576369762420654, 6.419510364532471]\n",
      "Epoch 205 Loss 28.1550 Current Train,Val,Test Scores [6.156557559967041, 6.695994853973389, 6.5345869064331055]\n",
      "Epoch 206 Loss 28.1512 Current Train,Val,Test Scores [6.653102397918701, 7.186896324157715, 6.977697849273682]\n",
      "Epoch 207 Loss 27.8477 Current Train,Val,Test Scores [7.112706184387207, 7.641073703765869, 7.3896589279174805]\n",
      "Epoch 208 Loss 27.7521 Current Train,Val,Test Scores [7.19417667388916, 7.719948768615723, 7.473106384277344]\n",
      "Epoch 209 Loss 27.5671 Current Train,Val,Test Scores [7.905782699584961, 8.423255920410156, 8.106925010681152]\n",
      "Epoch 210 Loss 27.3038 Current Train,Val,Test Scores [7.417567253112793, 7.939121723175049, 7.692521572113037]\n",
      "Epoch 211 Loss 27.2124 Current Train,Val,Test Scores [6.85219669342041, 7.381291389465332, 7.21476936340332]\n",
      "Epoch 212 Loss 27.0128 Current Train,Val,Test Scores [6.290166854858398, 6.830946922302246, 6.740015029907227]\n",
      "Epoch 213 Loss 26.7769 Current Train,Val,Test Scores [4.192831039428711, 4.759436130523682, 4.937394142150879]\n",
      "Epoch 214 Loss 26.5545 Current Train,Val,Test Scores [3.736360788345337, 4.273195743560791, 4.601193904876709]\n",
      "Epoch 215 Loss 26.4760 Current Train,Val,Test Scores [4.088657379150391, 4.661866188049316, 4.896667003631592]\n",
      "Epoch 216 Loss 26.1931 Current Train,Val,Test Scores [5.467286586761475, 6.0191569328308105, 6.069365978240967]\n",
      "Epoch 217 Loss 25.9620 Current Train,Val,Test Scores [6.37412691116333, 6.916748523712158, 6.869753837585449]\n",
      "Epoch 218 Loss 25.7542 Current Train,Val,Test Scores [5.914700031280518, 6.461001396179199, 6.476370811462402]\n",
      "Epoch 219 Loss 25.6579 Current Train,Val,Test Scores [6.347530841827393, 6.893959999084473, 6.869480133056641]\n",
      "Epoch 220 Loss 25.5510 Current Train,Val,Test Scores [4.249590873718262, 4.81592321395874, 5.046180248260498]\n",
      "Epoch 221 Loss 25.4453 Current Train,Val,Test Scores [4.8688178062438965, 5.422580718994141, 5.625699520111084]\n",
      "Epoch 222 Loss 25.3692 Current Train,Val,Test Scores [3.847053050994873, 4.361745834350586, 4.675560474395752]\n",
      "Epoch 223 Loss 25.3787 Current Train,Val,Test Scores [5.292912483215332, 5.846874237060547, 5.988182544708252]\n",
      "Epoch 224 Loss 25.1843 Current Train,Val,Test Scores [4.866225719451904, 5.413015842437744, 5.6172661781311035]\n",
      "Epoch 225 Loss 25.0080 Current Train,Val,Test Scores [6.861589431762695, 7.420718669891357, 7.357309341430664]\n",
      "Epoch 226 Loss 24.6087 Current Train,Val,Test Scores [7.070907115936279, 7.607337474822998, 7.545355319976807]\n",
      "Epoch 227 Loss 24.2078 Current Train,Val,Test Scores [7.916048049926758, 8.432026863098145, 8.290292739868164]\n",
      "Epoch 228 Loss 23.9005 Current Train,Val,Test Scores [8.99617862701416, 9.470283508300781, 9.26836109161377]\n",
      "Epoch 229 Loss 23.7174 Current Train,Val,Test Scores [9.024971008300781, 9.459257125854492, 9.30480670928955]\n",
      "Epoch 230 Loss 23.5969 Current Train,Val,Test Scores [10.661608695983887, 11.091742515563965, 10.744093894958496]\n",
      "Epoch 231 Loss 23.5227 Current Train,Val,Test Scores [9.609365463256836, 10.127137184143066, 9.730887413024902]\n",
      "Epoch 232 Loss 23.5512 Current Train,Val,Test Scores [12.931343078613281, 13.443625450134277, 12.664307594299316]\n",
      "Epoch 233 Loss 23.7446 Current Train,Val,Test Scores [10.235265731811523, 10.695230484008789, 10.211756706237793]\n",
      "Epoch 234 Loss 24.3886 Current Train,Val,Test Scores [14.604402542114258, 15.121761322021484, 14.096632957458496]\n",
      "Epoch 235 Loss 24.8666 Current Train,Val,Test Scores [4.637944221496582, 5.41079044342041, 5.203692436218262]\n",
      "Epoch 236 Loss 24.3929 Current Train,Val,Test Scores [4.873513698577881, 5.591154098510742, 5.716841220855713]\n",
      "Epoch 237 Loss 23.0000 Current Train,Val,Test Scores [35.04692459106445, 33.12196731567383, 32.52482223510742]\n",
      "Epoch 238 Loss 22.4289 Current Train,Val,Test Scores [34.86940002441406, 32.98533248901367, 32.379154205322266]\n",
      "Epoch 239 Loss 22.5739 Current Train,Val,Test Scores [37.77285385131836, 35.80084228515625, 34.938140869140625]\n",
      "Epoch 240 Loss 22.4689 Current Train,Val,Test Scores [50.10544967651367, 48.021728515625, 45.740394592285156]\n",
      "Epoch 241 Loss 22.2130 Current Train,Val,Test Scores [21.37584114074707, 19.834306716918945, 20.410093307495117]\n",
      "Epoch 242 Loss 21.9942 Current Train,Val,Test Scores [49.622894287109375, 47.68929672241211, 45.1717529296875]\n",
      "Epoch 243 Loss 21.6837 Current Train,Val,Test Scores [42.30399703979492, 40.449039459228516, 38.832618713378906]\n",
      "Epoch 244 Loss 20.6477 Current Train,Val,Test Scores [56.604881286621094, 54.593467712402344, 51.397621154785156]\n",
      "Epoch 245 Loss 20.1386 Current Train,Val,Test Scores [70.8727035522461, 69.5459213256836, 62.948341369628906]\n",
      "Epoch 246 Loss 20.6299 Current Train,Val,Test Scores [72.45796203613281, 70.72166442871094, 64.40182495117188]\n",
      "Epoch 247 Loss 20.9453 Current Train,Val,Test Scores [88.78761291503906, 87.0949478149414, 78.41802978515625]\n",
      "Epoch 248 Loss 20.5767 Current Train,Val,Test Scores [91.47453308105469, 89.58892059326172, 80.76396942138672]\n",
      "Epoch 249 Loss 20.3593 Current Train,Val,Test Scores [56.92189025878906, 55.159969329833984, 50.56008529663086]\n",
      "Epoch 250 Loss 19.5012 Current Train,Val,Test Scores [29.889324188232422, 28.499143600463867, 26.936647415161133]\n",
      "Epoch 251 Loss 19.7527 Current Train,Val,Test Scores [41.575923919677734, 40.14936065673828, 37.045196533203125]\n",
      "Epoch 252 Loss 20.6635 Current Train,Val,Test Scores [23.85139274597168, 22.746030807495117, 21.619592666625977]\n",
      "Epoch 253 Loss 19.6315 Current Train,Val,Test Scores [4.578186511993408, 5.059800624847412, 5.350212097167969]\n",
      "Epoch 254 Loss 20.1395 Current Train,Val,Test Scores [22.891475677490234, 21.787946701049805, 20.67849349975586]\n",
      "Epoch 255 Loss 19.9789 Current Train,Val,Test Scores [28.54341697692871, 27.362430572509766, 25.363447189331055]\n",
      "Epoch 256 Loss 19.7632 Current Train,Val,Test Scores [13.825258255004883, 13.45876407623291, 12.945230484008789]\n",
      "Epoch 257 Loss 20.4011 Current Train,Val,Test Scores [21.13214874267578, 20.113235473632812, 19.26393699645996]\n",
      "Epoch 258 Loss 19.7667 Current Train,Val,Test Scores [54.859046936035156, 53.58853530883789, 48.340126037597656]\n",
      "Epoch 259 Loss 20.2218 Current Train,Val,Test Scores [42.314998626708984, 40.965789794921875, 37.49810791015625]\n",
      "Epoch 260 Loss 19.5070 Current Train,Val,Test Scores [68.60651397705078, 66.99667358398438, 60.42645263671875]\n",
      "Epoch 261 Loss 18.4514 Current Train,Val,Test Scores [56.389686584472656, 54.821441650390625, 49.75471496582031]\n",
      "Epoch 262 Loss 18.7597 Current Train,Val,Test Scores [51.4789924621582, 50.00613021850586, 45.45719909667969]\n",
      "Epoch 263 Loss 18.0282 Current Train,Val,Test Scores [72.11387634277344, 70.42742919921875, 63.55143737792969]\n",
      "Epoch 264 Loss 17.0462 Current Train,Val,Test Scores [86.84928894042969, 85.27849578857422, 76.50238037109375]\n",
      "Epoch 265 Loss 16.8764 Current Train,Val,Test Scores [76.80667877197266, 75.26577758789062, 67.65814971923828]\n",
      "Epoch 266 Loss 17.2884 Current Train,Val,Test Scores [53.420684814453125, 52.17122268676758, 47.12110900878906]\n",
      "Epoch 267 Loss 17.3449 Current Train,Val,Test Scores [23.11319351196289, 22.204143524169922, 20.88838768005371]\n",
      "Epoch 268 Loss 17.0794 Current Train,Val,Test Scores [8.371352195739746, 8.53509521484375, 8.436495780944824]\n",
      "Epoch 269 Loss 18.3899 Current Train,Val,Test Scores [17.527721405029297, 17.008804321289062, 16.072046279907227]\n",
      "Epoch 270 Loss 20.8902 Current Train,Val,Test Scores [5.402549743652344, 5.928262233734131, 6.050477981567383]\n",
      "Epoch 271 Loss 20.4725 Current Train,Val,Test Scores [3.9774909019470215, 4.50476598739624, 4.803712368011475]\n",
      "Epoch 272 Loss 31.3711 Current Train,Val,Test Scores [27.359771728515625, 26.44804573059082, 24.5872802734375]\n",
      "Epoch 273 Loss 38.9970 Current Train,Val,Test Scores [113.16903686523438, 111.68065643310547, 99.18980407714844]\n",
      "Epoch 274 Loss 42.1954 Current Train,Val,Test Scores [32.97242736816406, 32.180015563964844, 29.392148971557617]\n",
      "Epoch 275 Loss 45.5553 Current Train,Val,Test Scores [56.714454650878906, 55.72207260131836, 49.883365631103516]\n",
      "Epoch 276 Loss 41.2759 Current Train,Val,Test Scores [42.30197525024414, 41.1876335144043, 37.49789810180664]\n",
      "Epoch 277 Loss 37.6166 Current Train,Val,Test Scores [104.18968200683594, 102.73954010009766, 91.50108337402344]\n",
      "Epoch 278 Loss 35.2973 Current Train,Val,Test Scores [82.20938110351562, 80.30236053466797, 73.01070404052734]\n",
      "Epoch 279 Loss 34.6627 Current Train,Val,Test Scores [35.816192626953125, 36.05978775024414, 32.45942687988281]\n",
      "Epoch 280 Loss 34.2743 Current Train,Val,Test Scores [53.15800857543945, 53.312843322753906, 47.76262664794922]\n",
      "Epoch 281 Loss 33.6267 Current Train,Val,Test Scores [58.55413818359375, 58.651275634765625, 52.56721496582031]\n",
      "Epoch 282 Loss 35.9133 Current Train,Val,Test Scores [64.45905303955078, 64.67117309570312, 57.774818420410156]\n",
      "Epoch 283 Loss 34.4263 Current Train,Val,Test Scores [63.57847595214844, 63.861385345458984, 57.00786209106445]\n",
      "Epoch 284 Loss 33.2600 Current Train,Val,Test Scores [134.79330444335938, 132.79173278808594, 119.07157135009766]\n",
      "Epoch 285 Loss 30.7729 Current Train,Val,Test Scores [162.46697998046875, 160.61109924316406, 142.6781463623047]\n",
      "Epoch 286 Loss 29.3374 Current Train,Val,Test Scores [120.51148223876953, 119.1791763305664, 105.9020004272461]\n",
      "Epoch 287 Loss 29.3689 Current Train,Val,Test Scores [77.0429916381836, 76.32997131347656, 68.04582977294922]\n",
      "Epoch 288 Loss 29.7625 Current Train,Val,Test Scores [60.02600860595703, 60.32512283325195, 53.901275634765625]\n",
      "Epoch 289 Loss 30.3022 Current Train,Val,Test Scores [59.56336212158203, 59.83140182495117, 53.501914978027344]\n",
      "Epoch 290 Loss 29.3078 Current Train,Val,Test Scores [54.4494514465332, 54.71257781982422, 48.99028015136719]\n",
      "Epoch 291 Loss 28.0615 Current Train,Val,Test Scores [47.221126556396484, 47.491493225097656, 42.623565673828125]\n",
      "Epoch 292 Loss 28.0063 Current Train,Val,Test Scores [40.6280517578125, 40.93059158325195, 36.8478889465332]\n",
      "Epoch 293 Loss 27.8769 Current Train,Val,Test Scores [33.49443817138672, 33.809532165527344, 30.524656295776367]\n",
      "Epoch 294 Loss 27.6531 Current Train,Val,Test Scores [26.305143356323242, 26.68717384338379, 24.202089309692383]\n",
      "Epoch 295 Loss 27.0402 Current Train,Val,Test Scores [19.801654815673828, 20.225128173828125, 18.494966506958008]\n",
      "Epoch 296 Loss 26.4028 Current Train,Val,Test Scores [20.4812068939209, 20.4473876953125, 19.00713539123535]\n",
      "Epoch 297 Loss 26.2333 Current Train,Val,Test Scores [56.41752624511719, 55.31798553466797, 49.80951690673828]\n",
      "Epoch 298 Loss 26.3422 Current Train,Val,Test Scores [55.33272171020508, 53.24988555908203, 49.885013580322266]\n",
      "Epoch 299 Loss 26.5295 Current Train,Val,Test Scores [39.302520751953125, 37.44050979614258, 35.883941650390625]\n",
      "Epoch 300 Loss 26.3491 Current Train,Val,Test Scores [4.445143222808838, 4.932779788970947, 5.103515148162842]\n",
      "Epoch 301 Loss 25.8653 Current Train,Val,Test Scores [3.5100975036621094, 4.022898197174072, 4.368419170379639]\n",
      "Epoch 302 Loss 25.4470 Current Train,Val,Test Scores [4.479389190673828, 5.006279468536377, 5.196811676025391]\n",
      "Epoch 303 Loss 25.0087 Current Train,Val,Test Scores [2.9226253032684326, 3.4128715991973877, 3.8981833457946777]\n",
      "Epoch 304 Loss 24.6756 Current Train,Val,Test Scores [2.7969908714294434, 3.5660946369171143, 3.803957223892212]\n",
      "Epoch 305 Loss 24.6449 Current Train,Val,Test Scores [2.6265294551849365, 3.277228355407715, 3.591172218322754]\n",
      "Epoch 306 Loss 24.6913 Current Train,Val,Test Scores [2.3706018924713135, 2.930255889892578, 3.3406314849853516]\n",
      "Epoch 307 Loss 24.5676 Current Train,Val,Test Scores [5.793186664581299, 6.393815517425537, 6.347604274749756]\n",
      "Epoch 308 Loss 24.3597 Current Train,Val,Test Scores [5.491691589355469, 6.095118045806885, 6.081342697143555]\n",
      "Epoch 309 Loss 23.9953 Current Train,Val,Test Scores [5.3288798332214355, 5.938457012176514, 5.937255382537842]\n",
      "Epoch 310 Loss 23.6037 Current Train,Val,Test Scores [3.08832049369812, 3.6825718879699707, 4.002511501312256]\n",
      "Epoch 311 Loss 23.3957 Current Train,Val,Test Scores [2.366619110107422, 2.967163324356079, 3.377122163772583]\n",
      "Epoch 312 Loss 23.2212 Current Train,Val,Test Scores [3.142666816711426, 3.7581698894500732, 4.027270793914795]\n",
      "Epoch 313 Loss 23.1057 Current Train,Val,Test Scores [3.261303663253784, 3.8851001262664795, 4.131527900695801]\n",
      "Epoch 314 Loss 23.1018 Current Train,Val,Test Scores [4.222836971282959, 4.830613613128662, 4.97577428817749]\n",
      "Epoch 315 Loss 23.1465 Current Train,Val,Test Scores [3.5827322006225586, 4.198153018951416, 4.42373514175415]\n",
      "Epoch 316 Loss 23.2391 Current Train,Val,Test Scores [4.910699367523193, 5.511176109313965, 5.588137149810791]\n",
      "Epoch 317 Loss 23.5961 Current Train,Val,Test Scores [3.665578842163086, 4.284705638885498, 4.510790824890137]\n",
      "Epoch 318 Loss 24.3935 Current Train,Val,Test Scores [5.926642417907715, 6.524290084838867, 6.482193470001221]\n",
      "Epoch 319 Loss 26.2265 Current Train,Val,Test Scores [2.1901323795318604, 2.7888033390045166, 3.26061749458313]\n",
      "Epoch 320 Loss 28.1109 Current Train,Val,Test Scores [5.674654006958008, 6.302985191345215, 6.257068157196045]\n",
      "Epoch 321 Loss 32.1856 Current Train,Val,Test Scores [32.15655517578125, 31.309280395507812, 28.748979568481445]\n",
      "Epoch 322 Loss 33.1565 Current Train,Val,Test Scores [5.511900901794434, 6.177915096282959, 6.124211311340332]\n",
      "Epoch 323 Loss 36.0723 Current Train,Val,Test Scores [27.280689239501953, 26.266918182373047, 24.313457489013672]\n",
      "Epoch 324 Loss 32.0041 Current Train,Val,Test Scores [6.749095916748047, 7.381296634674072, 7.219292640686035]\n",
      "Epoch 325 Loss 27.8334 Current Train,Val,Test Scores [2.0193727016448975, 2.7056233882904053, 3.106337308883667]\n",
      "Epoch 326 Loss 22.4308 Current Train,Val,Test Scores [2.164896011352539, 2.8179056644439697, 3.202842950820923]\n",
      "Epoch 327 Loss 20.9005 Current Train,Val,Test Scores [2.1057026386260986, 2.7621116638183594, 3.1281144618988037]\n",
      "Epoch 328 Loss 22.9649 Current Train,Val,Test Scores [1.9807156324386597, 2.7979822158813477, 3.1835334300994873]\n",
      "Epoch 329 Loss 25.0425 Current Train,Val,Test Scores [1.7362605333328247, 2.3694097995758057, 2.7964587211608887]\n",
      "Epoch 330 Loss 25.3705 Current Train,Val,Test Scores [3.562614679336548, 4.04692268371582, 4.452491760253906]\n",
      "Epoch 331 Loss 22.4950 Current Train,Val,Test Scores [1.762625813484192, 2.4492850303649902, 2.82903790473938]\n",
      "Epoch 332 Loss 20.2671 Current Train,Val,Test Scores [1.8896440267562866, 2.5269362926483154, 2.9141430854797363]\n",
      "Epoch 333 Loss 19.9236 Current Train,Val,Test Scores [2.4872655868530273, 3.1760168075561523, 3.4824295043945312]\n",
      "Epoch 334 Loss 21.1846 Current Train,Val,Test Scores [1.7238737344741821, 2.3186984062194824, 2.708022117614746]\n",
      "Epoch 335 Loss 22.5601 Current Train,Val,Test Scores [5.535455226898193, 5.889148235321045, 6.054531097412109]\n",
      "Epoch 336 Loss 21.8635 Current Train,Val,Test Scores [1.899715781211853, 2.7555465698242188, 3.0914974212646484]\n",
      "Epoch 337 Loss 21.5521 Current Train,Val,Test Scores [125.30803680419922, 122.40830993652344, 111.5781478881836]\n",
      "Epoch 338 Loss 24.2613 Current Train,Val,Test Scores [91.30451965332031, 88.96135711669922, 81.69336700439453]\n",
      "Epoch 339 Loss 24.1943 Current Train,Val,Test Scores [177.44212341308594, 174.8175506591797, 155.73435974121094]\n",
      "Epoch 340 Loss 20.7495 Current Train,Val,Test Scores [123.01325225830078, 121.45723724365234, 107.32453155517578]\n",
      "Epoch 341 Loss 22.5617 Current Train,Val,Test Scores [59.0344352722168, 58.20222091674805, 51.9091682434082]\n",
      "Epoch 342 Loss 22.0933 Current Train,Val,Test Scores [42.81950759887695, 42.324745178222656, 37.34580993652344]\n",
      "Epoch 343 Loss 18.1313 Current Train,Val,Test Scores [36.423580169677734, 36.072975158691406, 31.762983322143555]\n",
      "Epoch 344 Loss 19.2161 Current Train,Val,Test Scores [12.632226943969727, 12.450567245483398, 12.098137855529785]\n",
      "Epoch 345 Loss 18.9518 Current Train,Val,Test Scores [48.5948371887207, 48.077049255371094, 42.47898483276367]\n",
      "Epoch 346 Loss 16.9632 Current Train,Val,Test Scores [107.98372650146484, 106.51207733154297, 94.26739501953125]\n",
      "Epoch 347 Loss 18.6606 Current Train,Val,Test Scores [117.8577880859375, 116.1981201171875, 102.98538208007812]\n",
      "Epoch 348 Loss 17.5863 Current Train,Val,Test Scores [115.26205444335938, 113.38956451416016, 101.01592254638672]\n",
      "Epoch 349 Loss 17.8733 Current Train,Val,Test Scores [104.09314727783203, 102.5345687866211, 90.8746337890625]\n",
      "Epoch 350 Loss 17.4523 Current Train,Val,Test Scores [91.00286102294922, 89.66819763183594, 79.4573974609375]\n",
      "Epoch 351 Loss 16.7800 Current Train,Val,Test Scores [34.69873046875, 33.558589935302734, 30.72481918334961]\n",
      "Epoch 352 Loss 16.6085 Current Train,Val,Test Scores [14.60237979888916, 14.166287422180176, 13.530570983886719]\n",
      "Epoch 353 Loss 16.3859 Current Train,Val,Test Scores [16.65682601928711, 15.968860626220703, 15.14192008972168]\n",
      "Epoch 354 Loss 15.4465 Current Train,Val,Test Scores [16.384033203125, 15.781373023986816, 14.91279125213623]\n",
      "Epoch 355 Loss 15.6858 Current Train,Val,Test Scores [13.510647773742676, 13.169746398925781, 12.484611511230469]\n",
      "Epoch 356 Loss 14.8165 Current Train,Val,Test Scores [24.65880012512207, 23.876079559326172, 21.518638610839844]\n",
      "Epoch 357 Loss 14.7185 Current Train,Val,Test Scores [14.627490043640137, 14.292543411254883, 13.27762222290039]\n",
      "Epoch 358 Loss 15.0534 Current Train,Val,Test Scores [6.84204626083374, 7.331401348114014, 6.865054130554199]\n",
      "Epoch 359 Loss 14.0031 Current Train,Val,Test Scores [5.954887390136719, 6.742276191711426, 6.128775596618652]\n",
      "Epoch 360 Loss 14.8816 Current Train,Val,Test Scores [22.338409423828125, 22.04375457763672, 19.272756576538086]\n",
      "Epoch 361 Loss 13.8760 Current Train,Val,Test Scores [29.881572723388672, 29.53498077392578, 25.814258575439453]\n",
      "Epoch 362 Loss 13.9140 Current Train,Val,Test Scores [4.3869194984436035, 5.252685546875, 5.277654647827148]\n",
      "Epoch 363 Loss 13.8618 Current Train,Val,Test Scores [4.00195837020874, 4.658414840698242, 4.547177314758301]\n",
      "Epoch 364 Loss 13.5863 Current Train,Val,Test Scores [3.8765530586242676, 4.536260604858398, 4.4176530838012695]\n",
      "Epoch 365 Loss 14.3767 Current Train,Val,Test Scores [4.45819616317749, 5.332703113555908, 5.3314924240112305]\n",
      "Epoch 366 Loss 16.1223 Current Train,Val,Test Scores [3.419741630554199, 4.085869312286377, 3.9762003421783447]\n",
      "Epoch 367 Loss 20.3166 Current Train,Val,Test Scores [3.9677700996398926, 4.6667680740356445, 4.83010721206665]\n",
      "Epoch 368 Loss 33.0614 Current Train,Val,Test Scores [5.313025951385498, 5.9852070808410645, 5.8146281242370605]\n",
      "Epoch 369 Loss 54.5071 Current Train,Val,Test Scores [9.802083969116211, 10.48546314239502, 9.669851303100586]\n",
      "Epoch 370 Loss 116.7348 Current Train,Val,Test Scores [11.404158592224121, 11.990693092346191, 11.140911102294922]\n",
      "Epoch 371 Loss 88.6516 Current Train,Val,Test Scores [10.28069019317627, 10.85997486114502, 10.187728881835938]\n",
      "Epoch 372 Loss 76.4547 Current Train,Val,Test Scores [268.1560974121094, 264.0001525878906, 235.6895751953125]\n",
      "Epoch 373 Loss 52.3120 Current Train,Val,Test Scores [51.5545539855957, 51.57490539550781, 46.34090042114258]\n",
      "Epoch 374 Loss 105.9077 Current Train,Val,Test Scores [107.64873504638672, 106.87837219238281, 95.61996459960938]\n",
      "Epoch 375 Loss 243.5328 Current Train,Val,Test Scores [9.788361549377441, 10.385226249694824, 9.761897087097168]\n",
      "Epoch 376 Loss 88.8068 Current Train,Val,Test Scores [48.21704864501953, 46.95676040649414, 43.314186096191406]\n",
      "Epoch 377 Loss 107.8621 Current Train,Val,Test Scores [719.9974365234375, 710.0595092773438, 631.1380004882812]\n",
      "Epoch 378 Loss 81.5728 Current Train,Val,Test Scores [1595.5509033203125, 1572.88232421875, 1399.333984375]\n",
      "Epoch 379 Loss 192.6287 Current Train,Val,Test Scores [145.18057250976562, 143.86085510253906, 128.5666961669922]\n",
      "Epoch 380 Loss 96.7905 Current Train,Val,Test Scores [150.4685821533203, 147.6595001220703, 133.1404266357422]\n",
      "Epoch 381 Loss 165.8680 Current Train,Val,Test Scores [274.138916015625, 270.9251708984375, 241.8123321533203]\n",
      "Epoch 382 Loss 246.9734 Current Train,Val,Test Scores [513.427490234375, 505.39697265625, 451.515380859375]\n",
      "Epoch 383 Loss 353.5022 Current Train,Val,Test Scores [107.69107055664062, 105.63419342041016, 95.69422912597656]\n",
      "Epoch 384 Loss 295.2910 Current Train,Val,Test Scores [126.49345397949219, 125.4715347290039, 112.51544189453125]\n",
      "Epoch 385 Loss 361.4849 Current Train,Val,Test Scores [186.8641357421875, 183.8443145751953, 165.20925903320312]\n",
      "Epoch 386 Loss 622.0134 Current Train,Val,Test Scores [164.375732421875, 161.85850524902344, 145.6360626220703]\n",
      "Epoch 387 Loss 754.6132 Current Train,Val,Test Scores [69.75791931152344, 70.02851104736328, 62.885005950927734]\n",
      "Epoch 388 Loss 643.0585 Current Train,Val,Test Scores [101.4351577758789, 101.3146743774414, 90.8327865600586]\n",
      "Epoch 389 Loss 579.2979 Current Train,Val,Test Scores [82.3492202758789, 81.32273864746094, 74.15721893310547]\n",
      "Epoch 390 Loss 613.7830 Current Train,Val,Test Scores [117.72856140136719, 116.13555145263672, 105.50431060791016]\n",
      "Epoch 391 Loss 568.1935 Current Train,Val,Test Scores [63.577064514160156, 62.71394729614258, 58.38876724243164]\n",
      "Epoch 392 Loss 467.7954 Current Train,Val,Test Scores [51.62435531616211, 52.1013069152832, 48.28056335449219]\n",
      "Epoch 393 Loss 776.1191 Current Train,Val,Test Scores [32.74053955078125, 33.37041473388672, 31.969823837280273]\n",
      "Epoch 394 Loss 687.9302 Current Train,Val,Test Scores [41.79749298095703, 40.94135665893555, 40.237274169921875]\n",
      "Epoch 395 Loss 623.8033 Current Train,Val,Test Scores [37.76056671142578, 36.63945770263672, 36.59857940673828]\n",
      "Epoch 396 Loss 468.5202 Current Train,Val,Test Scores [72.85205078125, 72.61675262451172, 67.42719268798828]\n",
      "Epoch 397 Loss 347.2107 Current Train,Val,Test Scores [107.44511413574219, 107.5669937133789, 97.63690185546875]\n",
      "Epoch 398 Loss 409.5073 Current Train,Val,Test Scores [170.4662322998047, 170.27783203125, 153.03610229492188]\n",
      "Epoch 399 Loss 329.6285 Current Train,Val,Test Scores [220.26817321777344, 219.96920776367188, 196.902099609375]\n",
      "Epoch 400 Loss 320.5535 Current Train,Val,Test Scores [210.9084930419922, 210.81845092773438, 188.58837890625]\n",
      "Epoch 401 Loss 243.0125 Current Train,Val,Test Scores [197.26922607421875, 197.27064514160156, 176.40975952148438]\n",
      "Epoch 402 Loss 287.1362 Current Train,Val,Test Scores [204.81082153320312, 204.73184204101562, 182.99517822265625]\n",
      "Epoch 403 Loss 254.0198 Current Train,Val,Test Scores [210.36587524414062, 210.2987060546875, 187.93994140625]\n",
      "Epoch 404 Loss 225.2913 Current Train,Val,Test Scores [190.96559143066406, 191.06964111328125, 170.79507446289062]\n",
      "Epoch 405 Loss 219.5767 Current Train,Val,Test Scores [153.6615753173828, 154.00631713867188, 137.84402465820312]\n",
      "Epoch 406 Loss 217.7886 Current Train,Val,Test Scores [133.08584594726562, 133.51788330078125, 119.68437957763672]\n",
      "Epoch 407 Loss 209.5899 Current Train,Val,Test Scores [124.03250885009766, 124.33023071289062, 111.76347351074219]\n",
      "Epoch 408 Loss 192.4791 Current Train,Val,Test Scores [109.96168518066406, 110.25711059570312, 99.24518585205078]\n",
      "Epoch 409 Loss 198.2179 Current Train,Val,Test Scores [89.6820068359375, 90.10894775390625, 81.15461730957031]\n",
      "Epoch 410 Loss 176.1793 Current Train,Val,Test Scores [75.75736999511719, 76.1960220336914, 68.75955200195312]\n",
      "Epoch 411 Loss 177.5157 Current Train,Val,Test Scores [67.8909912109375, 68.19742584228516, 61.82902526855469]\n",
      "Epoch 412 Loss 157.8888 Current Train,Val,Test Scores [55.694000244140625, 55.97953796386719, 50.928001403808594]\n",
      "Epoch 413 Loss 157.6401 Current Train,Val,Test Scores [39.58168029785156, 39.8675651550293, 36.74498748779297]\n",
      "Epoch 414 Loss 144.6326 Current Train,Val,Test Scores [31.166542053222656, 31.509122848510742, 29.338298797607422]\n",
      "Epoch 415 Loss 137.2352 Current Train,Val,Test Scores [25.009923934936523, 25.37197494506836, 23.923866271972656]\n",
      "Epoch 416 Loss 133.0812 Current Train,Val,Test Scores [18.15529441833496, 18.530397415161133, 17.86091423034668]\n",
      "Epoch 417 Loss 118.5942 Current Train,Val,Test Scores [11.924113273620605, 12.307241439819336, 12.279739379882812]\n",
      "Epoch 418 Loss 123.1990 Current Train,Val,Test Scores [8.453248977661133, 8.910890579223633, 9.165580749511719]\n",
      "Epoch 419 Loss 105.9567 Current Train,Val,Test Scores [123.60966491699219, 123.60435485839844, 110.71475982666016]\n",
      "Epoch 420 Loss 115.4638 Current Train,Val,Test Scores [132.1993408203125, 132.01548767089844, 118.2076187133789]\n",
      "Epoch 421 Loss 96.0261 Current Train,Val,Test Scores [179.30990600585938, 178.5220489501953, 159.56871032714844]\n",
      "Epoch 422 Loss 100.1000 Current Train,Val,Test Scores [303.41729736328125, 300.8625793457031, 268.59368896484375]\n",
      "Epoch 423 Loss 94.8922 Current Train,Val,Test Scores [330.6777648925781, 327.72821044921875, 292.4648132324219]\n",
      "Epoch 424 Loss 81.6422 Current Train,Val,Test Scores [358.48785400390625, 355.237060546875, 316.68890380859375]\n",
      "Epoch 425 Loss 88.8959 Current Train,Val,Test Scores [488.8404541015625, 483.9222717285156, 430.97064208984375]\n",
      "Epoch 426 Loss 84.2081 Current Train,Val,Test Scores [506.3841247558594, 501.31134033203125, 446.3518981933594]\n",
      "Epoch 427 Loss 74.3622 Current Train,Val,Test Scores [546.5975952148438, 541.0060424804688, 481.7162780761719]\n",
      "Epoch 428 Loss 74.6211 Current Train,Val,Test Scores [633.1865234375, 626.6566772460938, 557.6732177734375]\n",
      "Epoch 429 Loss 76.5789 Current Train,Val,Test Scores [574.366455078125, 568.5233154296875, 506.05413818359375]\n",
      "Epoch 430 Loss 72.8410 Current Train,Val,Test Scores [616.8757934570312, 610.9494018554688, 543.3335571289062]\n",
      "Epoch 431 Loss 67.4971 Current Train,Val,Test Scores [666.1409912109375, 660.049072265625, 586.7267456054688]\n",
      "Epoch 432 Loss 69.5379 Current Train,Val,Test Scores [539.9008178710938, 535.14306640625, 475.67926025390625]\n",
      "Epoch 433 Loss 71.1082 Current Train,Val,Test Scores [615.1378173828125, 609.98291015625, 541.9766235351562]\n",
      "Epoch 434 Loss 64.0593 Current Train,Val,Test Scores [566.0189819335938, 561.5982666015625, 498.8558044433594]\n",
      "Epoch 435 Loss 61.7819 Current Train,Val,Test Scores [464.5177307128906, 461.4060974121094, 409.6009216308594]\n",
      "Epoch 436 Loss 64.3785 Current Train,Val,Test Scores [538.3204956054688, 534.470703125, 474.5590515136719]\n",
      "Epoch 437 Loss 62.0001 Current Train,Val,Test Scores [444.55377197265625, 441.9329833984375, 392.0925598144531]\n",
      "Epoch 438 Loss 58.1692 Current Train,Val,Test Scores [423.6715393066406, 421.36981201171875, 373.72705078125]\n",
      "Epoch 439 Loss 56.7365 Current Train,Val,Test Scores [443.35150146484375, 440.87841796875, 391.0686340332031]\n",
      "Epoch 440 Loss 56.7361 Current Train,Val,Test Scores [330.08453369140625, 328.6700744628906, 291.30438232421875]\n",
      "Epoch 441 Loss 56.6051 Current Train,Val,Test Scores [380.9060363769531, 379.1525573730469, 336.1582946777344]\n",
      "Epoch 442 Loss 53.3649 Current Train,Val,Test Scores [319.5424499511719, 318.2754211425781, 282.017333984375]\n",
      "Epoch 443 Loss 50.1261 Current Train,Val,Test Scores [291.4522705078125, 290.28424072265625, 257.2331848144531]\n",
      "Epoch 444 Loss 49.1509 Current Train,Val,Test Scores [295.89447021484375, 294.824462890625, 261.1548767089844]\n",
      "Epoch 445 Loss 48.2543 Current Train,Val,Test Scores [220.8880615234375, 219.69802856445312, 195.14157104492188]\n",
      "Epoch 446 Loss 48.1605 Current Train,Val,Test Scores [244.69883728027344, 243.51560974121094, 216.08096313476562]\n",
      "Epoch 447 Loss 46.0830 Current Train,Val,Test Scores [206.89678955078125, 205.65635681152344, 182.69830322265625]\n",
      "Epoch 448 Loss 43.8596 Current Train,Val,Test Scores [198.3668670654297, 197.0542449951172, 175.1402130126953]\n",
      "Epoch 449 Loss 43.1291 Current Train,Val,Test Scores [193.96481323242188, 192.60191345214844, 171.2596435546875]\n",
      "Epoch 450 Loss 42.1529 Current Train,Val,Test Scores [167.75283813476562, 166.05482482910156, 148.18089294433594]\n",
      "Epoch 451 Loss 42.3685 Current Train,Val,Test Scores [182.7209930419922, 181.0927276611328, 161.33311462402344]\n",
      "Epoch 452 Loss 40.9411 Current Train,Val,Test Scores [174.8115997314453, 172.34451293945312, 154.64370727539062]\n",
      "Epoch 453 Loss 40.1081 Current Train,Val,Test Scores [167.641845703125, 165.0599365234375, 148.24217224121094]\n",
      "Epoch 454 Loss 39.3717 Current Train,Val,Test Scores [162.26063537597656, 159.6851806640625, 143.4595947265625]\n",
      "Epoch 455 Loss 38.9186 Current Train,Val,Test Scores [164.5295867919922, 161.6084442138672, 145.6474609375]\n",
      "Epoch 456 Loss 38.7051 Current Train,Val,Test Scores [173.27972412109375, 170.5453338623047, 153.1022186279297]\n",
      "Epoch 457 Loss 37.6222 Current Train,Val,Test Scores [182.86546325683594, 179.6610107421875, 161.72531127929688]\n",
      "Epoch 458 Loss 36.7335 Current Train,Val,Test Scores [193.3553009033203, 190.09364318847656, 170.8940887451172]\n",
      "Epoch 459 Loss 35.5662 Current Train,Val,Test Scores [209.41409301757812, 205.8620147705078, 185.03941345214844]\n",
      "Epoch 460 Loss 35.0947 Current Train,Val,Test Scores [233.11192321777344, 229.26097106933594, 205.91952514648438]\n",
      "Epoch 461 Loss 34.5709 Current Train,Val,Test Scores [257.0187683105469, 253.14126586914062, 226.74948120117188]\n",
      "Epoch 462 Loss 34.4761 Current Train,Val,Test Scores [277.3412780761719, 273.00048828125, 244.88087463378906]\n",
      "Epoch 463 Loss 34.1262 Current Train,Val,Test Scores [300.6768493652344, 296.2270812988281, 265.249755859375]\n",
      "Epoch 464 Loss 33.6944 Current Train,Val,Test Scores [325.52423095703125, 320.73046875, 287.3063049316406]\n",
      "Epoch 465 Loss 32.8314 Current Train,Val,Test Scores [347.7866516113281, 343.2006530761719, 306.6010437011719]\n",
      "Epoch 466 Loss 32.2132 Current Train,Val,Test Scores [363.2143859863281, 358.4063720703125, 320.13873291015625]\n",
      "Epoch 467 Loss 31.4931 Current Train,Val,Test Scores [381.74639892578125, 376.6650695800781, 336.3944091796875]\n",
      "Epoch 468 Loss 31.2162 Current Train,Val,Test Scores [404.5024108886719, 399.1289978027344, 356.3124694824219]\n",
      "Epoch 469 Loss 30.8027 Current Train,Val,Test Scores [425.8221130371094, 420.0655212402344, 375.0547180175781]\n",
      "Epoch 470 Loss 30.6354 Current Train,Val,Test Scores [453.3522644042969, 447.34637451171875, 399.0754089355469]\n",
      "Epoch 471 Loss 30.2813 Current Train,Val,Test Scores [472.0238342285156, 465.5811462402344, 415.5939636230469]\n",
      "Epoch 472 Loss 30.0081 Current Train,Val,Test Scores [505.6075134277344, 499.1258544921875, 444.8014831542969]\n",
      "Epoch 473 Loss 29.5518 Current Train,Val,Test Scores [525.1050415039062, 517.8794555664062, 462.1324462890625]\n",
      "Epoch 474 Loss 29.1681 Current Train,Val,Test Scores [562.5316162109375, 555.3984375, 494.744384765625]\n",
      "Epoch 475 Loss 28.8109 Current Train,Val,Test Scores [575.6598510742188, 567.6820678710938, 506.54168701171875]\n",
      "Epoch 476 Loss 28.5189 Current Train,Val,Test Scores [614.280029296875, 606.3568115234375, 540.1590576171875]\n",
      "Epoch 477 Loss 28.4454 Current Train,Val,Test Scores [627.5540161132812, 618.8320922851562, 552.129150390625]\n",
      "Epoch 478 Loss 28.9341 Current Train,Val,Test Scores [678.121337890625, 669.7049560546875, 596.1571044921875]\n",
      "Epoch 479 Loss 29.9094 Current Train,Val,Test Scores [671.7858276367188, 662.4887084960938, 591.0242919921875]\n",
      "Epoch 480 Loss 33.3324 Current Train,Val,Test Scores [745.9843139648438, 737.1846313476562, 655.806884765625]\n",
      "Epoch 481 Loss 37.1239 Current Train,Val,Test Scores [674.5380249023438, 664.4171142578125, 593.6077270507812]\n",
      "Epoch 482 Loss 49.5032 Current Train,Val,Test Scores [743.4689331054688, 734.064208984375, 653.8766479492188]\n",
      "Epoch 483 Loss 50.8974 Current Train,Val,Test Scores [551.3241577148438, 543.1506958007812, 485.4735107421875]\n",
      "Epoch 484 Loss 63.9153 Current Train,Val,Test Scores [474.9216613769531, 466.9202880859375, 418.5455017089844]\n",
      "Epoch 485 Loss 43.0938 Current Train,Val,Test Scores [574.0338745117188, 565.3037109375, 505.3988952636719]\n",
      "Epoch 486 Loss 32.2607 Current Train,Val,Test Scores [619.4188232421875, 609.5054321289062, 545.45166015625]\n",
      "Epoch 487 Loss 31.4950 Current Train,Val,Test Scores [542.4971313476562, 532.03515625, 477.5540466308594]\n",
      "Epoch 488 Loss 34.5108 Current Train,Val,Test Scores [642.61181640625, 629.9884643554688, 565.8128051757812]\n",
      "Epoch 489 Loss 43.2250 Current Train,Val,Test Scores [759.6011352539062, 745.879150390625, 667.9708862304688]\n",
      "Epoch 490 Loss 28.2373 Current Train,Val,Test Scores [870.4627685546875, 854.8548583984375, 765.4413452148438]\n",
      "Epoch 491 Loss 26.2791 Current Train,Val,Test Scores [851.4542846679688, 836.4244995117188, 748.525634765625]\n",
      "Epoch 492 Loss 27.8195 Current Train,Val,Test Scores [855.8804931640625, 841.5032958984375, 752.1448974609375]\n",
      "Epoch 493 Loss 35.9377 Current Train,Val,Test Scores [797.1112670898438, 782.6900634765625, 700.7409057617188]\n",
      "Epoch 494 Loss 36.9699 Current Train,Val,Test Scores [755.4275512695312, 742.6129760742188, 663.8377075195312]\n",
      "Epoch 495 Loss 30.7780 Current Train,Val,Test Scores [519.3809814453125, 508.41436767578125, 456.67889404296875]\n",
      "Epoch 496 Loss 24.3681 Current Train,Val,Test Scores [377.70587158203125, 368.7867431640625, 332.47296142578125]\n",
      "Epoch 497 Loss 26.1976 Current Train,Val,Test Scores [382.9227600097656, 374.6352844238281, 336.8955993652344]\n",
      "Epoch 498 Loss 25.4984 Current Train,Val,Test Scores [375.8294982910156, 367.0917663574219, 330.74554443359375]\n",
      "Epoch 499 Loss 27.3987 Current Train,Val,Test Scores [371.662353515625, 363.45703125, 326.9997253417969]\n",
      "Best Train,Val,Test Scores [1.7238737344741821, 2.3186984062194824, 2.708022117614746]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creates a HeteroGNN model from the previously constructed hetero graph and trains it.\n",
    "\"\"\"\n",
    "\n",
    "best_model = None\n",
    "best_tvt_scores = (float(\"inf\"),float(\"inf\"),float(\"inf\"))\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    # Train\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    # Test for the accuracy of the model\n",
    "    cur_tvt_scores, best_tvt_scores, best_model = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_tvt_scores)\n",
    "    print(f\"Epoch {epoch} Loss {loss:.4f} Current Train,Val,Test Scores {[score.item() for score in cur_tvt_scores]}\")\n",
    "\n",
    "print(\"Best Train,Val,Test Scores\", [score.item() for score in best_tvt_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Score: 3.5347557067871094\n",
      "tensor([3.7143], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([9.2473], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([117.6678], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([11.5941], device='cuda:0', grad_fn=<SelectBackward0>) tensor([94.], device='cuda:0')\n",
      "tensor([169.4548], device='cuda:0', grad_fn=<SelectBackward0>) tensor([48.], device='cuda:0')\n",
      "tensor([4.8506], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([148.3758], device='cuda:0', grad_fn=<SelectBackward0>) tensor([111.], device='cuda:0')\n",
      "tensor([11.8923], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([9.9554], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([1.2624], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.8052], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([14.3258], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([7.1329], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([17.0624], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([2.8923], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([4.2447], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([47.3592], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([9.1579], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([10.6999], device='cuda:0', grad_fn=<SelectBackward0>) tensor([58.], device='cuda:0')\n",
      "tensor([7.8238], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([7.1719], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([8.8845], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([2.0164], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([11.7586], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([7.7086], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([5.6528], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([6.3750], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([8.8944], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([193.5004], device='cuda:0', grad_fn=<SelectBackward0>) tensor([32.], device='cuda:0')\n",
      "tensor([8.2276], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([45.7223], device='cuda:0', grad_fn=<SelectBackward0>) tensor([19.], device='cuda:0')\n",
      "tensor([12.7096], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([18.3513], device='cuda:0', grad_fn=<SelectBackward0>) tensor([144.], device='cuda:0')\n",
      "tensor([83.5605], device='cuda:0', grad_fn=<SelectBackward0>) tensor([79.], device='cuda:0')\n",
      "tensor([5.5363], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([5.3908], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([8.5851], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([4.8865], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([8.9230], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([7.7545], device='cuda:0', grad_fn=<SelectBackward0>) tensor([51.], device='cuda:0')\n",
      "tensor([5.2685], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.9616], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([101.3589], device='cuda:0', grad_fn=<SelectBackward0>) tensor([40.], device='cuda:0')\n",
      "tensor([-0.0263], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([2.9592], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([195.7030], device='cuda:0', grad_fn=<SelectBackward0>) tensor([607.], device='cuda:0')\n",
      "tensor([11.7417], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([8.9573], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([14.5609], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "model.load_state_dict(torch.load('./best_model.pkl'))\n",
    "preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "# mask = preds['event'] > 0\n",
    "# preds['event'][preds['event'] > 0].shape\n",
    "\n",
    "# print(preds['event'][0], hetero_graph.node_target['event'][0]) \n",
    "\n",
    "\n",
    "#print(hetero_graph.node_feature['event'])\n",
    "\n",
    "# Assuming val_idx is the dictionary containing the validation indices for 'event' node type\n",
    "\n",
    "# TODO: Best validation score doesn't match the best validation score from the training loop\n",
    "best_val = torch.sum(torch.abs(preds['event'][val_idx['event']] - hetero_graph.node_target['event'][val_idx['event']])) / val_idx['event'].shape[0]\n",
    "\n",
    "# Print the validation loss\n",
    "print(f\"Best Validation Score: {best_val.item()}\")\n",
    "\n",
    "# for i in range(3000):\n",
    "#     if hetero_graph.node_target['event'][i] != -1: # concepts have node target -1\n",
    "#         print(preds['event'][i], hetero_graph.node_target['event'][i])\n",
    "        \n",
    "    \n",
    "for i in range(1000):    \n",
    "    if hetero_graph.node_target['event'][test_idx['event']][i] != -1:\n",
    "        print(preds['event'][test_idx['event']][i], hetero_graph.node_target['event'][test_idx['event']][i])\n",
    "\n",
    "\n",
    "# for i in range(1000):\n",
    "#     # Extract the target value and check if it is not equal to -1\n",
    "#     target = hetero_graph.node_target['event'][test_idx['event']][i].item()\n",
    "#     if target != -1:\n",
    "#         # Extract the prediction value\n",
    "#         pred = preds['event'][test_idx['event']][i].item()\n",
    "#         print(f\"Prediction: {pred:.4f}, Target: {target:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-15 16:53:44,983] A new study created in memory with name: no-name-dec145c2-7bae-441d-9988-dca921fceab5\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamgrobelnik\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\Heterogeneus\\wandb\\run-20231115_165348-phqal3l6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/phqal3l6' target=\"_blank\">grateful-shadow-30</a></strong> to <a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/phqal3l6' target=\"_blank\">https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/phqal3l6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f1e13742da46bd9bf2dd59180c484d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.009 MB uploaded\\r'), FloatProgress(value=0.6199817054578718, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>████▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>███▇▇▇▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▅▆▄▆█▆▇▆▄▃▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>1.8033</td></tr><tr><td>epoch</td><td>216</td></tr><tr><td>train_loss</td><td>807.52484</td></tr><tr><td>val_score</td><td>63.99316</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-shadow-30</strong> at: <a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/phqal3l6' target=\"_blank\">https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/phqal3l6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231115_165348-phqal3l6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-15 16:54:24,543] Trial 0 finished with value: 1.8032952547073364 and parameters: {'lr': 0.0003304449748517102, 'weight_decay': 0.00026366772696340374, 'hidden_size': 107, 'epochs': 217}. Best is trial 0 with value: 1.8032952547073364.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb12e47427484f31acf2d8687e9c886d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\Heterogeneus\\wandb\\run-20231115_165424-v5d8vcse</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/v5d8vcse' target=\"_blank\">young-night-31</a></strong> to <a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/v5d8vcse' target=\"_blank\">https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/v5d8vcse</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1b88dbb3904220b31d42230c0e27d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.009 MB uploaded\\r'), FloatProgress(value=0.6197297571878493, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>█▇▆▆▆▆▆▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█████████▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>val_score</td><td>█▇▆▆▆▆▆▆▆▅▅▅▅▅▅▅▅▄▄▅▅▅▄▃▃▃▃▂▁▂▂▂▂▂▂▃▃▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>2.07675</td></tr><tr><td>epoch</td><td>264</td></tr><tr><td>train_loss</td><td>2507.3064</td></tr><tr><td>val_score</td><td>2.09944</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">young-night-31</strong> at: <a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/v5d8vcse' target=\"_blank\">https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/v5d8vcse</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231115_165424-v5d8vcse\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-15 16:55:00,243] Trial 1 finished with value: 2.076751232147217 and parameters: {'lr': 4.3237335615300063e-05, 'weight_decay': 2.4784271289641154e-05, 'hidden_size': 57, 'epochs': 265}. Best is trial 0 with value: 1.8032952547073364.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2697c68a596b46a8aff88dbf6f7dccfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888925108, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\Heterogeneus\\wandb\\run-20231115_165500-ylkme3q3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/ylkme3q3' target=\"_blank\">polished-puddle-32</a></strong> to <a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/ylkme3q3' target=\"_blank\">https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/ylkme3q3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8ce4ae12b04bf2b087ba0c80d71a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.009 MB uploaded\\r'), FloatProgress(value=0.14933414658940733, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>██████▇▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>val_score</td><td>▂▂▃▄▅▆▇▇▇▆▆▆▆▆▆▇██▇▅▄▃▂▂▃▂▂▂▂▁▁▁▂▂▃▂▃▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>2.56695</td></tr><tr><td>epoch</td><td>187</td></tr><tr><td>train_loss</td><td>2722.16504</td></tr><tr><td>val_score</td><td>2.56786</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polished-puddle-32</strong> at: <a href='https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/ylkme3q3' target=\"_blank\">https://wandb.ai/amgrobelnik/MLG_PredEvents_GNN%2BLMM/runs/ylkme3q3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231115_165500-ylkme3q3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-15 16:55:33,005] Trial 2 finished with value: 2.566950798034668 and parameters: {'lr': 1.8084265774303778e-05, 'weight_decay': 0.0003048045399202485, 'hidden_size': 77, 'epochs': 188}. Best is trial 0 with value: 1.8032952547073364.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced6b49eb9a84bc4a4b8bb5df75d5129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888884685, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-15 16:55:41,256] Trial 3 failed with parameters: {'lr': 0.00034086248965115197, 'weight_decay': 0.0003909437651516272, 'hidden_size': 53, 'epochs': 162} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_31484\\1481295212.py\", line 6, in objective\n",
      "    wandb.init(project=\"MLG_PredEvents_GNN+LMM\", config={\n",
      "  File \"c:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 1189, in init\n",
      "    raise e\n",
      "  File \"c:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 1166, in init\n",
      "    run = wi.init()\n",
      "  File \"c:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 811, in init\n",
      "    run_start_result = run_start_handle.wait(timeout=30)\n",
      "  File \"c:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 283, in wait\n",
      "    found, abandoned = self._slot._get_and_clear(timeout=wait_timeout)\n",
      "  File \"c:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 130, in _get_and_clear\n",
      "    if self._wait(timeout=timeout):\n",
      "  File \"c:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 126, in _wait\n",
      "    return self._event.wait(timeout=timeout)\n",
      "  File \"c:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\threading.py\", line 607, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"c:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\threading.py\", line 324, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "[W 2023-11-15 16:55:41,264] Trial 3 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_31484\\1481295212.py 6 objective\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\Heterogeneus\\Regression.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Create a study object and optimize the objective function\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Print the best hyperparameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32md:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\Heterogeneus\\Regression.ipynb Cell 15\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjective\u001b[39m(trial):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Initialize wandb run\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     wandb\u001b[39m.\u001b[39;49minit(project\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMLG_PredEvents_GNN+LMM\u001b[39;49m\u001b[39m\"\u001b[39;49m, config\u001b[39m=\u001b[39;49m{\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m: trial\u001b[39m.\u001b[39;49msuggest_float(\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1e-5\u001b[39;49m, \u001b[39m1e-1\u001b[39;49m, log\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m: trial\u001b[39m.\u001b[39;49msuggest_float(\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1e-5\u001b[39;49m, \u001b[39m1e-3\u001b[39;49m, log\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mhidden_size\u001b[39;49m\u001b[39m\"\u001b[39;49m: trial\u001b[39m.\u001b[39;49msuggest_int(\u001b[39m\"\u001b[39;49m\u001b[39mhidden_size\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m16\u001b[39;49m, \u001b[39m128\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mattn_size\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m32\u001b[39;49m,  \u001b[39m# Fixed value\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m\"\u001b[39;49m: trial\u001b[39m.\u001b[39;49msuggest_int(\u001b[39m\"\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m150\u001b[39;49m, \u001b[39m300\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mnum_layers\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m2\u001b[39;49m,  \u001b[39m# Fixed value\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     })\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Use wandb config\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/Heterogeneus/Regression.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     config \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39mconfig\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1189\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     \u001b[39massert\u001b[39;00m logger\n\u001b[0;32m   1188\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39minterrupted\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39me)\n\u001b[1;32m-> 1189\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m   1190\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1191\u001b[0m     error_seen \u001b[39m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1166\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1164\u001b[0m except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[0;32m   1165\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1166\u001b[0m     run \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39;49minit()\n\u001b[0;32m   1167\u001b[0m     except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[0;32m   1168\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:811\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    809\u001b[0m run_start_handle \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39minterface\u001b[39m.\u001b[39mdeliver_run_start(run\u001b[39m.\u001b[39m_run_obj)\n\u001b[0;32m    810\u001b[0m \u001b[39m# TODO: add progress to let user know we are doing something\u001b[39;00m\n\u001b[1;32m--> 811\u001b[0m run_start_result \u001b[39m=\u001b[39m run_start_handle\u001b[39m.\u001b[39;49mwait(timeout\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n\u001b[0;32m    812\u001b[0m \u001b[39mif\u001b[39;00m run_start_result \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    813\u001b[0m     run_start_handle\u001b[39m.\u001b[39mabandon()\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[1;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interface\u001b[39m.\u001b[39m_transport_keepalive_failed():\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m MailboxError(\u001b[39m\"\u001b[39m\u001b[39mtransport failed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 283\u001b[0m found, abandoned \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slot\u001b[39m.\u001b[39;49m_get_and_clear(timeout\u001b[39m=\u001b[39;49mwait_timeout)\n\u001b[0;32m    284\u001b[0m \u001b[39mif\u001b[39;00m found:\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Always update progress to 100% when done\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39mif\u001b[39;00m on_progress \u001b[39mand\u001b[39;00m progress_handle \u001b[39mand\u001b[39;00m progress_sent:\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_and_clear\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Optional[pb\u001b[39m.\u001b[39mResult], \u001b[39mbool\u001b[39m]:\n\u001b[0;32m    129\u001b[0m     found \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait(timeout\u001b[39m=\u001b[39;49mtimeout):\n\u001b[0;32m    131\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m             found \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wait\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout\u001b[39m=\u001b[39;49mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\mlg2\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[0;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Initialize wandb run\n",
    "    wandb.init(project=\"MLG_PredEvents_GNN+LMM\", config={\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True),\n",
    "        \"hidden_size\": trial.suggest_int(\"hidden_size\", 16, 128),\n",
    "        \"attn_size\": 32,  # Fixed value\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 150, 300),\n",
    "        \"num_layers\": 2,  # Fixed value\n",
    "    })\n",
    "\n",
    "    # Use wandb config\n",
    "    config = wandb.config\n",
    "\n",
    "    # Initialize the model with the new hyperparameters\n",
    "    model = HeteroGNN(hetero_graph, {\n",
    "        'hidden_size': config.hidden_size,\n",
    "        'attn_size': config.attn_size,\n",
    "        'device': args['device']\n",
    "    }, num_layers=config.num_layers, aggr=\"mean\").to(args['device'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "    # Initialize best scores with infinity\n",
    "    best_tvt_scores = (float(\"inf\"), float(\"inf\"), float(\"inf\"))\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "        cur_tvt_scores, best_tvt_scores, _ = test(model, hetero_graph, [train_idx, val_idx, test_idx], None, best_tvt_scores)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_score\": cur_tvt_scores[1],\n",
    "            \"best_val_score\": best_tvt_scores[1],\n",
    "        })\n",
    "\n",
    "        # Update the best validation score\n",
    "        if cur_tvt_scores[1] < best_tvt_scores[1]:\n",
    "            best_tvt_scores = (cur_tvt_scores[0], cur_tvt_scores[1], cur_tvt_scores[2])\n",
    "\n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "    # The objective value is the best validation score\n",
    "    return best_tvt_scores[1]\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"Value: {trial.value}\")\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
