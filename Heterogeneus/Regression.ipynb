{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # To simplify implementation, please initialize both self.lin_dst\n",
    "        # and self.lin_src out_features to out_channels\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "\n",
    "        self.lin_dst = nn.Linear(self.in_channels_src, self.out_channels)\n",
    "        self.lin_src = nn.Linear(self.in_channels_dst, self.out_channels)\n",
    "        self.lin_update = nn.Linear(2*self.out_channels, self.out_channels)\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "    ):\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "        ##########################################\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "        ## Note:\n",
    "        ## 1. Different from what we implemented in Colab 3, we use message_and_aggregate\n",
    "        ## to replace the message and aggregate. The benefit is that we can avoid\n",
    "        ## materializing x_i and x_j, and make the implementation more efficient.\n",
    "        ## 2. To implement efficiently, following PyG documentation is helpful:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html\n",
    "        ## 3. Here edge_index is torch_sparse SparseTensor.\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "        ##########################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~4 lines of code)\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        # print(aggr_out.shape, dst_out.shape)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        # print(aggr_out.shape, )\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "        ##########################################\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "            ############# Your code here #############\n",
    "            ## (~1 line of code)\n",
    "            ## Note:\n",
    "            ## 1. Initialize self.attn_proj here.\n",
    "            ## 2. You should use nn.Sequential for self.attn_proj\n",
    "            ## 3. nn.Linear and nn.Tanh are useful.\n",
    "            ## 4. You can create a vector parameter by using:\n",
    "            ## nn.Linear(some_size, 1, bias=False)\n",
    "            ## 5. The first linear layer should have out_features as args['attn_size']\n",
    "            ## 6. You can assume we only have one \"head\" for the attention.\n",
    "            ## 7. We recommend you to implement the mean aggregation first. After \n",
    "            ## the mean aggregation works well in the training, then you can \n",
    "            ## implement this part.\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "            #########################################\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        # TODO: Implement this function that aggregates all message type results.\n",
    "        # Here, xs is a list of tensors (embeddings) with respect to message \n",
    "        # type aggregation results.\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~2 lines of code)\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "            ##########################################\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~10 lines of code)\n",
    "            ## Note:\n",
    "            ## 1. Store the value of attention alpha (as a numpy array) to self.alpha,\n",
    "            ## which has the shape (len(xs), ) self.alpha will be not be used \n",
    "            ## to backpropagate etc. in the model. We will use it to see how much \n",
    "            ## attention the layer pays on different message types.\n",
    "            ## 2. torch.softmax and torch.cat are useful.\n",
    "            ## 3. You might need to reshape the tensors by using the \n",
    "            ## `view()` function https://pytorch.org/docs/stable/tensor_view.html\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out\n",
    "            ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    # TODO: Implement this function that returns a dictionary of `HeteroGNNConv` \n",
    "    # layers where the keys are message types. `hetero_graph` is deepsnap `HeteroGraph`\n",
    "    # object and the `conv` is the `HeteroGNNConv`.\n",
    "\n",
    "    convs = {}\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## (~9 lines of code)\n",
    "\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    ##########################################\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        self.convs1 = None\n",
    "        self.convs2 = None\n",
    "\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~10 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For self.convs1 and self.convs2, call generate_convs at first and then\n",
    "        ## pass the returned dictionary of `HeteroGNNConv` to `HeteroGNNWrapperConv`.\n",
    "        ## 2. For self.bns, self.relus and self.post_mps, the keys are node_types.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.node_types` will be helpful.\n",
    "        ## 3. Initialize all batchnorms to torch.nn.BatchNorm1d(hidden_size, eps=1.0).\n",
    "        ## 4. Initialize all relus to nn.LeakyReLU().\n",
    "        ## 5. For self.post_mps, each value in the ModuleDict is a linear layer \n",
    "        ## where the `out_features` is the number of classes for that node type.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.num_node_labels(node_type)` will be\n",
    "        ## useful.\n",
    "\n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            self.post_mps[node_type] = nn.Linear(self.hidden_size, hetero_graph.num_node_labels(node_type))\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(self, node_feature, edge_index):\n",
    "        # TODO: Implement the forward function. Notice that `node_feature` is \n",
    "        # a dictionary of tensors where keys are node types and values are \n",
    "        # corresponding feature tensors. The `edge_index` is a dictionary of \n",
    "        # tensors where keys are message types and values are corresponding\n",
    "        # edge index tensors (with respect to each message type).\n",
    "\n",
    "        x = node_feature\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~7 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. `deepsnap.hetero_gnn.forward_op` can be helpful.\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        # TODO: remove this for regression tasks\n",
    "        # x = forward_op(x, self.post_mps)\n",
    "        x = forward_op(x, self.fc)\n",
    "        #print(\"X\", x)\n",
    "\n",
    "        ##########################################\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        loss = 0\n",
    "        loss_func = torch.nn.L1Loss()\n",
    "\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For each node type in preds, accumulate computed loss to `loss`\n",
    "        ## 2. Loss need to be computed with respect to the given index\n",
    "\n",
    "        #for node_type in preds:\n",
    "        mask = y['event'] > 0\n",
    "\n",
    "        idx = torch.masked_select(indices['event'], mask)\n",
    "        loss += loss_func(preds['event'][idx], y['event'][idx])\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    loss = None\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## Note:\n",
    "    ## 1. `deepsnap.hetero_graph.HeteroGraph.node_label` is useful\n",
    "    ## 2. Compute the loss here\n",
    "    \n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "    ##########################################\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(model, graph, indices, best_model=None, best_val=0):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        \n",
    "        #print(\"Index\", index)\n",
    "        #print(\"Preds\", preds['event'])\n",
    "\n",
    "        idx = index['event']\n",
    "\n",
    "        L1 = torch.sum(torch.abs(preds['event'][idx] - graph.node_target['event'][idx]))\n",
    "        \n",
    "        accs.append(L1)\n",
    "        #print(\"ACC\", s)\n",
    "\n",
    "        #pred = preds['event'][idx]\n",
    "        \n",
    "        # num_node_types = 0\n",
    "        # micro = 0\n",
    "        # macro = 0\n",
    "        \n",
    "    \n",
    "\n",
    "        # for node_type in preds:\n",
    "        #     idx = index[node_type]\n",
    "        #     pred = preds[node_type][idx]\n",
    "        #     pred = pred.max(1)[1]\n",
    "        #     label_np = graph.node_label[node_type][idx].cpu().numpy()\n",
    "        #     pred_np = pred.cpu().numpy()\n",
    "        #     micro = f1_score(label_np, pred_np, average='micro')\n",
    "        #     macro = f1_score(label_np, pred_np, average='macro')\n",
    "        #     num_node_types += 1\n",
    "        # Averaging f1 score might not make sense, but in our example we only\n",
    "        # have one node type\n",
    "        # micro /= num_node_types\n",
    "        # macro /= num_node_types\n",
    "        #accs.append((micro, macro))\n",
    "    if accs[1] < best_val:\n",
    "        best_val = accs[1]\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    return accs, best_model, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change the following parameters\n",
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 64,\n",
    "    'epochs': 100,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.303,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [1, 1, 1],   # event 0\n",
    "                [2, 2, 2]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([1000, 20], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([0, 0], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVF 2\n",
      "CF 2\n",
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 2\n",
      "\t Feature 2\n",
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 2\n",
      "\t Feature 2\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 2\n",
      "\t Feature 2\n",
      "KEY ('event', 'related', 'concept') <class 'tuple'>\n",
      "KEY NUMS ('event', 'related', 'concept') 8487 8729\n",
      "MAX EDGES tensor(8448) tensor(8728) 8487 8729\n",
      "KEY ('event', 'similar', 'event') <class 'tuple'>\n",
      "KEY NUMS ('event', 'similar', 'event') 8487 8487\n",
      "MAX EDGES tensor(8486) tensor(8486) 8487 8487\n",
      "KEY ('concept', 'related', 'event') <class 'tuple'>\n",
      "KEY NUMS ('concept', 'related', 'event') 8729 8487\n",
      "MAX EDGES tensor(8728) tensor(8448) 8729 8487\n"
     ]
    }
   ],
   "source": [
    "with open(\"./1_concepts_similar.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "    # Convert to directed graph for compatibility with Deepsnap\n",
    "    G = G.to_directed()\n",
    "    \n",
    "hetero_graph = HeteroGraph(G, netlib=nx, directed=True)\n",
    "\n",
    "# Testing\n",
    "# hetero_graph = HeteroGraph(\n",
    "#     node_feature=S_node_feature,\n",
    "#     node_target=S_node_targets,\n",
    "#     edge_index=S_edge_index\n",
    "# )\n",
    "\n",
    "print(\"EVF\", hetero_graph.num_node_features(\"event\"))\n",
    "print(\"CF\", hetero_graph.num_node_features(\"concept\"))\n",
    "\n",
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "\n",
    "# Node feature and node label to device\n",
    "\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "\n",
    "# edge_index1 = hetero_graph.edge_index[(\"concept\", \"related\", \"event\")]\n",
    "# edge_index1 = hetero_graph.edge_index[(\"event\", \"related\", \"concept\")]\n",
    "\n",
    "# Edge_index to sparse tensor and to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key, type(key))\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    \n",
    "    \n",
    "    # TODO: remove quick fix\n",
    "    # if key == ('event', 'related', 'concept'):\n",
    "    #     edge_index = hetero_graph.edge_index[(\"concept\", \"related\", \"event\")]\n",
    "    # else:\n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max(), hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
    "    \n",
    "\n",
    "\n",
    "for key in hetero_graph.node_target:\n",
    "    hetero_graph.node_target[key] = hetero_graph.node_target[key].to(args['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5940])\n",
      "torch.Size([1698])\n",
      "torch.Size([849])\n"
     ]
    }
   ],
   "source": [
    "# train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "# val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "# test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "\n",
    "nEvents = hetero_graph.num_nodes(\"event\")\n",
    "nConcepts = hetero_graph.num_nodes(\"concept\")\n",
    "\n",
    "s1 = 0.7\n",
    "s2 = 0.8\n",
    "\n",
    "train_idx = {   \"event\": torch.tensor(range(0, int(nEvents * s1))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(0, int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "val_idx = {   \"event\": torch.tensor(range(int(nEvents * s1), int(nEvents * s2))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s1), int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "test_idx = {   \"event\": torch.tensor(range(int(nEvents * s2), 8487)).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s2), 8729)).to(args[\"device\"])\n",
    "            }\n",
    "\n",
    "print(train_idx[\"event\"].shape)\n",
    "print(test_idx[\"event\"].shape)\n",
    "print(val_idx[\"event\"].shape)\n",
    "\n",
    "# dataset = deepsnap.dataset.GraphDataset([hetero_graph], task='node')\n",
    "\n",
    "# dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
    "# datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\torch\\nn\\init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2.0034406185150146 Accs [tensor(1.7382e+09, device='cuda:0', grad_fn=<SumBackward0>), tensor(2.4978e+08, device='cuda:0', grad_fn=<SumBackward0>), tensor(4.8195e+08, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 1 Loss 2.1484909057617188 Accs [tensor(3.9940e+08, device='cuda:0', grad_fn=<SumBackward0>), tensor(57445136., device='cuda:0', grad_fn=<SumBackward0>), tensor(1.1130e+08, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 2 Loss 5.593308448791504 Accs [tensor(35800752., device='cuda:0', grad_fn=<SumBackward0>), tensor(5149715., device='cuda:0', grad_fn=<SumBackward0>), tensor(9956296., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 3 Loss 4.512626647949219 Accs [tensor(3031617.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(435486.3125, device='cuda:0', grad_fn=<SumBackward0>), tensor(842612.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 4 Loss 2.3463003635406494 Accs [tensor(1439884.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(206534.7188, device='cuda:0', grad_fn=<SumBackward0>), tensor(399077.0625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 5 Loss 2.3561244010925293 Accs [tensor(1305025.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(186910.7656, device='cuda:0', grad_fn=<SumBackward0>), tensor(362664.3750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 6 Loss 2.6765944957733154 Accs [tensor(817614.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(116783.3594, device='cuda:0', grad_fn=<SumBackward0>), tensor(227699.2812, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 7 Loss 2.3449041843414307 Accs [tensor(494078.5312, device='cuda:0', grad_fn=<SumBackward0>), tensor(70358.8438, device='cuda:0', grad_fn=<SumBackward0>), tensor(137630.6250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 8 Loss 1.777572751045227 Accs [tensor(309906.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(43995.0703, device='cuda:0', grad_fn=<SumBackward0>), tensor(86183.6094, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 9 Loss 1.2666621208190918 Accs [tensor(179845.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(25345.4707, device='cuda:0', grad_fn=<SumBackward0>), tensor(49873.8398, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 10 Loss 1.3709572553634644 Accs [tensor(172837.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(24378.2793, device='cuda:0', grad_fn=<SumBackward0>), tensor(48141.5195, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 11 Loss 1.4123387336730957 Accs [tensor(281947.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(40071.2266, device='cuda:0', grad_fn=<SumBackward0>), tensor(78295.8594, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 12 Loss 1.3830246925354004 Accs [tensor(445931.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(63630.4805, device='cuda:0', grad_fn=<SumBackward0>), tensor(123574.6484, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 13 Loss 1.2682310342788696 Accs [tensor(614554., device='cuda:0', grad_fn=<SumBackward0>), tensor(87882.4531, device='cuda:0', grad_fn=<SumBackward0>), tensor(170277.9062, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 14 Loss 1.1702536344528198 Accs [tensor(650473.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(93050.3359, device='cuda:0', grad_fn=<SumBackward0>), tensor(180282.6250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 15 Loss 1.2393261194229126 Accs [tensor(488932.0938, device='cuda:0', grad_fn=<SumBackward0>), tensor(69834.7656, device='cuda:0', grad_fn=<SumBackward0>), tensor(135570.6875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 16 Loss 1.2342641353607178 Accs [tensor(338973.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(48271.7266, device='cuda:0', grad_fn=<SumBackward0>), tensor(94057.8281, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 17 Loss 1.1935880184173584 Accs [tensor(145055.4688, device='cuda:0', grad_fn=<SumBackward0>), tensor(20374.6777, device='cuda:0', grad_fn=<SumBackward0>), tensor(40683.8008, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 18 Loss 1.212990164756775 Accs [tensor(234958.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(33329.3555, device='cuda:0', grad_fn=<SumBackward0>), tensor(65320.5430, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 19 Loss 1.2256343364715576 Accs [tensor(351049.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(50004.7344, device='cuda:0', grad_fn=<SumBackward0>), tensor(97516.1562, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 20 Loss 1.234776496887207 Accs [tensor(435906.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(62214.6797, device='cuda:0', grad_fn=<SumBackward0>), tensor(121020.2344, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 21 Loss 1.173632025718689 Accs [tensor(428734.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(61191.2930, device='cuda:0', grad_fn=<SumBackward0>), tensor(119027.6328, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 22 Loss 1.2302058935165405 Accs [tensor(404491.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(57710.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(112318.5469, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 23 Loss 1.2581719160079956 Accs [tensor(318188.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(45292.9922, device='cuda:0', grad_fn=<SumBackward0>), tensor(88397.9062, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 24 Loss 1.2311164140701294 Accs [tensor(199161.2188, device='cuda:0', grad_fn=<SumBackward0>), tensor(28159.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(55394.4141, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 25 Loss 1.1767933368682861 Accs [tensor(142492.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(20027.7109, device='cuda:0', grad_fn=<SumBackward0>), tensor(40018.6211, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 26 Loss 1.2267051935195923 Accs [tensor(128125.9219, device='cuda:0', grad_fn=<SumBackward0>), tensor(18077.8730, device='cuda:0', grad_fn=<SumBackward0>), tensor(36088.7812, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 27 Loss 1.2693358659744263 Accs [tensor(144603.2344, device='cuda:0', grad_fn=<SumBackward0>), tensor(20313.0449, device='cuda:0', grad_fn=<SumBackward0>), tensor(40574.3008, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 28 Loss 1.271096110343933 Accs [tensor(151699.2812, device='cuda:0', grad_fn=<SumBackward0>), tensor(21323.8516, device='cuda:0', grad_fn=<SumBackward0>), tensor(42489.0859, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 29 Loss 1.2512069940567017 Accs [tensor(133800.1094, device='cuda:0', grad_fn=<SumBackward0>), tensor(18783.0801, device='cuda:0', grad_fn=<SumBackward0>), tensor(37581.5859, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 30 Loss 1.1712251901626587 Accs [tensor(124543.2656, device='cuda:0', grad_fn=<SumBackward0>), tensor(17487.2324, device='cuda:0', grad_fn=<SumBackward0>), tensor(35022.5000, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 31 Loss 1.2350471019744873 Accs [tensor(128926.1641, device='cuda:0', grad_fn=<SumBackward0>), tensor(18063.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(36185.8125, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 32 Loss 1.2840033769607544 Accs [tensor(140503.4844, device='cuda:0', grad_fn=<SumBackward0>), tensor(19711.1602, device='cuda:0', grad_fn=<SumBackward0>), tensor(39285.8594, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 33 Loss 1.285756230354309 Accs [tensor(125424.5156, device='cuda:0', grad_fn=<SumBackward0>), tensor(17534.1602, device='cuda:0', grad_fn=<SumBackward0>), tensor(35160.2109, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 34 Loss 1.2368468046188354 Accs [tensor(112078.7812, device='cuda:0', grad_fn=<SumBackward0>), tensor(15667.9863, device='cuda:0', grad_fn=<SumBackward0>), tensor(31489.1621, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 35 Loss 1.1518365144729614 Accs [tensor(81590.0781, device='cuda:0', grad_fn=<SumBackward0>), tensor(11628.7734, device='cuda:0', grad_fn=<SumBackward0>), tensor(23127.2852, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 36 Loss 1.2373753786087036 Accs [tensor(119503.7969, device='cuda:0', grad_fn=<SumBackward0>), tensor(16683.8086, device='cuda:0', grad_fn=<SumBackward0>), tensor(33449.1094, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 37 Loss 1.2842317819595337 Accs [tensor(197187.8281, device='cuda:0', grad_fn=<SumBackward0>), tensor(27857.0938, device='cuda:0', grad_fn=<SumBackward0>), tensor(54639.8438, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 38 Loss 1.290107011795044 Accs [tensor(301107.3125, device='cuda:0', grad_fn=<SumBackward0>), tensor(42813.6953, device='cuda:0', grad_fn=<SumBackward0>), tensor(83449.2109, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 39 Loss 1.2564663887023926 Accs [tensor(358735., device='cuda:0', grad_fn=<SumBackward0>), tensor(51110.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(99430.4141, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 40 Loss 1.1986358165740967 Accs [tensor(356287.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(50750.3789, device='cuda:0', grad_fn=<SumBackward0>), tensor(98760.2422, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 41 Loss 1.1982791423797607 Accs [tensor(445453.2812, device='cuda:0', grad_fn=<SumBackward0>), tensor(63571.9492, device='cuda:0', grad_fn=<SumBackward0>), tensor(123474.3281, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 42 Loss 1.2451125383377075 Accs [tensor(428712.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(61161.1484, device='cuda:0', grad_fn=<SumBackward0>), tensor(118841.6875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 43 Loss 1.2463613748550415 Accs [tensor(405027.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(57754.1719, device='cuda:0', grad_fn=<SumBackward0>), tensor(112294.8281, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 44 Loss 1.20045006275177 Accs [tensor(332170.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(47290.2852, device='cuda:0', grad_fn=<SumBackward0>), tensor(92128.3359, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 45 Loss 1.1744400262832642 Accs [tensor(360020.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(51297.7578, device='cuda:0', grad_fn=<SumBackward0>), tensor(99858.6875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 46 Loss 1.2092610597610474 Accs [tensor(397554.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(56689.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(110267.1016, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 47 Loss 1.2029190063476562 Accs [tensor(446768.3438, device='cuda:0', grad_fn=<SumBackward0>), tensor(63762.6680, device='cuda:0', grad_fn=<SumBackward0>), tensor(123901.3438, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 48 Loss 1.1643940210342407 Accs [tensor(387781.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(55279.3125, device='cuda:0', grad_fn=<SumBackward0>), tensor(107510.8359, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 49 Loss 1.2166997194290161 Accs [tensor(354433.1562, device='cuda:0', grad_fn=<SumBackward0>), tensor(50478.8633, device='cuda:0', grad_fn=<SumBackward0>), tensor(98275.4375, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 50 Loss 1.2386391162872314 Accs [tensor(267555.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(37981.8594, device='cuda:0', grad_fn=<SumBackward0>), tensor(74208.5938, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 51 Loss 1.218951940536499 Accs [tensor(225497.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(31936.5898, device='cuda:0', grad_fn=<SumBackward0>), tensor(62566.8281, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 52 Loss 1.1602259874343872 Accs [tensor(245661.6406, device='cuda:0', grad_fn=<SumBackward0>), tensor(34841.4375, device='cuda:0', grad_fn=<SumBackward0>), tensor(68170.7422, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 53 Loss 1.2214423418045044 Accs [tensor(359974., device='cuda:0', grad_fn=<SumBackward0>), tensor(51279.3203, device='cuda:0', grad_fn=<SumBackward0>), tensor(99854.1250, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 54 Loss 1.2663546800613403 Accs [tensor(417052.8125, device='cuda:0', grad_fn=<SumBackward0>), tensor(59489.2227, device='cuda:0', grad_fn=<SumBackward0>), tensor(115699.2188, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 55 Loss 1.2678099870681763 Accs [tensor(421321.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(60101.3711, device='cuda:0', grad_fn=<SumBackward0>), tensor(116900.6328, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 56 Loss 1.240769624710083 Accs [tensor(358619.5938, device='cuda:0', grad_fn=<SumBackward0>), tensor(51089.1094, device='cuda:0', grad_fn=<SumBackward0>), tensor(99502.1719, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 57 Loss 1.1766102313995361 Accs [tensor(299306.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(42554.9531, device='cuda:0', grad_fn=<SumBackward0>), tensor(83038.2344, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 58 Loss 1.2153785228729248 Accs [tensor(236144.7812, device='cuda:0', grad_fn=<SumBackward0>), tensor(33460.8828, device='cuda:0', grad_fn=<SumBackward0>), tensor(65556.6094, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 59 Loss 1.2577632665634155 Accs [tensor(198640.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(28076.2969, device='cuda:0', grad_fn=<SumBackward0>), tensor(55220.8281, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 60 Loss 1.201331377029419 Accs [tensor(181070.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(25564.4082, device='cuda:0', grad_fn=<SumBackward0>), tensor(50462.6289, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 61 Loss 1.083572268486023 Accs [tensor(117014.5781, device='cuda:0', grad_fn=<SumBackward0>), tensor(16340.6816, device='cuda:0', grad_fn=<SumBackward0>), tensor(32819.2109, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 62 Loss 1.0152252912521362 Accs [tensor(140022.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(19651.0332, device='cuda:0', grad_fn=<SumBackward0>), tensor(39071.8477, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 63 Loss 1.0103018283843994 Accs [tensor(239742., device='cuda:0', grad_fn=<SumBackward0>), tensor(33969.4102, device='cuda:0', grad_fn=<SumBackward0>), tensor(66738.6094, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 64 Loss 0.985836386680603 Accs [tensor(481368.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(68667.0391, device='cuda:0', grad_fn=<SumBackward0>), tensor(134408.9688, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 65 Loss 1.0272393226623535 Accs [tensor(450955.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(64296.9922, device='cuda:0', grad_fn=<SumBackward0>), tensor(126076.9453, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 66 Loss 0.9730784893035889 Accs [tensor(469330.7812, device='cuda:0', grad_fn=<SumBackward0>), tensor(66940.4531, device='cuda:0', grad_fn=<SumBackward0>), tensor(131259.6719, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 67 Loss 0.9924141764640808 Accs [tensor(466297.2188, device='cuda:0', grad_fn=<SumBackward0>), tensor(66488.5312, device='cuda:0', grad_fn=<SumBackward0>), tensor(130685.3438, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 68 Loss 0.9645223021507263 Accs [tensor(553701.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(79085.5000, device='cuda:0', grad_fn=<SumBackward0>), tensor(154692.5625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 69 Loss 0.9797782897949219 Accs [tensor(553361.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(79054.9766, device='cuda:0', grad_fn=<SumBackward0>), tensor(154399.2188, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 70 Loss 0.9727402925491333 Accs [tensor(549356.3125, device='cuda:0', grad_fn=<SumBackward0>), tensor(78471.0469, device='cuda:0', grad_fn=<SumBackward0>), tensor(153416.9375, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 71 Loss 0.9916435480117798 Accs [tensor(567549.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(81115.9844, device='cuda:0', grad_fn=<SumBackward0>), tensor(158240.9688, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 72 Loss 0.9908396005630493 Accs [tensor(552467.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(78961.4844, device='cuda:0', grad_fn=<SumBackward0>), tensor(153925., device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 73 Loss 0.9668123126029968 Accs [tensor(603561.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(86318.5625, device='cuda:0', grad_fn=<SumBackward0>), tensor(168099.2656, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 74 Loss 1.002099633216858 Accs [tensor(456222.2188, device='cuda:0', grad_fn=<SumBackward0>), tensor(65138.3906, device='cuda:0', grad_fn=<SumBackward0>), tensor(126992.9844, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 75 Loss 0.9824228882789612 Accs [tensor(491445.7188, device='cuda:0', grad_fn=<SumBackward0>), tensor(70215.0547, device='cuda:0', grad_fn=<SumBackward0>), tensor(136753.0625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 76 Loss 0.9815442562103271 Accs [tensor(530731.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(75866.9844, device='cuda:0', grad_fn=<SumBackward0>), tensor(147640.8594, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 77 Loss 0.9993069171905518 Accs [tensor(421914., device='cuda:0', grad_fn=<SumBackward0>), tensor(60219.4492, device='cuda:0', grad_fn=<SumBackward0>), tensor(117326.5156, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 78 Loss 0.9680671691894531 Accs [tensor(349438., device='cuda:0', grad_fn=<SumBackward0>), tensor(49788.6680, device='cuda:0', grad_fn=<SumBackward0>), tensor(97122.6875, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 79 Loss 0.9737609028816223 Accs [tensor(287307., device='cuda:0', grad_fn=<SumBackward0>), tensor(40864.0625, device='cuda:0', grad_fn=<SumBackward0>), tensor(79859.5469, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 80 Loss 0.9598591327667236 Accs [tensor(261205.2656, device='cuda:0', grad_fn=<SumBackward0>), tensor(37109.0430, device='cuda:0', grad_fn=<SumBackward0>), tensor(72601.7031, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 81 Loss 0.9731848835945129 Accs [tensor(321183.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(45727.1875, device='cuda:0', grad_fn=<SumBackward0>), tensor(89189.3594, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 82 Loss 0.9707399010658264 Accs [tensor(325160.7500, device='cuda:0', grad_fn=<SumBackward0>), tensor(46308.8047, device='cuda:0', grad_fn=<SumBackward0>), tensor(90254.9375, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 83 Loss 0.9583114385604858 Accs [tensor(336842.8750, device='cuda:0', grad_fn=<SumBackward0>), tensor(47982.1133, device='cuda:0', grad_fn=<SumBackward0>), tensor(93477.3281, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 84 Loss 0.9680591821670532 Accs [tensor(355732.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(50687.6445, device='cuda:0', grad_fn=<SumBackward0>), tensor(98698.8438, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 85 Loss 0.9683859348297119 Accs [tensor(329429.6250, device='cuda:0', grad_fn=<SumBackward0>), tensor(46910.2109, device='cuda:0', grad_fn=<SumBackward0>), tensor(91400.7188, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 86 Loss 0.9699419140815735 Accs [tensor(358483.2188, device='cuda:0', grad_fn=<SumBackward0>), tensor(51073.9453, device='cuda:0', grad_fn=<SumBackward0>), tensor(99445.1562, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 87 Loss 0.9787672162055969 Accs [tensor(291648.2500, device='cuda:0', grad_fn=<SumBackward0>), tensor(41468.6484, device='cuda:0', grad_fn=<SumBackward0>), tensor(80922.9453, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 88 Loss 0.9599703550338745 Accs [tensor(203302.1406, device='cuda:0', grad_fn=<SumBackward0>), tensor(28759.0430, device='cuda:0', grad_fn=<SumBackward0>), tensor(56437.2734, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 89 Loss 0.9747725129127502 Accs [tensor(163733.3438, device='cuda:0', grad_fn=<SumBackward0>), tensor(23051.0312, device='cuda:0', grad_fn=<SumBackward0>), tensor(45540.0703, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 90 Loss 0.9595939517021179 Accs [tensor(168583.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(23752.9414, device='cuda:0', grad_fn=<SumBackward0>), tensor(46837.1641, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 91 Loss 0.993992030620575 Accs [tensor(208125.9375, device='cuda:0', grad_fn=<SumBackward0>), tensor(29431.2773, device='cuda:0', grad_fn=<SumBackward0>), tensor(57763.8281, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 92 Loss 0.9943444132804871 Accs [tensor(260466.2969, device='cuda:0', grad_fn=<SumBackward0>), tensor(36967.2969, device='cuda:0', grad_fn=<SumBackward0>), tensor(72268.5625, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 93 Loss 0.9669141173362732 Accs [tensor(250269.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(35498.5859, device='cuda:0', grad_fn=<SumBackward0>), tensor(69457.3750, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 94 Loss 0.9565680027008057 Accs [tensor(170183.5312, device='cuda:0', grad_fn=<SumBackward0>), tensor(23985.6953, device='cuda:0', grad_fn=<SumBackward0>), tensor(47381.1562, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 95 Loss 0.981738269329071 Accs [tensor(72235.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(10496.6543, device='cuda:0', grad_fn=<SumBackward0>), tensor(20645.1797, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 96 Loss 0.982387900352478 Accs [tensor(198246.8281, device='cuda:0', grad_fn=<SumBackward0>), tensor(28005.1973, device='cuda:0', grad_fn=<SumBackward0>), tensor(55055.0195, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 97 Loss 1.0139386653900146 Accs [tensor(224526.1250, device='cuda:0', grad_fn=<SumBackward0>), tensor(31786.6016, device='cuda:0', grad_fn=<SumBackward0>), tensor(62321.3203, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 98 Loss 1.0042321681976318 Accs [tensor(232399.8906, device='cuda:0', grad_fn=<SumBackward0>), tensor(32932.6797, device='cuda:0', grad_fn=<SumBackward0>), tensor(64488.8594, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Epoch 99 Loss 0.9781503081321716 Accs [tensor(203160., device='cuda:0', grad_fn=<SumBackward0>), tensor(28729.5508, device='cuda:0', grad_fn=<SumBackward0>), tensor(56445.0547, device='cuda:0', grad_fn=<SumBackward0>)]\n",
      "Best accs [tensor(72235.9062, device='cuda:0', grad_fn=<SumBackward0>), tensor(10496.6543, device='cuda:0', grad_fn=<SumBackward0>), tensor(20645.1797, device='cuda:0', grad_fn=<SumBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n",
    "    # print(\n",
    "    #     f\"Epoch {epoch + 1}: loss {round(loss, 5)}, \"\n",
    "    #     f\"train micro {round(accs[0][0] * 100, 2)}%, train macro {round(accs[0][1] * 100, 2)}%, \"\n",
    "    #     f\"valid micro {round(accs[1][0] * 100, 2)}%, valid macro {round(accs[1][1] * 100, 2)}%, \"\n",
    "    #     f\"test micro {round(accs[2][0] * 100, 2)}%, test macro {round(accs[2][1] * 100, 2)}%\"\n",
    "    # )\n",
    "    print(f\"Epoch {epoch} Loss {loss} Accs {accs}\")\n",
    "\n",
    "best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx])\n",
    "\n",
    "print(\"Best accs\", best_accs)\n",
    "\n",
    "\n",
    "\n",
    "# print(\n",
    "#     f\"Best model: \"\n",
    "#     f\"train micro {round(best_accs[0][0] * 100, 2)}%, train macro {round(best_accs[0][1] * 100, 2)}%, \"\n",
    "#     f\"valid micro {round(best_accs[1][0] * 100, 2)}%, valid macro {round(best_accs[1][1] * 100, 2)}%, \"\n",
    "#     f\"test micro {round(best_accs[2][0] * 100, 2)}%, test macro {round(best_accs[2][1] * 100, 2)}%\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best_model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "mask = preds['event'] > 0\n",
    "preds['event'][preds['event'] > 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
