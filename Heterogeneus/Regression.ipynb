{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # To simplify implementation, please initialize both self.lin_dst\n",
    "        # and self.lin_src out_features to out_channels\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "\n",
    "        self.lin_dst = nn.Linear(self.in_channels_src, self.out_channels)\n",
    "        self.lin_src = nn.Linear(self.in_channels_dst, self.out_channels)\n",
    "        self.lin_update = nn.Linear(2*self.out_channels, self.out_channels)\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "    ):\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "        ##########################################\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "        ## Note:\n",
    "        ## 1. Different from what we implemented in Colab 3, we use message_and_aggregate\n",
    "        ## to replace the message and aggregate. The benefit is that we can avoid\n",
    "        ## materializing x_i and x_j, and make the implementation more efficient.\n",
    "        ## 2. To implement efficiently, following PyG documentation is helpful:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html\n",
    "        ## 3. Here edge_index is torch_sparse SparseTensor.\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "        ##########################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~4 lines of code)\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        # print(aggr_out.shape, dst_out.shape)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        # print(aggr_out.shape, )\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "        ##########################################\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "            ############# Your code here #############\n",
    "            ## (~1 line of code)\n",
    "            ## Note:\n",
    "            ## 1. Initialize self.attn_proj here.\n",
    "            ## 2. You should use nn.Sequential for self.attn_proj\n",
    "            ## 3. nn.Linear and nn.Tanh are useful.\n",
    "            ## 4. You can create a vector parameter by using:\n",
    "            ## nn.Linear(some_size, 1, bias=False)\n",
    "            ## 5. The first linear layer should have out_features as args['attn_size']\n",
    "            ## 6. You can assume we only have one \"head\" for the attention.\n",
    "            ## 7. We recommend you to implement the mean aggregation first. After \n",
    "            ## the mean aggregation works well in the training, then you can \n",
    "            ## implement this part.\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "            #########################################\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        # TODO: Implement this function that aggregates all message type results.\n",
    "        # Here, xs is a list of tensors (embeddings) with respect to message \n",
    "        # type aggregation results.\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~2 lines of code)\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "            ##########################################\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~10 lines of code)\n",
    "            ## Note:\n",
    "            ## 1. Store the value of attention alpha (as a numpy array) to self.alpha,\n",
    "            ## which has the shape (len(xs), ) self.alpha will be not be used \n",
    "            ## to backpropagate etc. in the model. We will use it to see how much \n",
    "            ## attention the layer pays on different message types.\n",
    "            ## 2. torch.softmax and torch.cat are useful.\n",
    "            ## 3. You might need to reshape the tensors by using the \n",
    "            ## `view()` function https://pytorch.org/docs/stable/tensor_view.html\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out\n",
    "            ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    # TODO: Implement this function that returns a dictionary of `HeteroGNNConv` \n",
    "    # layers where the keys are message types. `hetero_graph` is deepsnap `HeteroGraph`\n",
    "    # object and the `conv` is the `HeteroGNNConv`.\n",
    "\n",
    "    convs = {}\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## (~9 lines of code)\n",
    "\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    ##########################################\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        self.convs1 = None\n",
    "        self.convs2 = None\n",
    "\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~10 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For self.convs1 and self.convs2, call generate_convs at first and then\n",
    "        ## pass the returned dictionary of `HeteroGNNConv` to `HeteroGNNWrapperConv`.\n",
    "        ## 2. For self.bns, self.relus and self.post_mps, the keys are node_types.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.node_types` will be helpful.\n",
    "        ## 3. Initialize all batchnorms to torch.nn.BatchNorm1d(hidden_size, eps=1.0).\n",
    "        ## 4. Initialize all relus to nn.LeakyReLU().\n",
    "        ## 5. For self.post_mps, each value in the ModuleDict is a linear layer \n",
    "        ## where the `out_features` is the number of classes for that node type.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.num_node_labels(node_type)` will be\n",
    "        ## useful.\n",
    "\n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            self.post_mps[node_type] = nn.Linear(self.hidden_size, hetero_graph.num_node_labels(node_type))\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(self, node_feature, edge_index):\n",
    "        # TODO: Implement the forward function. Notice that `node_feature` is \n",
    "        # a dictionary of tensors where keys are node types and values are \n",
    "        # corresponding feature tensors. The `edge_index` is a dictionary of \n",
    "        # tensors where keys are message types and values are corresponding\n",
    "        # edge index tensors (with respect to each message type).\n",
    "\n",
    "        x = node_feature\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~7 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. `deepsnap.hetero_gnn.forward_op` can be helpful.\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        # TODO: remove this for regression tasks\n",
    "        # x = forward_op(x, self.post_mps)\n",
    "        x = forward_op(x, self.fc)\n",
    "        #print(\"X\", x)\n",
    "\n",
    "        ##########################################\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "\n",
    "        loss = 0\n",
    "        loss_func = torch.nn.L1Loss()\n",
    "\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For each node type in preds, accumulate computed loss to `loss`\n",
    "        ## 2. Loss need to be computed with respect to the given index\n",
    "\n",
    "        for node_type in preds:\n",
    "            idx = indices[node_type]\n",
    "            loss += loss_func(preds[node_type][idx], y[node_type][idx])\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    loss = None\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## Note:\n",
    "    ## 1. `deepsnap.hetero_graph.HeteroGraph.node_label` is useful\n",
    "    ## 2. Compute the loss here\n",
    "    \n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "    ##########################################\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(model, graph, indices, best_model=None, best_val=0):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        num_node_types = 0\n",
    "        micro = 0\n",
    "        macro = 0\n",
    "        for node_type in preds:\n",
    "            idx = index[node_type]\n",
    "            pred = preds[node_type][idx]\n",
    "            pred = pred.max(1)[1]\n",
    "            label_np = graph.node_label[node_type][idx].cpu().numpy()\n",
    "            pred_np = pred.cpu().numpy()\n",
    "            micro = f1_score(label_np, pred_np, average='micro')\n",
    "            macro = f1_score(label_np, pred_np, average='macro')\n",
    "            num_node_types += 1\n",
    "        # Averaging f1 score might not make sense, but in our example we only\n",
    "        # have one node type\n",
    "        micro /= num_node_types\n",
    "        macro /= num_node_types\n",
    "        accs.append((micro, macro))\n",
    "    if accs[1][0] > best_val:\n",
    "        best_val = accs[1][0]\n",
    "        best_model = copy.deepcopy(model)\n",
    "    return accs, best_model, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change the following parameters\n",
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 64,\n",
    "    'epochs': 100,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.303,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [0, 0, 0],   # event 0\n",
    "                [1, 1, 1]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([1000, 20], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([0, 0], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 3\n",
      "\t Feature 3\n",
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 3\n",
      "\t Feature 3\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 3\n",
      "\t Feature 3\n",
      "KEY ('event', 'similar', 'event')\n",
      "KEY NUMS ('event', 'similar', 'event') 2 2\n",
      "MAX EDGES tensor(1) tensor(1)\n",
      "KEY ('event', 'related', 'concept')\n",
      "KEY NUMS ('event', 'related', 'concept') 2 2\n",
      "MAX EDGES tensor(1) tensor(1)\n",
      "KEY ('concept', 'related', 'event')\n",
      "KEY NUMS ('concept', 'related', 'event') 2 2\n",
      "MAX EDGES tensor(1) tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# with open(\"./1_concepts_similar.pkl\", \"rb\") as f:\n",
    "#     G = pickle.load(f)\n",
    "#hetero_graph = HeteroGraph(G, netlib=nx, directed=False)\n",
    "\n",
    "hetero_graph = HeteroGraph(\n",
    "    node_feature=S_node_feature,\n",
    "    node_target=S_node_targets,\n",
    "    edge_index=S_edge_index\n",
    ")\n",
    "\n",
    "# # Construct a deepsnap tensor backend HeteroGraph\n",
    "# # hetero_graph = HeteroGraph(\n",
    "# #     node_feature=node_feature,\n",
    "# #     node_label=node_label,\n",
    "# #     edge_index=edge_index,\n",
    "# #     directed=True\n",
    "# # )\n",
    "\n",
    "# hetero_graph = HeteroGraph(G)\n",
    "\n",
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "\n",
    "# Node feature and node label to device\n",
    "\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "\n",
    "# Edge_index to sparse tensor and to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key)\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max())\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "/usr/local/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 500081.09375\n",
      "Epoch 1 Loss 490550.9375\n",
      "Epoch 2 Loss 476743.625\n",
      "Epoch 3 Loss 454489.375\n",
      "Epoch 4 Loss 424473.0625\n",
      "Epoch 5 Loss 388255.5625\n",
      "Epoch 6 Loss 348348.125\n",
      "Epoch 7 Loss 308399.78125\n",
      "Epoch 8 Loss 273377.6875\n",
      "Epoch 9 Loss 249449.15625\n",
      "Epoch 10 Loss 242565.984375\n",
      "Epoch 11 Loss 253589.953125\n",
      "Epoch 12 Loss 272647.0\n",
      "Epoch 13 Loss 285726.59375\n",
      "Epoch 14 Loss 286374.0625\n",
      "Epoch 15 Loss 277040.0\n",
      "Epoch 16 Loss 263752.71875\n",
      "Epoch 17 Loss 251833.734375\n",
      "Epoch 18 Loss 244269.140625\n",
      "Epoch 19 Loss 241674.1875\n",
      "Epoch 20 Loss 243010.625\n",
      "Epoch 21 Loss 246508.5625\n",
      "Epoch 22 Loss 250419.328125\n",
      "Epoch 23 Loss 253442.625\n",
      "Epoch 24 Loss 254859.1875\n",
      "Epoch 25 Loss 254487.453125\n",
      "Epoch 26 Loss 252569.515625\n",
      "Epoch 27 Loss 249642.578125\n",
      "Epoch 28 Loss 246405.0625\n",
      "Epoch 29 Loss 243568.484375\n",
      "Epoch 30 Loss 241693.921875\n",
      "Epoch 31 Loss 241040.3125\n",
      "Epoch 32 Loss 241484.640625\n",
      "Epoch 33 Loss 242574.203125\n",
      "Epoch 34 Loss 243711.765625\n",
      "Epoch 35 Loss 244392.34375\n",
      "Epoch 36 Loss 244374.78125\n",
      "Epoch 37 Loss 243722.171875\n",
      "Epoch 38 Loss 242716.890625\n",
      "Epoch 39 Loss 241711.296875\n",
      "Epoch 40 Loss 240985.875\n",
      "Epoch 41 Loss 240667.90625\n",
      "Epoch 42 Loss 240724.046875\n",
      "Epoch 43 Loss 241008.90625\n",
      "Epoch 44 Loss 241338.03125\n",
      "Epoch 45 Loss 241555.625\n",
      "Epoch 46 Loss 241578.203125\n",
      "Epoch 47 Loss 241406.953125\n",
      "Epoch 48 Loss 241110.875\n",
      "Epoch 49 Loss 240791.25\n",
      "Epoch 50 Loss 240540.0\n",
      "Epoch 51 Loss 240408.0\n",
      "Epoch 52 Loss 240393.421875\n",
      "Epoch 53 Loss 240453.203125\n",
      "Epoch 54 Loss 240528.6875\n",
      "Epoch 55 Loss 240572.0625\n",
      "Epoch 56 Loss 240561.515625\n",
      "Epoch 57 Loss 240502.4375\n",
      "Epoch 58 Loss 240418.0\n",
      "Epoch 59 Loss 240335.875\n",
      "Epoch 60 Loss 240276.90625\n",
      "Epoch 61 Loss 240248.96875\n",
      "Epoch 62 Loss 240246.875\n",
      "Epoch 63 Loss 240257.265625\n",
      "Epoch 64 Loss 240265.828125\n",
      "Epoch 65 Loss 240263.375\n",
      "Epoch 66 Loss 240248.578125\n",
      "Epoch 67 Loss 240226.453125\n",
      "Epoch 68 Loss 240204.21875\n",
      "Epoch 69 Loss 240187.09375\n",
      "Epoch 70 Loss 240176.265625\n",
      "Epoch 71 Loss 240169.640625\n",
      "Epoch 72 Loss 240164.296875\n",
      "Epoch 73 Loss 240158.6875\n",
      "Epoch 74 Loss 240153.03125\n",
      "Epoch 75 Loss 240148.359375\n",
      "Epoch 76 Loss 240145.1875\n",
      "Epoch 77 Loss 240143.09375\n",
      "Epoch 78 Loss 240140.78125\n",
      "Epoch 79 Loss 240137.234375\n",
      "Epoch 80 Loss 240132.15625\n",
      "Epoch 81 Loss 240126.25\n",
      "Epoch 82 Loss 240120.90625\n",
      "Epoch 83 Loss 240117.375\n",
      "Epoch 84 Loss 240116.3125\n",
      "Epoch 85 Loss 240117.140625\n",
      "Epoch 86 Loss 240118.5625\n",
      "Epoch 87 Loss 240118.921875\n",
      "Epoch 88 Loss 240117.25\n",
      "Epoch 89 Loss 240113.71875\n",
      "Epoch 90 Loss 240109.546875\n",
      "Epoch 91 Loss 240106.3125\n",
      "Epoch 92 Loss 240105.09375\n",
      "Epoch 93 Loss 240105.84375\n",
      "Epoch 94 Loss 240107.5625\n",
      "Epoch 95 Loss 240108.859375\n",
      "Epoch 96 Loss 240108.765625\n",
      "Epoch 97 Loss 240107.15625\n",
      "Epoch 98 Loss 240104.78125\n",
      "Epoch 99 Loss 240102.65625\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_val = 0\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    #accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n",
    "    # print(\n",
    "    #     f\"Epoch {epoch + 1}: loss {round(loss, 5)}, \"\n",
    "    #     f\"train micro {round(accs[0][0] * 100, 2)}%, train macro {round(accs[0][1] * 100, 2)}%, \"\n",
    "    #     f\"valid micro {round(accs[1][0] * 100, 2)}%, valid macro {round(accs[1][1] * 100, 2)}%, \"\n",
    "    #     f\"test micro {round(accs[2][0] * 100, 2)}%, test macro {round(accs[2][1] * 100, 2)}%\"\n",
    "    # )\n",
    "    print(f\"Epoch {epoch} Loss {loss}\")\n",
    "\n",
    "# best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx])\n",
    "# print(\n",
    "#     f\"Best model: \"\n",
    "#     f\"train micro {round(best_accs[0][0] * 100, 2)}%, train macro {round(best_accs[0][1] * 100, 2)}%, \"\n",
    "#     f\"valid micro {round(best_accs[1][0] * 100, 2)}%, valid macro {round(best_accs[1][1] * 100, 2)}%, \"\n",
    "#     f\"test micro {round(best_accs[2][0] * 100, 2)}%, test macro {round(best_accs[2][1] * 100, 2)}%\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
