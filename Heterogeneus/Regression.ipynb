{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # To simplify implementation, please initialize both self.lin_dst\n",
    "        # and self.lin_src out_features to out_channels\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "\n",
    "        self.lin_dst = nn.Linear(self.in_channels_src, self.out_channels)\n",
    "        self.lin_src = nn.Linear(self.in_channels_dst, self.out_channels)\n",
    "        self.lin_update = nn.Linear(2*self.out_channels, self.out_channels)\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "    ):\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "        ##########################################\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "        ## Note:\n",
    "        ## 1. Different from what we implemented in Colab 3, we use message_and_aggregate\n",
    "        ## to replace the message and aggregate. The benefit is that we can avoid\n",
    "        ## materializing x_i and x_j, and make the implementation more efficient.\n",
    "        ## 2. To implement efficiently, following PyG documentation is helpful:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html\n",
    "        ## 3. Here edge_index is torch_sparse SparseTensor.\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "        ##########################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~4 lines of code)\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        # print(aggr_out.shape, dst_out.shape)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        # print(aggr_out.shape, )\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "        ##########################################\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "            ############# Your code here #############\n",
    "            ## (~1 line of code)\n",
    "            ## Note:\n",
    "            ## 1. Initialize self.attn_proj here.\n",
    "            ## 2. You should use nn.Sequential for self.attn_proj\n",
    "            ## 3. nn.Linear and nn.Tanh are useful.\n",
    "            ## 4. You can create a vector parameter by using:\n",
    "            ## nn.Linear(some_size, 1, bias=False)\n",
    "            ## 5. The first linear layer should have out_features as args['attn_size']\n",
    "            ## 6. You can assume we only have one \"head\" for the attention.\n",
    "            ## 7. We recommend you to implement the mean aggregation first. After \n",
    "            ## the mean aggregation works well in the training, then you can \n",
    "            ## implement this part.\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "            #########################################\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        # TODO: Implement this function that aggregates all message type results.\n",
    "        # Here, xs is a list of tensors (embeddings) with respect to message \n",
    "        # type aggregation results.\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~2 lines of code)\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "            ##########################################\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~10 lines of code)\n",
    "            ## Note:\n",
    "            ## 1. Store the value of attention alpha (as a numpy array) to self.alpha,\n",
    "            ## which has the shape (len(xs), ) self.alpha will be not be used \n",
    "            ## to backpropagate etc. in the model. We will use it to see how much \n",
    "            ## attention the layer pays on different message types.\n",
    "            ## 2. torch.softmax and torch.cat are useful.\n",
    "            ## 3. You might need to reshape the tensors by using the \n",
    "            ## `view()` function https://pytorch.org/docs/stable/tensor_view.html\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out\n",
    "            ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    # TODO: Implement this function that returns a dictionary of `HeteroGNNConv` \n",
    "    # layers where the keys are message types. `hetero_graph` is deepsnap `HeteroGraph`\n",
    "    # object and the `conv` is the `HeteroGNNConv`.\n",
    "\n",
    "    convs = {}\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## (~9 lines of code)\n",
    "\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    ##########################################\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        self.convs1 = None\n",
    "        self.convs2 = None\n",
    "\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~10 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For self.convs1 and self.convs2, call generate_convs at first and then\n",
    "        ## pass the returned dictionary of `HeteroGNNConv` to `HeteroGNNWrapperConv`.\n",
    "        ## 2. For self.bns, self.relus and self.post_mps, the keys are node_types.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.node_types` will be helpful.\n",
    "        ## 3. Initialize all batchnorms to torch.nn.BatchNorm1d(hidden_size, eps=1.0).\n",
    "        ## 4. Initialize all relus to nn.LeakyReLU().\n",
    "        ## 5. For self.post_mps, each value in the ModuleDict is a linear layer \n",
    "        ## where the `out_features` is the number of classes for that node type.\n",
    "        ## `deepsnap.hetero_graph.HeteroGraph.num_node_labels(node_type)` will be\n",
    "        ## useful.\n",
    "\n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            self.post_mps[node_type] = nn.Linear(self.hidden_size, hetero_graph.num_node_labels(node_type))\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(self, node_feature, edge_index):\n",
    "        # TODO: Implement the forward function. Notice that `node_feature` is \n",
    "        # a dictionary of tensors where keys are node types and values are \n",
    "        # corresponding feature tensors. The `edge_index` is a dictionary of \n",
    "        # tensors where keys are message types and values are corresponding\n",
    "        # edge index tensors (with respect to each message type).\n",
    "\n",
    "        x = node_feature\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~7 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. `deepsnap.hetero_gnn.forward_op` can be helpful.\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        # TODO: remove this for regression tasks\n",
    "        # x = forward_op(x, self.post_mps)\n",
    "        x = forward_op(x, self.fc)\n",
    "        #print(\"X\", x)\n",
    "\n",
    "        ##########################################\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        loss = 0\n",
    "        loss_func = torch.nn.L1Loss()\n",
    "\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~3 lines of code)\n",
    "        ## Note:\n",
    "        ## 1. For each node type in preds, accumulate computed loss to `loss`\n",
    "        ## 2. Loss need to be computed with respect to the given index\n",
    "\n",
    "        #for node_type in preds:\n",
    "        mask = y['event'] > 0 # only select nodes with non-negative targets\n",
    "\n",
    "        idx = torch.masked_select(indices['event'], mask)\n",
    "        loss += loss_func(preds['event'][idx], y['event'][idx])\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    loss = None\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## Note:\n",
    "    ## 1. `deepsnap.hetero_graph.HeteroGraph.node_label` is useful\n",
    "    ## 2. Compute the loss here\n",
    "    \n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "    ##########################################\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(model, graph, indices, best_model=None, best_val=0):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        \n",
    "        #print(\"Index\", index)\n",
    "        #print(\"Preds\", preds['event'])\n",
    "\n",
    "        idx = index['event']\n",
    "\n",
    "        L1 = torch.sum(torch.abs(preds['event'][idx] - graph.node_target['event'][idx]))\n",
    "        \n",
    "        accs.append(L1)\n",
    "        #print(\"ACC\", s)\n",
    "\n",
    "        #pred = preds['event'][idx]\n",
    "        \n",
    "        # num_node_types = 0\n",
    "        # micro = 0\n",
    "        # macro = 0\n",
    "        \n",
    "    \n",
    "\n",
    "        # for node_type in preds:\n",
    "        #     idx = index[node_type]\n",
    "        #     pred = preds[node_type][idx]\n",
    "        #     pred = pred.max(1)[1]\n",
    "        #     label_np = graph.node_label[node_type][idx].cpu().numpy()\n",
    "        #     pred_np = pred.cpu().numpy()\n",
    "        #     micro = f1_score(label_np, pred_np, average='micro')\n",
    "        #     macro = f1_score(label_np, pred_np, average='macro')\n",
    "        #     num_node_types += 1\n",
    "        # Averaging f1 score might not make sense, but in our example we only\n",
    "        # have one node type\n",
    "        # micro /= num_node_types\n",
    "        # macro /= num_node_types\n",
    "        #accs.append((micro, macro))\n",
    "    if accs[1] < best_val:\n",
    "        best_val = accs[1]\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    return accs, best_model, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change the following parameters\n",
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 64,\n",
    "    'epochs': 100,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.303,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [1, 1, 1],   # event 0\n",
    "                [2, 2, 2]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([1000, 20], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([0, 0], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVF 2\n",
      "CF 2\n",
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 2\n",
      "\t Feature 2\n",
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 2\n",
      "\t Feature 2\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 2\n",
      "\t Feature 2\n",
      "KEY ('event', 'related', 'concept') <class 'tuple'>\n",
      "KEY NUMS ('event', 'related', 'concept') 8487 8729\n",
      "MAX EDGES tensor(8448) tensor(8728) 8487 8729\n",
      "KEY ('event', 'similar', 'event') <class 'tuple'>\n",
      "KEY NUMS ('event', 'similar', 'event') 8487 8487\n",
      "MAX EDGES tensor(8486) tensor(8486) 8487 8487\n",
      "KEY ('concept', 'related', 'event') <class 'tuple'>\n",
      "KEY NUMS ('concept', 'related', 'event') 8729 8487\n",
      "MAX EDGES tensor(8728) tensor(8448) 8729 8487\n"
     ]
    }
   ],
   "source": [
    "with open(\"./1_concepts_similar.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "    # Convert to directed graph for compatibility with Deepsnap\n",
    "    G = G.to_directed()\n",
    "    \n",
    "hetero_graph = HeteroGraph(G, netlib=nx, directed=True)\n",
    "\n",
    "# Testing\n",
    "# hetero_graph = HeteroGraph(\n",
    "#     node_feature=S_node_feature,\n",
    "#     node_target=S_node_targets,\n",
    "#     edge_index=S_edge_index\n",
    "# )\n",
    "\n",
    "print(\"EVF\", hetero_graph.num_node_features(\"event\"))\n",
    "print(\"CF\", hetero_graph.num_node_features(\"concept\"))\n",
    "\n",
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "\n",
    "# Node feature and node label to device\n",
    "\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "\n",
    "# edge_index1 = hetero_graph.edge_index[(\"concept\", \"related\", \"event\")]\n",
    "# edge_index1 = hetero_graph.edge_index[(\"event\", \"related\", \"concept\")]\n",
    "\n",
    "# Edge_index to sparse tensor and to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key, type(key))\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    \n",
    "    \n",
    "    # TODO: remove quick fix\n",
    "    # if key == ('event', 'related', 'concept'):\n",
    "    #     edge_index = hetero_graph.edge_index[(\"concept\", \"related\", \"event\")]\n",
    "    # else:\n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max(), hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
    "    \n",
    "\n",
    "\n",
    "for key in hetero_graph.node_target:\n",
    "    hetero_graph.node_target[key] = hetero_graph.node_target[key].to(args['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5940])\n",
      "torch.Size([1698])\n",
      "torch.Size([849])\n"
     ]
    }
   ],
   "source": [
    "# train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "# val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "# test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1])}\n",
    "\n",
    "nEvents = hetero_graph.num_nodes(\"event\")\n",
    "nConcepts = hetero_graph.num_nodes(\"concept\")\n",
    "\n",
    "s1 = 0.7\n",
    "s2 = 0.8\n",
    "\n",
    "train_idx = {   \"event\": torch.tensor(range(0, int(nEvents * s1))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(0, int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "val_idx = {   \"event\": torch.tensor(range(int(nEvents * s1), int(nEvents * s2))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s1), int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "test_idx = {   \"event\": torch.tensor(range(int(nEvents * s2), nEvents)).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s2), nConcepts)).to(args[\"device\"])\n",
    "            }\n",
    "\n",
    "print(train_idx[\"event\"].shape)\n",
    "print(test_idx[\"event\"].shape)\n",
    "print(val_idx[\"event\"].shape)\n",
    "\n",
    "# dataset = deepsnap.dataset.GraphDataset([hetero_graph], task='node')\n",
    "\n",
    "# dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
    "# datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2.198608875274658 Accs [tensor(9.6392e+08, grad_fn=<SumBackward0>), tensor(1.3904e+08, grad_fn=<SumBackward0>), tensor(2.7148e+08, grad_fn=<SumBackward0>)]\n",
      "Epoch 1 Loss 2.539403200149536 Accs [tensor(1.3178e+08, grad_fn=<SumBackward0>), tensor(18922856., grad_fn=<SumBackward0>), tensor(36758836., grad_fn=<SumBackward0>)]\n",
      "Epoch 2 Loss 3.251824378967285 Accs [tensor(5462817., grad_fn=<SumBackward0>), tensor(786434.0625, grad_fn=<SumBackward0>), tensor(1523125.5000, grad_fn=<SumBackward0>)]\n",
      "Epoch 3 Loss 3.4014203548431396 Accs [tensor(5344329.5000, grad_fn=<SumBackward0>), tensor(770706.3125, grad_fn=<SumBackward0>), tensor(1504326.2500, grad_fn=<SumBackward0>)]\n",
      "Epoch 4 Loss 3.565242052078247 Accs [tensor(4050808., grad_fn=<SumBackward0>), tensor(583626.5000, grad_fn=<SumBackward0>), tensor(1134562.7500, grad_fn=<SumBackward0>)]\n",
      "Epoch 5 Loss 1.6745727062225342 Accs [tensor(3283487., grad_fn=<SumBackward0>), tensor(472265.4688, grad_fn=<SumBackward0>), tensor(914554.8125, grad_fn=<SumBackward0>)]\n",
      "Epoch 6 Loss 2.7013440132141113 Accs [tensor(3978789.5000, grad_fn=<SumBackward0>), tensor(571761.8125, grad_fn=<SumBackward0>), tensor(1107780., grad_fn=<SumBackward0>)]\n",
      "Epoch 7 Loss 3.285761833190918 Accs [tensor(3755660.2500, grad_fn=<SumBackward0>), tensor(539493.5000, grad_fn=<SumBackward0>), tensor(1046083.4375, grad_fn=<SumBackward0>)]\n",
      "Epoch 8 Loss 2.893404483795166 Accs [tensor(2674155.2500, grad_fn=<SumBackward0>), tensor(383855.3750, grad_fn=<SumBackward0>), tensor(745271.5625, grad_fn=<SumBackward0>)]\n",
      "Epoch 9 Loss 2.0809686183929443 Accs [tensor(1392929.5000, grad_fn=<SumBackward0>), tensor(199617.7344, grad_fn=<SumBackward0>), tensor(388521.5625, grad_fn=<SumBackward0>)]\n",
      "Epoch 10 Loss 1.5244706869125366 Accs [tensor(350767.5625, grad_fn=<SumBackward0>), tensor(50042.3984, grad_fn=<SumBackward0>), tensor(97971.7188, grad_fn=<SumBackward0>)]\n",
      "Epoch 11 Loss 1.318501353263855 Accs [tensor(152370.6562, grad_fn=<SumBackward0>), tensor(22059.1582, grad_fn=<SumBackward0>), tensor(42797.1680, grad_fn=<SumBackward0>)]\n",
      "Epoch 12 Loss 1.5042908191680908 Accs [tensor(190167.3438, grad_fn=<SumBackward0>), tensor(27466.5469, grad_fn=<SumBackward0>), tensor(53495.6172, grad_fn=<SumBackward0>)]\n",
      "Epoch 13 Loss 1.4076300859451294 Accs [tensor(19021.8281, grad_fn=<SumBackward0>), tensor(2955.9102, grad_fn=<SumBackward0>), tensor(5737.4570, grad_fn=<SumBackward0>)]\n",
      "Epoch 14 Loss 1.2871661186218262 Accs [tensor(202355.4531, grad_fn=<SumBackward0>), tensor(28658.9570, grad_fn=<SumBackward0>), tensor(56625.9922, grad_fn=<SumBackward0>)]\n",
      "Epoch 15 Loss 1.120099663734436 Accs [tensor(375367.3438, grad_fn=<SumBackward0>), tensor(53340.1367, grad_fn=<SumBackward0>), tensor(104853.0156, grad_fn=<SumBackward0>)]\n",
      "Epoch 16 Loss 1.2068192958831787 Accs [tensor(384421.2500, grad_fn=<SumBackward0>), tensor(54527.5078, grad_fn=<SumBackward0>), tensor(107513.1250, grad_fn=<SumBackward0>)]\n",
      "Epoch 17 Loss 1.3600603342056274 Accs [tensor(293608.5000, grad_fn=<SumBackward0>), tensor(41470.9961, grad_fn=<SumBackward0>), tensor(82152.8438, grad_fn=<SumBackward0>)]\n",
      "Epoch 18 Loss 1.3475420475006104 Accs [tensor(186314.2812, grad_fn=<SumBackward0>), tensor(26145.5371, grad_fn=<SumBackward0>), tensor(52410.3555, grad_fn=<SumBackward0>)]\n",
      "Epoch 19 Loss 1.167765736579895 Accs [tensor(94630.1016, grad_fn=<SumBackward0>), tensor(13790.4463, grad_fn=<SumBackward0>), tensor(26936.9395, grad_fn=<SumBackward0>)]\n",
      "Epoch 20 Loss 1.0467666387557983 Accs [tensor(49826.9883, grad_fn=<SumBackward0>), tensor(7577.7402, grad_fn=<SumBackward0>), tensor(14328.5371, grad_fn=<SumBackward0>)]\n",
      "Epoch 21 Loss 1.225016713142395 Accs [tensor(49313.7227, grad_fn=<SumBackward0>), tensor(7452.9575, grad_fn=<SumBackward0>), tensor(14212.6387, grad_fn=<SumBackward0>)]\n",
      "Epoch 22 Loss 1.241107702255249 Accs [tensor(69795.9062, grad_fn=<SumBackward0>), tensor(10253.3184, grad_fn=<SumBackward0>), tensor(20027.0547, grad_fn=<SumBackward0>)]\n",
      "Epoch 23 Loss 1.0739803314208984 Accs [tensor(98110.0156, grad_fn=<SumBackward0>), tensor(14038.9941, grad_fn=<SumBackward0>), tensor(27771.8008, grad_fn=<SumBackward0>)]\n",
      "Epoch 24 Loss 1.0886197090148926 Accs [tensor(116988.3125, grad_fn=<SumBackward0>), tensor(16414.4688, grad_fn=<SumBackward0>), tensor(32864.8203, grad_fn=<SumBackward0>)]\n",
      "Epoch 25 Loss 1.216103196144104 Accs [tensor(116416.5625, grad_fn=<SumBackward0>), tensor(16238.2656, grad_fn=<SumBackward0>), tensor(32644.5664, grad_fn=<SumBackward0>)]\n",
      "Epoch 26 Loss 1.2736603021621704 Accs [tensor(97412.6562, grad_fn=<SumBackward0>), tensor(13700.6465, grad_fn=<SumBackward0>), tensor(27401.0918, grad_fn=<SumBackward0>)]\n",
      "Epoch 27 Loss 1.1513175964355469 Accs [tensor(74386.4922, grad_fn=<SumBackward0>), tensor(10675.1406, grad_fn=<SumBackward0>), tensor(21082.9648, grad_fn=<SumBackward0>)]\n",
      "Epoch 28 Loss 0.981155276298523 Accs [tensor(59250.1094, grad_fn=<SumBackward0>), tensor(8665.5156, grad_fn=<SumBackward0>), tensor(16972.6094, grad_fn=<SumBackward0>)]\n",
      "Epoch 29 Loss 1.0340774059295654 Accs [tensor(55089.7656, grad_fn=<SumBackward0>), tensor(8070.4185, grad_fn=<SumBackward0>), tensor(15832.8281, grad_fn=<SumBackward0>)]\n",
      "Epoch 30 Loss 1.0743731260299683 Accs [tensor(65381.5078, grad_fn=<SumBackward0>), tensor(9462.2832, grad_fn=<SumBackward0>), tensor(18693.1250, grad_fn=<SumBackward0>)]\n",
      "Epoch 31 Loss 1.00156569480896 Accs [tensor(84108.7734, grad_fn=<SumBackward0>), tensor(11982.8066, grad_fn=<SumBackward0>), tensor(23835.3828, grad_fn=<SumBackward0>)]\n",
      "Epoch 32 Loss 0.9674441814422607 Accs [tensor(101001.4453, grad_fn=<SumBackward0>), tensor(14207.2656, grad_fn=<SumBackward0>), tensor(28424.6953, grad_fn=<SumBackward0>)]\n",
      "Epoch 33 Loss 0.9798054099082947 Accs [tensor(112075.0703, grad_fn=<SumBackward0>), tensor(15661.5654, grad_fn=<SumBackward0>), tensor(31425.2559, grad_fn=<SumBackward0>)]\n",
      "Epoch 34 Loss 0.9911358952522278 Accs [tensor(119810.6016, grad_fn=<SumBackward0>), tensor(16735.5391, grad_fn=<SumBackward0>), tensor(33541.6562, grad_fn=<SumBackward0>)]\n",
      "Epoch 35 Loss 1.0252177715301514 Accs [tensor(114921.2891, grad_fn=<SumBackward0>), tensor(16087.0410, grad_fn=<SumBackward0>), tensor(32252.5430, grad_fn=<SumBackward0>)]\n",
      "Epoch 36 Loss 1.0004624128341675 Accs [tensor(105294.6562, grad_fn=<SumBackward0>), tensor(14830.2471, grad_fn=<SumBackward0>), tensor(29692.6641, grad_fn=<SumBackward0>)]\n",
      "Epoch 37 Loss 0.9693936705589294 Accs [tensor(94504.0938, grad_fn=<SumBackward0>), tensor(13411.4277, grad_fn=<SumBackward0>), tensor(26757.7656, grad_fn=<SumBackward0>)]\n",
      "Epoch 38 Loss 1.048781156539917 Accs [tensor(90771.2656, grad_fn=<SumBackward0>), tensor(12941.0557, grad_fn=<SumBackward0>), tensor(25734.7598, grad_fn=<SumBackward0>)]\n",
      "Epoch 39 Loss 1.0568917989730835 Accs [tensor(101977.3203, grad_fn=<SumBackward0>), tensor(14428.0664, grad_fn=<SumBackward0>), tensor(28788.3398, grad_fn=<SumBackward0>)]\n",
      "Epoch 40 Loss 1.030727744102478 Accs [tensor(117763.2188, grad_fn=<SumBackward0>), tensor(16480.5840, grad_fn=<SumBackward0>), tensor(33043.5273, grad_fn=<SumBackward0>)]\n",
      "Epoch 41 Loss 0.9833500385284424 Accs [tensor(130385.1953, grad_fn=<SumBackward0>), tensor(18259.8848, grad_fn=<SumBackward0>), tensor(36399.1016, grad_fn=<SumBackward0>)]\n",
      "Epoch 42 Loss 1.0468549728393555 Accs [tensor(132717.5312, grad_fn=<SumBackward0>), tensor(18605.2637, grad_fn=<SumBackward0>), tensor(37034.3359, grad_fn=<SumBackward0>)]\n",
      "Epoch 43 Loss 1.078926920890808 Accs [tensor(127642.2109, grad_fn=<SumBackward0>), tensor(17878.3418, grad_fn=<SumBackward0>), tensor(35675.2422, grad_fn=<SumBackward0>)]\n",
      "Epoch 44 Loss 1.071664571762085 Accs [tensor(107692.9766, grad_fn=<SumBackward0>), tensor(15066.8037, grad_fn=<SumBackward0>), tensor(30274.0664, grad_fn=<SumBackward0>)]\n",
      "Epoch 45 Loss 1.032799482345581 Accs [tensor(84951.6953, grad_fn=<SumBackward0>), tensor(12079.2109, grad_fn=<SumBackward0>), tensor(24075.4727, grad_fn=<SumBackward0>)]\n",
      "Epoch 46 Loss 0.9759981632232666 Accs [tensor(74328.6875, grad_fn=<SumBackward0>), tensor(10675.6445, grad_fn=<SumBackward0>), tensor(21132.0215, grad_fn=<SumBackward0>)]\n",
      "Epoch 47 Loss 1.0008220672607422 Accs [tensor(69438.7500, grad_fn=<SumBackward0>), tensor(10024.6797, grad_fn=<SumBackward0>), tensor(19765.7500, grad_fn=<SumBackward0>)]\n",
      "Epoch 48 Loss 0.9869560599327087 Accs [tensor(80907.6641, grad_fn=<SumBackward0>), tensor(11541.9727, grad_fn=<SumBackward0>), tensor(22919.7129, grad_fn=<SumBackward0>)]\n",
      "Epoch 49 Loss 0.9802485704421997 Accs [tensor(81217.7500, grad_fn=<SumBackward0>), tensor(11581.0527, grad_fn=<SumBackward0>), tensor(23002.7148, grad_fn=<SumBackward0>)]\n",
      "Epoch 50 Loss 0.9873061776161194 Accs [tensor(87011.1719, grad_fn=<SumBackward0>), tensor(12347.0010, grad_fn=<SumBackward0>), tensor(24596.3438, grad_fn=<SumBackward0>)]\n",
      "Epoch 51 Loss 0.9574432969093323 Accs [tensor(102382.2500, grad_fn=<SumBackward0>), tensor(14363.2334, grad_fn=<SumBackward0>), tensor(28790.9375, grad_fn=<SumBackward0>)]\n",
      "Epoch 52 Loss 0.9585006833076477 Accs [tensor(108516.1484, grad_fn=<SumBackward0>), tensor(15179.1084, grad_fn=<SumBackward0>), tensor(30469.2344, grad_fn=<SumBackward0>)]\n",
      "Epoch 53 Loss 0.9792914986610413 Accs [tensor(111081.7031, grad_fn=<SumBackward0>), tensor(15532.9395, grad_fn=<SumBackward0>), tensor(31172.4648, grad_fn=<SumBackward0>)]\n",
      "Epoch 54 Loss 0.9691100120544434 Accs [tensor(113594.8750, grad_fn=<SumBackward0>), tensor(15881.8604, grad_fn=<SumBackward0>), tensor(31861.9180, grad_fn=<SumBackward0>)]\n",
      "Epoch 55 Loss 0.9887160062789917 Accs [tensor(107284.4688, grad_fn=<SumBackward0>), tensor(15015.0391, grad_fn=<SumBackward0>), tensor(30147.5371, grad_fn=<SumBackward0>)]\n",
      "Epoch 56 Loss 0.9914010763168335 Accs [tensor(99218.3750, grad_fn=<SumBackward0>), tensor(13996.1533, grad_fn=<SumBackward0>), tensor(27984.6016, grad_fn=<SumBackward0>)]\n",
      "Epoch 57 Loss 0.9570983648300171 Accs [tensor(97425.4766, grad_fn=<SumBackward0>), tensor(13820.5176, grad_fn=<SumBackward0>), tensor(27543.4062, grad_fn=<SumBackward0>)]\n",
      "Epoch 58 Loss 1.022192358970642 Accs [tensor(102901.6094, grad_fn=<SumBackward0>), tensor(14557.6787, grad_fn=<SumBackward0>), tensor(29053.5820, grad_fn=<SumBackward0>)]\n",
      "Epoch 59 Loss 1.0448167324066162 Accs [tensor(115964.6719, grad_fn=<SumBackward0>), tensor(16258.8672, grad_fn=<SumBackward0>), tensor(32602.4414, grad_fn=<SumBackward0>)]\n",
      "Epoch 60 Loss 1.0223933458328247 Accs [tensor(142077.2031, grad_fn=<SumBackward0>), tensor(19958.6914, grad_fn=<SumBackward0>), tensor(39646.1094, grad_fn=<SumBackward0>)]\n",
      "Epoch 61 Loss 0.9633001685142517 Accs [tensor(176400.0781, grad_fn=<SumBackward0>), tensor(24895.3633, grad_fn=<SumBackward0>), tensor(48931.2461, grad_fn=<SumBackward0>)]\n",
      "Epoch 62 Loss 1.0358095169067383 Accs [tensor(200406.1406, grad_fn=<SumBackward0>), tensor(28350.3398, grad_fn=<SumBackward0>), tensor(55581.0352, grad_fn=<SumBackward0>)]\n",
      "Epoch 63 Loss 1.0775145292282104 Accs [tensor(193953.3906, grad_fn=<SumBackward0>), tensor(27420.1230, grad_fn=<SumBackward0>), tensor(53795.8438, grad_fn=<SumBackward0>)]\n",
      "Epoch 64 Loss 1.0700818300247192 Accs [tensor(159272.2969, grad_fn=<SumBackward0>), tensor(22432.3008, grad_fn=<SumBackward0>), tensor(44284.6562, grad_fn=<SumBackward0>)]\n",
      "Epoch 65 Loss 1.0275719165802002 Accs [tensor(122842.2969, grad_fn=<SumBackward0>), tensor(17185.9141, grad_fn=<SumBackward0>), tensor(34410.2188, grad_fn=<SumBackward0>)]\n",
      "Epoch 66 Loss 0.9606499671936035 Accs [tensor(108826.9531, grad_fn=<SumBackward0>), tensor(15221.9121, grad_fn=<SumBackward0>), tensor(30561.9629, grad_fn=<SumBackward0>)]\n",
      "Epoch 67 Loss 0.9908931255340576 Accs [tensor(116293.3594, grad_fn=<SumBackward0>), tensor(16250.6094, grad_fn=<SumBackward0>), tensor(32593.7793, grad_fn=<SumBackward0>)]\n",
      "Epoch 68 Loss 0.9754273295402527 Accs [tensor(134790.5000, grad_fn=<SumBackward0>), tensor(18909.5312, grad_fn=<SumBackward0>), tensor(37600.7422, grad_fn=<SumBackward0>)]\n",
      "Epoch 69 Loss 0.9815545082092285 Accs [tensor(140417., grad_fn=<SumBackward0>), tensor(19719.2363, grad_fn=<SumBackward0>), tensor(39117.5781, grad_fn=<SumBackward0>)]\n",
      "Epoch 70 Loss 0.9890527725219727 Accs [tensor(134729.1719, grad_fn=<SumBackward0>), tensor(18900.4277, grad_fn=<SumBackward0>), tensor(37582.2109, grad_fn=<SumBackward0>)]\n",
      "Epoch 71 Loss 0.9568173289299011 Accs [tensor(118712.0781, grad_fn=<SumBackward0>), tensor(16590.7188, grad_fn=<SumBackward0>), tensor(33251.4961, grad_fn=<SumBackward0>)]\n",
      "Epoch 72 Loss 1.0188391208648682 Accs [tensor(108143.1250, grad_fn=<SumBackward0>), tensor(15126.4785, grad_fn=<SumBackward0>), tensor(30375.4648, grad_fn=<SumBackward0>)]\n",
      "Epoch 73 Loss 1.0416074991226196 Accs [tensor(100153.0234, grad_fn=<SumBackward0>), tensor(14075.0938, grad_fn=<SumBackward0>), tensor(28203.4180, grad_fn=<SumBackward0>)]\n",
      "Epoch 74 Loss 1.0227502584457397 Accs [tensor(100631.8281, grad_fn=<SumBackward0>), tensor(14138.7393, grad_fn=<SumBackward0>), tensor(28333.2051, grad_fn=<SumBackward0>)]\n",
      "Epoch 75 Loss 0.9669365882873535 Accs [tensor(109470.7891, grad_fn=<SumBackward0>), tensor(15325.2754, grad_fn=<SumBackward0>), tensor(30764.4805, grad_fn=<SumBackward0>)]\n",
      "Epoch 76 Loss 1.0257383584976196 Accs [tensor(121964.3438, grad_fn=<SumBackward0>), tensor(17066.4258, grad_fn=<SumBackward0>), tensor(34203.6484, grad_fn=<SumBackward0>)]\n",
      "Epoch 77 Loss 1.0661063194274902 Accs [tensor(125353.5469, grad_fn=<SumBackward0>), tensor(17553.6113, grad_fn=<SumBackward0>), tensor(35087.5742, grad_fn=<SumBackward0>)]\n",
      "Epoch 78 Loss 1.0642794370651245 Accs [tensor(133574.4375, grad_fn=<SumBackward0>), tensor(18734.6074, grad_fn=<SumBackward0>), tensor(37289.9141, grad_fn=<SumBackward0>)]\n",
      "Epoch 79 Loss 1.0230133533477783 Accs [tensor(131884.1875, grad_fn=<SumBackward0>), tensor(18490.9395, grad_fn=<SumBackward0>), tensor(36822.1875, grad_fn=<SumBackward0>)]\n",
      "Epoch 80 Loss 0.9574368000030518 Accs [tensor(136904.7969, grad_fn=<SumBackward0>), tensor(19211.4746, grad_fn=<SumBackward0>), tensor(38169.8281, grad_fn=<SumBackward0>)]\n",
      "Epoch 81 Loss 0.9863971471786499 Accs [tensor(169265.2500, grad_fn=<SumBackward0>), tensor(23865.5566, grad_fn=<SumBackward0>), tensor(46915.7852, grad_fn=<SumBackward0>)]\n",
      "Epoch 82 Loss 0.9729800820350647 Accs [tensor(198578.2031, grad_fn=<SumBackward0>), tensor(28082.0234, grad_fn=<SumBackward0>), tensor(55037.8281, grad_fn=<SumBackward0>)]\n",
      "Epoch 83 Loss 0.9817958474159241 Accs [tensor(206796.5312, grad_fn=<SumBackward0>), tensor(29264.2324, grad_fn=<SumBackward0>), tensor(57319.5117, grad_fn=<SumBackward0>)]\n",
      "Epoch 84 Loss 0.9882819652557373 Accs [tensor(196350., grad_fn=<SumBackward0>), tensor(27760.6816, grad_fn=<SumBackward0>), tensor(54429.2344, grad_fn=<SumBackward0>)]\n",
      "Epoch 85 Loss 0.9536174535751343 Accs [tensor(177247.8281, grad_fn=<SumBackward0>), tensor(25013.3398, grad_fn=<SumBackward0>), tensor(49171.3828, grad_fn=<SumBackward0>)]\n",
      "Epoch 86 Loss 1.017242193222046 Accs [tensor(160464.7969, grad_fn=<SumBackward0>), tensor(22601.9043, grad_fn=<SumBackward0>), tensor(44636.1680, grad_fn=<SumBackward0>)]\n",
      "Epoch 87 Loss 1.0400676727294922 Accs [tensor(153018.1406, grad_fn=<SumBackward0>), tensor(21525.5469, grad_fn=<SumBackward0>), tensor(42567.8594, grad_fn=<SumBackward0>)]\n",
      "Epoch 88 Loss 1.0214167833328247 Accs [tensor(200676.8438, grad_fn=<SumBackward0>), tensor(28383.1602, grad_fn=<SumBackward0>), tensor(55632.7266, grad_fn=<SumBackward0>)]\n",
      "Epoch 89 Loss 0.9646722674369812 Accs [tensor(239991.7969, grad_fn=<SumBackward0>), tensor(34041.9688, grad_fn=<SumBackward0>), tensor(66530.9375, grad_fn=<SumBackward0>)]\n",
      "Epoch 90 Loss 1.0284398794174194 Accs [tensor(232501.5469, grad_fn=<SumBackward0>), tensor(32966.7266, grad_fn=<SumBackward0>), tensor(64463.4609, grad_fn=<SumBackward0>)]\n",
      "Epoch 91 Loss 1.069491982460022 Accs [tensor(197769.2500, grad_fn=<SumBackward0>), tensor(27970.1914, grad_fn=<SumBackward0>), tensor(54842.3359, grad_fn=<SumBackward0>)]\n",
      "Epoch 92 Loss 1.0655995607376099 Accs [tensor(154589.6094, grad_fn=<SumBackward0>), tensor(21758.8594, grad_fn=<SumBackward0>), tensor(42963.9453, grad_fn=<SumBackward0>)]\n",
      "Epoch 93 Loss 1.024490237236023 Accs [tensor(127219.1953, grad_fn=<SumBackward0>), tensor(17818.9961, grad_fn=<SumBackward0>), tensor(35564.1562, grad_fn=<SumBackward0>)]\n",
      "Epoch 94 Loss 0.9588661789894104 Accs [tensor(147497.7188, grad_fn=<SumBackward0>), tensor(20747.6738, grad_fn=<SumBackward0>), tensor(41191.1992, grad_fn=<SumBackward0>)]\n",
      "Epoch 95 Loss 0.9907147288322449 Accs [tensor(186804.9062, grad_fn=<SumBackward0>), tensor(26417.0898, grad_fn=<SumBackward0>), tensor(52017.9258, grad_fn=<SumBackward0>)]\n",
      "Epoch 96 Loss 0.9838541150093079 Accs [tensor(211805.3750, grad_fn=<SumBackward0>), tensor(30011.4141, grad_fn=<SumBackward0>), tensor(58883.4219, grad_fn=<SumBackward0>)]\n",
      "Epoch 97 Loss 0.9670862555503845 Accs [tensor(199907.5312, grad_fn=<SumBackward0>), tensor(28290.6914, grad_fn=<SumBackward0>), tensor(55519.0547, grad_fn=<SumBackward0>)]\n",
      "Epoch 98 Loss 0.9679144024848938 Accs [tensor(176130.0156, grad_fn=<SumBackward0>), tensor(24859.5469, grad_fn=<SumBackward0>), tensor(48863.8125, grad_fn=<SumBackward0>)]\n",
      "Epoch 99 Loss 0.9715955853462219 Accs [tensor(176051.5156, grad_fn=<SumBackward0>), tensor(24843.7539, grad_fn=<SumBackward0>), tensor(48826.2812, grad_fn=<SumBackward0>)]\n",
      "Best accs [tensor(19021.8281, grad_fn=<SumBackward0>), tensor(2955.9102, grad_fn=<SumBackward0>), tensor(5737.4570, grad_fn=<SumBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n",
    "    # print(\n",
    "    #     f\"Epoch {epoch + 1}: loss {round(loss, 5)}, \"\n",
    "    #     f\"train micro {round(accs[0][0] * 100, 2)}%, train macro {round(accs[0][1] * 100, 2)}%, \"\n",
    "    #     f\"valid micro {round(accs[1][0] * 100, 2)}%, valid macro {round(accs[1][1] * 100, 2)}%, \"\n",
    "    #     f\"test micro {round(accs[2][0] * 100, 2)}%, test macro {round(accs[2][1] * 100, 2)}%\"\n",
    "    # )\n",
    "    print(f\"Epoch {epoch} Loss {loss} Accs {accs}\")\n",
    "\n",
    "best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx])\n",
    "\n",
    "print(\"Best accs\", best_accs)\n",
    "\n",
    "\n",
    "\n",
    "# print(\n",
    "#     f\"Best model: \"\n",
    "#     f\"train micro {round(best_accs[0][0] * 100, 2)}%, train macro {round(best_accs[0][1] * 100, 2)}%, \"\n",
    "#     f\"valid micro {round(best_accs[1][0] * 100, 2)}%, valid macro {round(best_accs[1][1] * 100, 2)}%, \"\n",
    "#     f\"test micro {round(best_accs[2][0] * 100, 2)}%, test macro {round(best_accs[2][1] * 100, 2)}%\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best_model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "# mask = preds['event'] > 0\n",
    "# preds['event'][preds['event'] > 0].shape\n",
    "\n",
    "\n",
    "for i in range(3000):\n",
    "    if preds['event'][i] > 0:\n",
    "        print(preds['event'][i], hetero_graph.node_target['event'][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
