{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        self.lin_dst = nn.Linear(in_channels_dst, out_channels)\n",
    "        self.lin_src = nn.Linear(in_channels_src, out_channels)\n",
    "        self.lin_update = nn.Linear(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "        ):\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        \"\"\"\n",
    "        Initializes the HeteroGNNWrapperConv instance.\n",
    "\n",
    "        :param convs: Dictionary of convolution layers for each message type.\n",
    "        :param args: Arguments dictionary containing hyperparameters like hidden_size and attn_size.\n",
    "        :param aggr: Aggregation method, defaults to 'mean'.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_features: Dictionary of node features for each node type.\n",
    "        :param edge_indices: Dictionary of edge indices for each message type.\n",
    "        :return: Aggregated node embeddings for each node type.\n",
    "        \"\"\"\n",
    "        \n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        \n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        \n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "                \n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        \"\"\"\n",
    "        Aggregates node embeddings using the specified aggregation method.\n",
    "\n",
    "        :param xs: List of node embeddings to aggregate.\n",
    "        :return: Aggregated node embeddings as a torch.Tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates convolutional layers for each message type in a heterogeneous graph.\n",
    "\n",
    "    :param hetero_graph: The heterogeneous graph for which convolutions are to be created.\n",
    "    :param conv: The convolutional layer class or constructor.\n",
    "    :param hidden_size: The number of features in the hidden layer.\n",
    "    :param first_layer: Boolean indicating if this is the first layer in the network.\n",
    "    \n",
    "    :return: A dictionary of convolutional layers, keyed by message type.\n",
    "    \"\"\"\n",
    "\n",
    "    convs = {}\n",
    "    \n",
    "    # Extracting all types of messages/edges in the heterogeneous graph.\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        \n",
    "        # Determine the input feature size for source and destination nodes.\n",
    "        # If it's the first layer, use the feature size of the nodes.\n",
    "        # Otherwise, use the hidden size, since from there on the size of embeddings\n",
    "        # is the same for all nodes.\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        \n",
    "        # Create a convolutional layer for this message type and add it to the dictionary.\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, num_layers, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "        \n",
    "        # Initialize the graph convolutional layers\n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        # Initialize batch normalization, ReLU, and fully connected layers for each node type\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            \n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            \n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "            \n",
    "    def forward(self, node_feature, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_feature: Dictionary of node features for each node type.\n",
    "        :param edge_index: Dictionary of edge indices for each message type.\n",
    "        :return: The output embeddings for each node type after passing through the model.\n",
    "        \"\"\"\n",
    "        x = node_feature\n",
    "\n",
    "        # Apply graph convolutional, batch normalization, and ReLU layers\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        x = forward_op(x, self.fc)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        \"\"\"\n",
    "        Computes the loss for the model.\n",
    "\n",
    "        :param preds: Predictions made by the model.\n",
    "        :param y: Ground truth target values.\n",
    "        :param indices: Indices of nodes for which loss should be calculated.\n",
    "        \n",
    "        :return: The computed loss value.\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = 0\n",
    "        loss_func = torch.nn.MSELoss() \n",
    "             \n",
    "        mask = y['event'][indices['event'], 0] != -1\n",
    "        non_zero_idx = torch.masked_select(indices['event'], mask)\n",
    "                \n",
    "        loss += loss_func(preds['event'][non_zero_idx], y['event'][non_zero_idx])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    \"\"\"\n",
    "    Trains the model on the given heterogeneous graph using the specified indices.\n",
    "\n",
    "    :param model: The graph neural network model to train.\n",
    "    :param optimizer: The optimizer used for training the model.\n",
    "    :param hetero_graph: The heterogeneous graph data.\n",
    "    :param train_idx: Indices for training nodes.\n",
    "\n",
    "    :return: The training loss as a float.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "    optimizer.zero_grad() # Zero out any existing gradients \n",
    "    \n",
    "    # Compute predictions using the model\n",
    "    # TODO: Use only train_idx instead of edge_index\n",
    "    # TODO: Train only on events not on concepts\n",
    "    \n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    # Compute the loss using model's loss function\n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "\n",
    "    loss.backward() # Backward pass: compute gradient of the loss\n",
    "    optimizer.step() # Perform a single optimization step, updates parameters\n",
    "    \n",
    "    return loss.item() \n",
    "\n",
    "def test(model, graph, indices, best_model, best_tvt_scores):\n",
    "    \"\"\"\n",
    "    Tests the model on given indices and updates the best model based on validation loss.\n",
    "\n",
    "    :param model: The trained graph neural network model.\n",
    "    :param graph: The heterogeneous graph data.\n",
    "    :param indices: List of indices for training, validation, and testing nodes.\n",
    "    :param best_model: The current best model based on validation loss.\n",
    "    :param best_val: The current best validation loss.\n",
    "    \n",
    "    :return: A tuple containing the list of losses for each dataset, the best model, and the best validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    tvt_scores = []\n",
    "    \n",
    "    # Evaluate the model on each set of indices\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        \n",
    "        idx = index['event']\n",
    "        \n",
    "                     \n",
    "        # mask = y['event'][indices['event'], 0] != -1\n",
    "        # non_zero_idx = torch.masked_select(indices['event'], mask)\n",
    "        #preds['event'][non_zero_idx], y['event'][non_zero_idx]\n",
    "        \n",
    "        # non_zero_targets = torch.masked_select(graph.node_target['event'][indices['event']], mask)\n",
    "        # non_zero_truth = torch.masked_select(graph.node_target['event'][indices['event']], mask)\n",
    "        \n",
    "        mask = graph.node_target['event'][idx, 0] != -1\n",
    "        non_zero_idx = torch.masked_select(idx, mask)\n",
    "        \n",
    "        \n",
    "        L1 = torch.sum(torch.abs(preds['event'][non_zero_idx] - graph.node_target['event'][non_zero_idx])) / non_zero_idx.shape[0]\n",
    "        \n",
    "        tvt_scores.append(L1)\n",
    "    \n",
    "    # Update the best model and validation loss if the current model performs better\n",
    "    if tvt_scores[1] < best_tvt_scores[1]:\n",
    "        best_tvt_scores = tvt_scores\n",
    "        # torch.to_pickle(model, 'best_model.pkl')\n",
    "        # model.to_pickle('best_model.pkl')\n",
    "        # best_model = copy.deepcopy(model)\n",
    "        torch.save(model.state_dict(), './best_model.pkl')\n",
    "    \n",
    "    return tvt_scores, best_tvt_scores, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 48,\n",
    "    'epochs': 500,\n",
    "    'weight_decay': 0.0002930387278908051,\n",
    "    'lr': 0.05091434725288385,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell creates a small heterogeneous graph, primarily for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [1, 1, 1],   # event 0\n",
    "                [2, 2, 2]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([[50], [2000]], dtype=torch.float32),\n",
    "    # \"concept\": torch.tensor([[0], [0]], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}\n",
    "\n",
    "# Testing\n",
    "hetero_graph = HeteroGraph(\n",
    "    node_feature=S_node_feature,\n",
    "    node_target=S_node_targets,\n",
    "    edge_index=S_edge_index\n",
    ")\n",
    "\n",
    "train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./1_concepts_similar_llm (5).pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "    # Convert to directed graph for compatibility with Deepsnap\n",
    "    # G = G.to_directed()\n",
    "   \n",
    "        \n",
    "hetero_graph = HeteroGraph(G, netlib=nx, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 769\n",
      "\t Feature 769\n",
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 769\n",
      "\t Feature 1\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 1\n",
      "\t Feature 769\n",
      "KEY ('event', 'similar', 'event') <class 'tuple'>\n",
      "KEY NUMS ('event', 'similar', 'event') 8487 8487\n",
      "MAX EDGES tensor(8283) tensor(8283) 8487 8487\n",
      "KEY ('event', 'related', 'concept') <class 'tuple'>\n",
      "KEY NUMS ('event', 'related', 'concept') 8487 8729\n",
      "MAX EDGES tensor(8265) tensor(8728) 8487 8729\n",
      "KEY ('concept', 'related', 'event') <class 'tuple'>\n",
      "KEY NUMS ('concept', 'related', 'event') 8729 8487\n",
      "MAX EDGES tensor(8728) tensor(8265) 8729 8487\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block ensures that all tensors in a heterogeneous graph are transferred to the same \n",
    "computational device, as specified in the 'args' dictionary.\n",
    "\"\"\"\n",
    "\n",
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "# Send node features to device\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "# Create a torch.SparseTensor from edge_index and send it to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key, type(key))\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    \n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max(), hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
    "    \n",
    "# Send node targets to device\n",
    "for key in hetero_graph.node_target:\n",
    "    hetero_graph.node_target[key] = hetero_graph.node_target[key].to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5940])\n",
      "torch.Size([1698])\n",
      "torch.Size([849])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block creates a basic split of a graph's nodes into training, validation, and testing sets. \n",
    "It uses predefined ratios to divide 'event' and 'concept' nodes in the heterogeneous graph for a simple \n",
    "dataset split, mainly for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "nEvents = hetero_graph.num_nodes(\"event\")\n",
    "nConcepts = hetero_graph.num_nodes(\"concept\")\n",
    "\n",
    "s1 = 0.7\n",
    "s2 = 0.8\n",
    "\n",
    "train_idx = {   \"event\": torch.tensor(range(0, int(nEvents * s1))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(0, int(nConcepts * s1))).to(args[\"device\"])\n",
    "            }\n",
    "val_idx = {   \"event\": torch.tensor(range(int(nEvents * s1), int(nEvents * s2))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s1), int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "test_idx = {   \"event\": torch.tensor(range(int(nEvents * s2), nEvents)).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s2), nConcepts)).to(args[\"device\"])\n",
    "            }\n",
    "\n",
    "print(train_idx[\"event\"].shape)\n",
    "print(test_idx[\"event\"].shape)\n",
    "print(val_idx[\"event\"].shape)\n",
    "\n",
    "\n",
    "# TODO: Add node labels to the nodes and try to make the deepsnap split work even for regression!\n",
    "\n",
    "# dataset = deepsnap.dataset.GraphDataset([hetero_graph], task='node')\n",
    "\n",
    "# dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
    "# datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2822.3298 Current Train,Val,Test Scores [226.2257080078125, 227.09231567382812, 70.88436126708984]\n",
      "Epoch 1 Loss 2713.1873 Current Train,Val,Test Scores [1775.802490234375, 1762.856201171875, 605.40185546875]\n",
      "Epoch 2 Loss 2342.7920 Current Train,Val,Test Scores [1354.306396484375, 1344.85546875, 424.0990905761719]\n",
      "Epoch 3 Loss 2187.1240 Current Train,Val,Test Scores [1978.1473388671875, 1972.76171875, 631.7246704101562]\n",
      "Epoch 4 Loss 1873.4401 Current Train,Val,Test Scores [3043.424072265625, 3040.700439453125, 902.4088134765625]\n",
      "Epoch 5 Loss 1742.9336 Current Train,Val,Test Scores [2618.298583984375, 2617.506103515625, 766.2662963867188]\n",
      "Epoch 6 Loss 1695.5397 Current Train,Val,Test Scores [1211.90185546875, 1211.825439453125, 374.5335998535156]\n",
      "Epoch 7 Loss 1718.3269 Current Train,Val,Test Scores [60.6850700378418, 62.92409896850586, 65.31647491455078]\n",
      "Epoch 8 Loss 1762.7640 Current Train,Val,Test Scores [609.1616821289062, 612.8240356445312, 204.31471252441406]\n",
      "Epoch 9 Loss 1773.0851 Current Train,Val,Test Scores [965.544921875, 969.1481323242188, 295.41461181640625]\n",
      "Epoch 10 Loss 1740.8792 Current Train,Val,Test Scores [1036.859619140625, 1041.267333984375, 315.5842590332031]\n",
      "Epoch 11 Loss 1686.6483 Current Train,Val,Test Scores [711.9849853515625, 717.1538696289062, 232.7534942626953]\n",
      "Epoch 12 Loss 1633.9507 Current Train,Val,Test Scores [84.5916976928711, 89.895263671875, 69.72748565673828]\n",
      "Epoch 13 Loss 1595.9901 Current Train,Val,Test Scores [459.63433837890625, 451.7052917480469, 167.99826049804688]\n",
      "Epoch 14 Loss 1574.7471 Current Train,Val,Test Scores [573.4931640625, 565.2906494140625, 198.75790405273438]\n",
      "Epoch 15 Loss 1564.4949 Current Train,Val,Test Scores [745.1153564453125, 736.542724609375, 243.88926696777344]\n",
      "Epoch 16 Loss 1547.7548 Current Train,Val,Test Scores [931.0266723632812, 921.7619018554688, 291.26202392578125]\n",
      "Epoch 17 Loss 1513.5778 Current Train,Val,Test Scores [1089.86572265625, 1079.652587890625, 329.4002990722656]\n",
      "Epoch 18 Loss 1459.3738 Current Train,Val,Test Scores [645.5247192382812, 634.5335083007812, 205.35287475585938]\n",
      "Epoch 19 Loss 1383.0454 Current Train,Val,Test Scores [156.8959197998047, 162.92869567871094, 71.65990447998047]\n",
      "Epoch 20 Loss 1313.4202 Current Train,Val,Test Scores [61.01580047607422, 72.52690887451172, 39.76852035522461]\n",
      "Epoch 21 Loss 1260.0979 Current Train,Val,Test Scores [149.38473510742188, 155.63336181640625, 58.832557678222656]\n",
      "Epoch 22 Loss 1171.7390 Current Train,Val,Test Scores [318.1208801269531, 311.8063049316406, 99.42704010009766]\n",
      "Epoch 23 Loss 1090.6008 Current Train,Val,Test Scores [440.195068359375, 427.58624267578125, 126.76600646972656]\n",
      "Epoch 24 Loss 1027.7191 Current Train,Val,Test Scores [542.3912353515625, 529.7542724609375, 150.29869079589844]\n",
      "Epoch 25 Loss 944.1061 Current Train,Val,Test Scores [526.5718383789062, 513.966796875, 143.1482391357422]\n",
      "Epoch 26 Loss 885.4171 Current Train,Val,Test Scores [555.8279418945312, 543.3506469726562, 149.75396728515625]\n",
      "Epoch 27 Loss 873.4128 Current Train,Val,Test Scores [394.48944091796875, 382.1378479003906, 110.09419250488281]\n",
      "Epoch 28 Loss 975.0347 Current Train,Val,Test Scores [338.2486877441406, 330.5013732910156, 95.87913513183594]\n",
      "Epoch 29 Loss 741.1920 Current Train,Val,Test Scores [489.9722900390625, 477.34503173828125, 135.0428009033203]\n",
      "Epoch 30 Loss 980.5674 Current Train,Val,Test Scores [91.03225708007812, 100.58021545410156, 30.619600296020508]\n",
      "Epoch 31 Loss 793.4097 Current Train,Val,Test Scores [237.47149658203125, 227.35403442382812, 67.2342529296875]\n",
      "Epoch 32 Loss 857.1848 Current Train,Val,Test Scores [324.52288818359375, 317.5363464355469, 91.7189712524414]\n",
      "Epoch 33 Loss 695.4003 Current Train,Val,Test Scores [286.3238830566406, 282.2265930175781, 81.2947006225586]\n",
      "Epoch 34 Loss 811.2557 Current Train,Val,Test Scores [44.15672302246094, 56.78194046020508, 16.838834762573242]\n",
      "Epoch 35 Loss 646.7731 Current Train,Val,Test Scores [236.40335083007812, 248.93222045898438, 67.62972259521484]\n",
      "Epoch 36 Loss 737.8915 Current Train,Val,Test Scores [215.6116943359375, 228.32821655273438, 61.8820915222168]\n",
      "Epoch 37 Loss 644.2234 Current Train,Val,Test Scores [29.931110382080078, 41.14039611816406, 12.969058990478516]\n",
      "Epoch 38 Loss 635.6066 Current Train,Val,Test Scores [447.8619384765625, 435.4824523925781, 123.06837463378906]\n",
      "Epoch 39 Loss 648.7929 Current Train,Val,Test Scores [792.935791015625, 780.7615356445312, 213.42535400390625]\n",
      "Epoch 40 Loss 590.3026 Current Train,Val,Test Scores [987.0302734375, 974.8709716796875, 264.8408203125]\n",
      "Epoch 41 Loss 581.5048 Current Train,Val,Test Scores [1043.4810791015625, 1031.0611572265625, 279.8626403808594]\n",
      "Epoch 42 Loss 582.8716 Current Train,Val,Test Scores [823.2357788085938, 810.688720703125, 221.6329345703125]\n",
      "Epoch 43 Loss 574.1656 Current Train,Val,Test Scores [520.7859497070312, 508.29962158203125, 142.08407592773438]\n",
      "Epoch 44 Loss 528.1846 Current Train,Val,Test Scores [312.4100036621094, 306.59051513671875, 88.28382873535156]\n",
      "Epoch 45 Loss 540.2422 Current Train,Val,Test Scores [264.06488037109375, 262.16339111328125, 75.6819076538086]\n",
      "Epoch 46 Loss 533.1378 Current Train,Val,Test Scores [414.6548767089844, 402.0138854980469, 114.875732421875]\n",
      "Epoch 47 Loss 511.1729 Current Train,Val,Test Scores [645.9103393554688, 633.3772583007812, 174.89779663085938]\n",
      "Epoch 48 Loss 496.1905 Current Train,Val,Test Scores [803.4055786132812, 790.8270263671875, 216.62188720703125]\n",
      "Epoch 49 Loss 501.5411 Current Train,Val,Test Scores [819.1363525390625, 806.492919921875, 220.80418395996094]\n",
      "Epoch 50 Loss 486.9046 Current Train,Val,Test Scores [701.7786254882812, 689.1547241210938, 189.72674560546875]\n",
      "Epoch 51 Loss 467.5311 Current Train,Val,Test Scores [529.9685668945312, 517.40869140625, 144.65052795410156]\n",
      "Epoch 52 Loss 449.2139 Current Train,Val,Test Scores [390.4697570800781, 378.59130859375, 108.66970825195312]\n",
      "Epoch 53 Loss 450.0421 Current Train,Val,Test Scores [317.0503845214844, 310.8649597167969, 89.64041137695312]\n",
      "Epoch 54 Loss 427.3964 Current Train,Val,Test Scores [301.1016845703125, 296.26666259765625, 85.49322509765625]\n",
      "Epoch 55 Loss 416.8299 Current Train,Val,Test Scores [306.0591125488281, 300.833740234375, 86.80382537841797]\n",
      "Epoch 56 Loss 405.9895 Current Train,Val,Test Scores [290.6051940917969, 286.7546691894531, 82.77568817138672]\n",
      "Epoch 57 Loss 391.7276 Current Train,Val,Test Scores [248.0229034423828, 247.4859619140625, 71.59596252441406]\n",
      "Epoch 58 Loss 387.9270 Current Train,Val,Test Scores [182.06341552734375, 186.31634521484375, 54.16960906982422]\n",
      "Epoch 59 Loss 357.8342 Current Train,Val,Test Scores [135.0074920654297, 141.8891143798828, 41.4329948425293]\n",
      "Epoch 60 Loss 363.0037 Current Train,Val,Test Scores [71.36384582519531, 82.08509063720703, 24.06861114501953]\n",
      "Epoch 61 Loss 331.5748 Current Train,Val,Test Scores [43.50688552856445, 54.827396392822266, 16.634878158569336]\n",
      "Epoch 62 Loss 330.2864 Current Train,Val,Test Scores [88.94731140136719, 98.99137878417969, 28.591768264770508]\n",
      "Epoch 63 Loss 314.5145 Current Train,Val,Test Scores [105.69093322753906, 114.8418197631836, 33.120548248291016]\n",
      "Epoch 64 Loss 304.3384 Current Train,Val,Test Scores [202.6815185546875, 204.53636169433594, 58.83274459838867]\n",
      "Epoch 65 Loss 307.5997 Current Train,Val,Test Scores [168.64224243164062, 173.53994750976562, 49.99414825439453]\n",
      "Epoch 66 Loss 304.7144 Current Train,Val,Test Scores [339.62872314453125, 328.9438781738281, 94.6105728149414]\n",
      "Epoch 67 Loss 308.4431 Current Train,Val,Test Scores [224.6925811767578, 225.62493896484375, 64.78990936279297]\n",
      "Epoch 68 Loss 266.8474 Current Train,Val,Test Scores [216.4641571044922, 217.9118194580078, 62.568260192871094]\n",
      "Epoch 69 Loss 239.0841 Current Train,Val,Test Scores [232.18304443359375, 231.43101501464844, 66.54410552978516]\n",
      "Epoch 70 Loss 241.8967 Current Train,Val,Test Scores [217.75328063964844, 218.8425750732422, 62.82870101928711]\n",
      "Epoch 71 Loss 222.4047 Current Train,Val,Test Scores [231.82662963867188, 231.85556030273438, 66.50407409667969]\n",
      "Epoch 72 Loss 220.2010 Current Train,Val,Test Scores [265.3946533203125, 262.8067321777344, 75.3359603881836]\n",
      "Epoch 73 Loss 210.9393 Current Train,Val,Test Scores [313.11663818359375, 305.2314147949219, 87.60398864746094]\n",
      "Epoch 74 Loss 197.0241 Current Train,Val,Test Scores [204.56588745117188, 206.9173126220703, 59.336952209472656]\n",
      "Epoch 75 Loss 194.6516 Current Train,Val,Test Scores [198.14059448242188, 201.0757598876953, 57.67821502685547]\n",
      "Epoch 76 Loss 225.7776 Current Train,Val,Test Scores [247.95188903808594, 246.9156494140625, 70.74540710449219]\n",
      "Epoch 77 Loss 378.2026 Current Train,Val,Test Scores [134.94236755371094, 141.63204956054688, 40.78204345703125]\n",
      "Epoch 78 Loss 573.7118 Current Train,Val,Test Scores [309.1012878417969, 301.7442321777344, 86.7447509765625]\n",
      "Epoch 79 Loss 385.9806 Current Train,Val,Test Scores [1310.719970703125, 1299.4420166015625, 348.8565979003906]\n",
      "Epoch 80 Loss 1053.4840 Current Train,Val,Test Scores [14823.4951171875, 14844.09375, 3914.261962890625]\n",
      "Epoch 81 Loss 983.2931 Current Train,Val,Test Scores [10435.248046875, 10457.7880859375, 2754.25732421875]\n",
      "Epoch 82 Loss 1296.5769 Current Train,Val,Test Scores [742.8411865234375, 731.1703491210938, 200.13467407226562]\n",
      "Epoch 83 Loss 878.7216 Current Train,Val,Test Scores [71.78794860839844, 84.7612533569336, 24.178064346313477]\n",
      "Epoch 84 Loss 341.2374 Current Train,Val,Test Scores [526.8363647460938, 539.6472778320312, 144.35580444335938]\n",
      "Epoch 85 Loss 576.5089 Current Train,Val,Test Scores [1235.5703125, 1248.340087890625, 331.7780456542969]\n",
      "Epoch 86 Loss 404.4060 Current Train,Val,Test Scores [1207.3514404296875, 1220.185302734375, 324.2872619628906]\n",
      "Epoch 87 Loss 609.4335 Current Train,Val,Test Scores [391.4136657714844, 404.32611083984375, 108.53160095214844]\n",
      "Epoch 88 Loss 451.6070 Current Train,Val,Test Scores [338.00469970703125, 329.69671630859375, 95.11640930175781]\n",
      "Epoch 89 Loss 547.8491 Current Train,Val,Test Scores [1064.120361328125, 1050.6148681640625, 285.5556945800781]\n",
      "Epoch 90 Loss 364.1703 Current Train,Val,Test Scores [1384.7760009765625, 1371.3067626953125, 370.4005432128906]\n",
      "Epoch 91 Loss 446.4025 Current Train,Val,Test Scores [901.9756469726562, 888.8341674804688, 242.78799438476562]\n",
      "Epoch 92 Loss 412.4966 Current Train,Val,Test Scores [286.2799072265625, 282.4999084472656, 81.8880615234375]\n",
      "Epoch 93 Loss 291.5122 Current Train,Val,Test Scores [61.26517105102539, 72.01318359375, 21.88688850402832]\n",
      "Epoch 94 Loss 311.5407 Current Train,Val,Test Scores [2155.8359375, 2140.685791015625, 574.448974609375]\n",
      "Epoch 95 Loss 300.9453 Current Train,Val,Test Scores [4582.943359375, 4570.11865234375, 1215.9765625]\n",
      "Epoch 96 Loss 274.1739 Current Train,Val,Test Scores [6875.25341796875, 6862.83154296875, 1821.95166015625]\n",
      "Epoch 97 Loss 245.4529 Current Train,Val,Test Scores [8769.1748046875, 8757.0693359375, 2322.572509765625]\n",
      "Epoch 98 Loss 274.9880 Current Train,Val,Test Scores [9811.3046875, 9798.7568359375, 2598.083740234375]\n",
      "Epoch 99 Loss 260.4792 Current Train,Val,Test Scores [10733.318359375, 10720.0615234375, 2841.84814453125]\n",
      "Epoch 100 Loss 248.6265 Current Train,Val,Test Scores [10942.0625, 10928.5966796875, 2896.948974609375]\n",
      "Epoch 101 Loss 242.6598 Current Train,Val,Test Scores [8838.4970703125, 8825.5439453125, 2340.614501953125]\n",
      "Epoch 102 Loss 223.8060 Current Train,Val,Test Scores [5548.60498046875, 5536.55712890625, 1470.5894775390625]\n",
      "Epoch 103 Loss 207.6743 Current Train,Val,Test Scores [1685.5018310546875, 1673.61328125, 448.70599365234375]\n",
      "Epoch 104 Loss 194.5644 Current Train,Val,Test Scores [37.26347732543945, 48.29238510131836, 15.710806846618652]\n",
      "Epoch 105 Loss 183.4074 Current Train,Val,Test Scores [41.865562438964844, 52.899139404296875, 16.83311653137207]\n",
      "Epoch 106 Loss 168.6015 Current Train,Val,Test Scores [21.17357635498047, 34.204261779785156, 10.959575653076172]\n",
      "Epoch 107 Loss 162.8774 Current Train,Val,Test Scores [24.76446533203125, 37.78603744506836, 11.911839485168457]\n",
      "Epoch 108 Loss 154.0822 Current Train,Val,Test Scores [133.09036254882812, 140.0408172607422, 41.3280029296875]\n",
      "Epoch 109 Loss 153.3103 Current Train,Val,Test Scores [307.30621337890625, 301.82977294921875, 87.53326416015625]\n",
      "Epoch 110 Loss 149.1817 Current Train,Val,Test Scores [319.6827697753906, 313.11572265625, 90.74360656738281]\n",
      "Epoch 111 Loss 146.9632 Current Train,Val,Test Scores [143.2360382080078, 149.5117950439453, 44.067108154296875]\n",
      "Epoch 112 Loss 145.0510 Current Train,Val,Test Scores [61.8049430847168, 74.82866668701172, 21.68817710876465]\n",
      "Epoch 113 Loss 140.7655 Current Train,Val,Test Scores [26.1181697845459, 36.95001983642578, 12.734991073608398]\n",
      "Epoch 114 Loss 133.8697 Current Train,Val,Test Scores [157.20433044433594, 162.7807159423828, 47.77861785888672]\n",
      "Epoch 115 Loss 128.1195 Current Train,Val,Test Scores [120.0408935546875, 128.0966339111328, 37.70561599731445]\n",
      "Epoch 116 Loss 126.8891 Current Train,Val,Test Scores [40.92319869995117, 52.01504135131836, 16.491161346435547]\n",
      "Epoch 117 Loss 117.7582 Current Train,Val,Test Scores [36.12873840332031, 49.12199020385742, 14.828519821166992]\n",
      "Epoch 118 Loss 114.2612 Current Train,Val,Test Scores [32.046077728271484, 45.04462432861328, 13.737951278686523]\n",
      "Epoch 119 Loss 113.2306 Current Train,Val,Test Scores [30.516565322875977, 41.7066764831543, 13.796939849853516]\n",
      "Epoch 120 Loss 117.3170 Current Train,Val,Test Scores [24.16337776184082, 34.97228240966797, 12.07796859741211]\n",
      "Epoch 121 Loss 132.0432 Current Train,Val,Test Scores [594.573486328125, 588.520263671875, 159.03578186035156]\n",
      "Epoch 122 Loss 149.2126 Current Train,Val,Test Scores [27.52849769592285, 40.574615478515625, 12.518288612365723]\n",
      "Epoch 123 Loss 208.5579 Current Train,Val,Test Scores [505.0820617675781, 494.13275146484375, 134.84922790527344]\n",
      "Epoch 124 Loss 175.6993 Current Train,Val,Test Scores [43.742156982421875, 56.792240142822266, 16.79808807373047]\n",
      "Epoch 125 Loss 149.1899 Current Train,Val,Test Scores [29.65896224975586, 42.60136413574219, 13.08261775970459]\n",
      "Epoch 126 Loss 100.3240 Current Train,Val,Test Scores [27.817386627197266, 40.75638198852539, 12.590679168701172]\n",
      "Epoch 127 Loss 109.3903 Current Train,Val,Test Scores [51.44837951660156, 64.5243911743164, 18.80681037902832]\n",
      "Epoch 128 Loss 147.6973 Current Train,Val,Test Scores [25.92511749267578, 38.81696319580078, 12.088640213012695]\n",
      "Epoch 129 Loss 116.8369 Current Train,Val,Test Scores [30.645322799682617, 43.62224578857422, 13.321717262268066]\n",
      "Epoch 130 Loss 91.4939 Current Train,Val,Test Scores [26.54574203491211, 39.538848876953125, 12.226407051086426]\n",
      "Epoch 131 Loss 106.9900 Current Train,Val,Test Scores [19.485883712768555, 30.60565185546875, 10.640266418457031]\n",
      "Epoch 132 Loss 112.8885 Current Train,Val,Test Scores [17.629074096679688, 29.698118209838867, 10.057942390441895]\n",
      "Epoch 133 Loss 97.6838 Current Train,Val,Test Scores [19.92584991455078, 30.820690155029297, 10.753066062927246]\n",
      "Epoch 134 Loss 86.6058 Current Train,Val,Test Scores [43.53299331665039, 55.19829559326172, 17.031877517700195]\n",
      "Epoch 135 Loss 97.0035 Current Train,Val,Test Scores [18.461702346801758, 29.84891128540039, 10.339425086975098]\n",
      "Epoch 136 Loss 101.1297 Current Train,Val,Test Scores [694.6819458007812, 681.8821411132812, 185.4202423095703]\n",
      "Epoch 137 Loss 85.2583 Current Train,Val,Test Scores [1752.7041015625, 1741.4239501953125, 465.8379211425781]\n",
      "Epoch 138 Loss 84.7024 Current Train,Val,Test Scores [835.9182739257812, 820.0316162109375, 222.43142700195312]\n",
      "Epoch 139 Loss 94.4852 Current Train,Val,Test Scores [4242.771484375, 4245.986328125, 1124.9881591796875]\n",
      "Epoch 140 Loss 90.9098 Current Train,Val,Test Scores [2269.97705078125, 2268.02783203125, 601.476318359375]\n",
      "Epoch 141 Loss 81.5079 Current Train,Val,Test Scores [2852.987548828125, 2852.322509765625, 756.3154296875]\n",
      "Epoch 142 Loss 79.4354 Current Train,Val,Test Scores [5208.7021484375, 5211.33349609375, 1380.432861328125]\n",
      "Epoch 143 Loss 85.8926 Current Train,Val,Test Scores [2138.926513671875, 2135.77880859375, 566.7310791015625]\n",
      "Epoch 144 Loss 89.0608 Current Train,Val,Test Scores [5187.021484375, 5189.54443359375, 1374.9300537109375]\n",
      "Epoch 145 Loss 81.0922 Current Train,Val,Test Scores [3716.362548828125, 3718.08447265625, 985.7055053710938]\n",
      "Epoch 146 Loss 75.1474 Current Train,Val,Test Scores [3326.085205078125, 3326.941162109375, 882.1151733398438]\n",
      "Epoch 147 Loss 76.0356 Current Train,Val,Test Scores [5220.859375, 5221.3037109375, 1383.2232666015625]\n",
      "Epoch 148 Loss 80.5668 Current Train,Val,Test Scores [1764.465576171875, 1759.746826171875, 468.93023681640625]\n",
      "Epoch 149 Loss 84.1858 Current Train,Val,Test Scores [4369.3583984375, 4368.345703125, 1156.4566650390625]\n",
      "Epoch 150 Loss 80.7640 Current Train,Val,Test Scores [1402.259521484375, 1391.8983154296875, 372.36822509765625]\n",
      "Epoch 151 Loss 75.7864 Current Train,Val,Test Scores [2450.87890625, 2441.560791015625, 649.4633178710938]\n",
      "Epoch 152 Loss 71.5216 Current Train,Val,Test Scores [1633.623779296875, 1623.4854736328125, 433.15777587890625]\n",
      "Epoch 153 Loss 70.0628 Current Train,Val,Test Scores [514.2197265625, 502.1500244140625, 136.6541290283203]\n",
      "Epoch 154 Loss 71.1756 Current Train,Val,Test Scores [1832.3560791015625, 1822.46484375, 485.1732482910156]\n",
      "Epoch 155 Loss 72.9049 Current Train,Val,Test Scores [40.64071273803711, 52.11439895629883, 16.839393615722656]\n",
      "Epoch 156 Loss 75.1102 Current Train,Val,Test Scores [1846.1492919921875, 1834.451904296875, 488.3761901855469]\n",
      "Epoch 157 Loss 75.3564 Current Train,Val,Test Scores [14.96496295928955, 28.01516342163086, 9.199520111083984]\n",
      "Epoch 158 Loss 76.3780 Current Train,Val,Test Scores [1008.1177368164062, 995.677001953125, 266.1259765625]\n",
      "Epoch 159 Loss 75.9796 Current Train,Val,Test Scores [20.70382308959961, 33.568363189697266, 10.488512992858887]\n",
      "Epoch 160 Loss 77.4862 Current Train,Val,Test Scores [466.8616943359375, 454.6816711425781, 123.99193572998047]\n",
      "Epoch 161 Loss 77.0497 Current Train,Val,Test Scores [27.59227752685547, 40.37636947631836, 12.280038833618164]\n",
      "Epoch 162 Loss 78.4451 Current Train,Val,Test Scores [46.372615814208984, 58.945953369140625, 17.398473739624023]\n",
      "Epoch 163 Loss 77.1272 Current Train,Val,Test Scores [42.52072525024414, 55.10346603393555, 16.264530181884766]\n",
      "Epoch 164 Loss 76.3882 Current Train,Val,Test Scores [17.793867111206055, 30.645071029663086, 9.753228187561035]\n",
      "Epoch 165 Loss 72.6991 Current Train,Val,Test Scores [44.35655212402344, 56.839935302734375, 16.734050750732422]\n",
      "Epoch 166 Loss 69.2351 Current Train,Val,Test Scores [27.778915405273438, 40.315975189208984, 12.311617851257324]\n",
      "Epoch 167 Loss 65.0936 Current Train,Val,Test Scores [33.86439514160156, 46.29402160644531, 13.905692100524902]\n",
      "Epoch 168 Loss 62.3036 Current Train,Val,Test Scores [32.759395599365234, 45.22821807861328, 13.608027458190918]\n",
      "Epoch 169 Loss 61.3933 Current Train,Val,Test Scores [30.187923431396484, 42.7031135559082, 12.921456336975098]\n",
      "Epoch 170 Loss 62.0057 Current Train,Val,Test Scores [37.22402572631836, 49.649898529052734, 14.799858093261719]\n",
      "Epoch 171 Loss 62.9690 Current Train,Val,Test Scores [24.696638107299805, 37.260711669921875, 11.453418731689453]\n",
      "Epoch 172 Loss 63.3999 Current Train,Val,Test Scores [26.892013549804688, 39.36729431152344, 12.045693397521973]\n",
      "Epoch 173 Loss 63.3911 Current Train,Val,Test Scores [21.431241989135742, 33.9785041809082, 10.571499824523926]\n",
      "Epoch 174 Loss 62.1941 Current Train,Val,Test Scores [26.67896270751953, 39.15393829345703, 11.967127799987793]\n",
      "Epoch 175 Loss 61.0263 Current Train,Val,Test Scores [26.326587677001953, 38.87603759765625, 11.860830307006836]\n",
      "Epoch 176 Loss 59.3029 Current Train,Val,Test Scores [33.310943603515625, 45.78492736816406, 13.712569236755371]\n",
      "Epoch 177 Loss 57.6888 Current Train,Val,Test Scores [31.610248565673828, 44.11958312988281, 13.248923301696777]\n",
      "Epoch 178 Loss 56.4551 Current Train,Val,Test Scores [34.21855163574219, 46.683685302734375, 13.932452201843262]\n",
      "Epoch 179 Loss 55.6997 Current Train,Val,Test Scores [38.02497482299805, 50.436710357666016, 14.959046363830566]\n",
      "Epoch 180 Loss 55.1581 Current Train,Val,Test Scores [41.02019119262695, 53.462398529052734, 15.733505249023438]\n",
      "Epoch 181 Loss 54.8591 Current Train,Val,Test Scores [52.10749816894531, 64.56580352783203, 18.68035888671875]\n",
      "Epoch 182 Loss 54.6965 Current Train,Val,Test Scores [38.386356353759766, 50.876434326171875, 15.013547897338867]\n",
      "Epoch 183 Loss 55.1873 Current Train,Val,Test Scores [48.0798225402832, 60.52073287963867, 17.593233108520508]\n",
      "Epoch 184 Loss 57.7243 Current Train,Val,Test Scores [29.821420669555664, 42.32795715332031, 12.724164009094238]\n",
      "Epoch 185 Loss 65.2388 Current Train,Val,Test Scores [57.838050842285156, 70.34850311279297, 20.178604125976562]\n",
      "Epoch 186 Loss 95.6722 Current Train,Val,Test Scores [22.747222900390625, 35.27517318725586, 10.840224266052246]\n",
      "Epoch 187 Loss 139.7865 Current Train,Val,Test Scores [51.528526306152344, 64.10555267333984, 18.48117446899414]\n",
      "Epoch 188 Loss 287.3142 Current Train,Val,Test Scores [47.842105865478516, 59.410247802734375, 17.781530380249023]\n",
      "Epoch 189 Loss 99.5980 Current Train,Val,Test Scores [23.164817810058594, 34.36575698852539, 11.322710037231445]\n",
      "Epoch 190 Loss 62.0541 Current Train,Val,Test Scores [145.54014587402344, 158.30030822753906, 43.35445785522461]\n",
      "Epoch 191 Loss 112.7835 Current Train,Val,Test Scores [240.85787963867188, 253.56935119628906, 68.59015655517578]\n",
      "Epoch 192 Loss 105.7234 Current Train,Val,Test Scores [173.99087524414062, 186.79164123535156, 50.88420104980469]\n",
      "Epoch 193 Loss 71.7187 Current Train,Val,Test Scores [34.78670883178711, 47.426910400390625, 14.044054985046387]\n",
      "Epoch 194 Loss 59.1402 Current Train,Val,Test Scores [83.32112884521484, 93.77197265625, 27.115863800048828]\n",
      "Epoch 195 Loss 78.5691 Current Train,Val,Test Scores [74.72515106201172, 85.44410705566406, 24.82464027404785]\n",
      "Epoch 196 Loss 81.2198 Current Train,Val,Test Scores [57.060367584228516, 68.31810760498047, 20.193988800048828]\n",
      "Epoch 197 Loss 51.8183 Current Train,Val,Test Scores [46.93782424926758, 58.19684600830078, 17.531219482421875]\n",
      "Epoch 198 Loss 84.0475 Current Train,Val,Test Scores [38.076839447021484, 49.42277908325195, 15.272441864013672]\n",
      "Epoch 199 Loss 84.9875 Current Train,Val,Test Scores [18.28221321105957, 30.10724449157715, 9.931790351867676]\n",
      "Epoch 200 Loss 54.3318 Current Train,Val,Test Scores [1857.37890625, 1821.9434814453125, 491.1094970703125]\n",
      "Epoch 201 Loss 82.1111 Current Train,Val,Test Scores [19.516555786132812, 30.79796028137207, 10.303336143493652]\n",
      "Epoch 202 Loss 89.8680 Current Train,Val,Test Scores [18.60485076904297, 29.830442428588867, 9.943787574768066]\n",
      "Epoch 203 Loss 54.0400 Current Train,Val,Test Scores [1346.1478271484375, 1309.77001953125, 356.3028869628906]\n",
      "Epoch 204 Loss 102.2301 Current Train,Val,Test Scores [43.801536560058594, 55.07381057739258, 16.692577362060547]\n",
      "Epoch 205 Loss 85.9400 Current Train,Val,Test Scores [63.36600875854492, 74.48296356201172, 21.839191436767578]\n",
      "Epoch 206 Loss 59.7239 Current Train,Val,Test Scores [88.00020599365234, 98.37413787841797, 28.402172088623047]\n",
      "Epoch 207 Loss 99.9593 Current Train,Val,Test Scores [87.56932067871094, 97.88471984863281, 28.233184814453125]\n",
      "Epoch 208 Loss 71.6544 Current Train,Val,Test Scores [61.817447662353516, 72.96898651123047, 21.39019012451172]\n",
      "Epoch 209 Loss 68.1952 Current Train,Val,Test Scores [62.34123229980469, 73.70158386230469, 21.60626792907715]\n",
      "Epoch 210 Loss 85.1473 Current Train,Val,Test Scores [45.33138656616211, 56.68861770629883, 17.030704498291016]\n",
      "Epoch 211 Loss 49.9952 Current Train,Val,Test Scores [48.4397087097168, 59.61539840698242, 17.86809539794922]\n",
      "Epoch 212 Loss 70.1463 Current Train,Val,Test Scores [257.1190185546875, 238.44363403320312, 71.31137084960938]\n",
      "Epoch 213 Loss 55.9043 Current Train,Val,Test Scores [845.591552734375, 807.7340698242188, 222.08993530273438]\n",
      "Epoch 214 Loss 57.8169 Current Train,Val,Test Scores [90.41132354736328, 100.49394989013672, 29.858795166015625]\n",
      "Epoch 215 Loss 60.1080 Current Train,Val,Test Scores [588.3378295898438, 550.1495361328125, 155.5743408203125]\n",
      "Epoch 216 Loss 51.7132 Current Train,Val,Test Scores [1623.4678955078125, 1584.07568359375, 428.3661804199219]\n",
      "Epoch 217 Loss 55.7127 Current Train,Val,Test Scores [72.76610565185547, 86.15096282958984, 24.822877883911133]\n",
      "Epoch 218 Loss 54.9914 Current Train,Val,Test Scores [59.07571029663086, 70.39811706542969, 20.42011833190918]\n",
      "Epoch 219 Loss 50.0296 Current Train,Val,Test Scores [88.49386596679688, 100.58354187011719, 29.468276977539062]\n",
      "Epoch 220 Loss 58.0042 Current Train,Val,Test Scores [38.35495376586914, 51.14064025878906, 15.375566482543945]\n",
      "Epoch 221 Loss 46.5838 Current Train,Val,Test Scores [26.882566452026367, 38.93285369873047, 12.289132118225098]\n",
      "Epoch 222 Loss 53.4061 Current Train,Val,Test Scores [86.75714111328125, 92.48350524902344, 28.35350227355957]\n",
      "Epoch 223 Loss 47.3863 Current Train,Val,Test Scores [434.6320495605469, 407.7189636230469, 115.5096206665039]\n",
      "Epoch 224 Loss 49.4794 Current Train,Val,Test Scores [332.6588439941406, 304.712158203125, 88.41045379638672]\n",
      "Epoch 225 Loss 48.6917 Current Train,Val,Test Scores [1808.4459228515625, 1769.406982421875, 477.0614318847656]\n",
      "Epoch 226 Loss 46.9497 Current Train,Val,Test Scores [2970.7177734375, 2932.302978515625, 785.0126342773438]\n",
      "Epoch 227 Loss 48.0627 Current Train,Val,Test Scores [1698.6275634765625, 1655.3475341796875, 448.60235595703125]\n",
      "Epoch 228 Loss 47.1323 Current Train,Val,Test Scores [1749.6043701171875, 1707.4854736328125, 461.9444274902344]\n",
      "Epoch 229 Loss 45.6236 Current Train,Val,Test Scores [2667.98779296875, 2628.3779296875, 704.7914428710938]\n",
      "Epoch 230 Loss 47.6092 Current Train,Val,Test Scores [1014.7830810546875, 974.96826171875, 266.8699645996094]\n",
      "Epoch 231 Loss 44.1441 Current Train,Val,Test Scores [95.0575942993164, 92.7027359008789, 29.367938995361328]\n",
      "Epoch 232 Loss 46.2558 Current Train,Val,Test Scores [115.658935546875, 108.0427017211914, 34.15314865112305]\n",
      "Epoch 233 Loss 44.5359 Current Train,Val,Test Scores [41.284332275390625, 53.06916809082031, 15.75044059753418]\n",
      "Epoch 234 Loss 44.6785 Current Train,Val,Test Scores [17.77961540222168, 29.652864456176758, 9.711363792419434]\n",
      "Epoch 235 Loss 44.3835 Current Train,Val,Test Scores [23.601032257080078, 36.01051330566406, 10.989243507385254]\n",
      "Epoch 236 Loss 44.3124 Current Train,Val,Test Scores [45.10203552246094, 57.39310073852539, 16.65035057067871]\n",
      "Epoch 237 Loss 43.1272 Current Train,Val,Test Scores [72.32156372070312, 84.48979187011719, 23.88392448425293]\n",
      "Epoch 238 Loss 44.4596 Current Train,Val,Test Scores [112.99248504638672, 125.18818664550781, 34.64281463623047]\n",
      "Epoch 239 Loss 42.5778 Current Train,Val,Test Scores [147.7794189453125, 160.01686096191406, 43.848838806152344]\n",
      "Epoch 240 Loss 43.5772 Current Train,Val,Test Scores [171.87400817871094, 184.07225036621094, 50.24052047729492]\n",
      "Epoch 241 Loss 42.9109 Current Train,Val,Test Scores [187.65098571777344, 200.07516479492188, 54.44899368286133]\n",
      "Epoch 242 Loss 42.5698 Current Train,Val,Test Scores [202.77011108398438, 215.8181610107422, 58.49573516845703]\n",
      "Epoch 243 Loss 42.6162 Current Train,Val,Test Scores [222.31471252441406, 234.57359313964844, 63.588218688964844]\n",
      "Epoch 244 Loss 42.6259 Current Train,Val,Test Scores [234.21942138671875, 246.5611114501953, 66.75020599365234]\n",
      "Epoch 245 Loss 41.8171 Current Train,Val,Test Scores [238.92445373535156, 251.21868896484375, 67.98468780517578]\n",
      "Epoch 246 Loss 42.3235 Current Train,Val,Test Scores [239.04336547851562, 251.1092987060547, 67.9999771118164]\n",
      "Epoch 247 Loss 41.8523 Current Train,Val,Test Scores [237.00233459472656, 248.9687957763672, 67.44680786132812]\n",
      "Epoch 248 Loss 41.4151 Current Train,Val,Test Scores [238.4322967529297, 250.33836364746094, 67.81773376464844]\n",
      "Epoch 249 Loss 41.8140 Current Train,Val,Test Scores [265.1379699707031, 277.1741638183594, 74.9166488647461]\n",
      "Epoch 250 Loss 41.3215 Current Train,Val,Test Scores [284.7144775390625, 296.7508850097656, 80.10572814941406]\n",
      "Epoch 251 Loss 41.1176 Current Train,Val,Test Scores [301.25506591796875, 313.1212463378906, 84.44879150390625]\n",
      "Epoch 252 Loss 41.1311 Current Train,Val,Test Scores [328.1649169921875, 339.9681091308594, 91.56359100341797]\n",
      "Epoch 253 Loss 40.9668 Current Train,Val,Test Scores [344.6824951171875, 356.47796630859375, 95.93865203857422]\n",
      "Epoch 254 Loss 40.5518 Current Train,Val,Test Scores [359.3425598144531, 371.4189453125, 99.8509521484375]\n",
      "Epoch 255 Loss 40.8209 Current Train,Val,Test Scores [374.0079650878906, 386.1116943359375, 103.75395965576172]\n",
      "Epoch 256 Loss 40.4587 Current Train,Val,Test Scores [340.6101989746094, 352.6482849121094, 94.93538665771484]\n",
      "Epoch 257 Loss 40.2174 Current Train,Val,Test Scores [275.11309814453125, 287.537109375, 77.63436126708984]\n",
      "Epoch 258 Loss 40.2418 Current Train,Val,Test Scores [224.18490600585938, 236.24098205566406, 64.14503479003906]\n",
      "Epoch 259 Loss 40.0488 Current Train,Val,Test Scores [166.39988708496094, 179.6053466796875, 49.04692840576172]\n",
      "Epoch 260 Loss 39.7735 Current Train,Val,Test Scores [91.5186767578125, 106.17682647705078, 30.081195831298828]\n",
      "Epoch 261 Loss 39.7854 Current Train,Val,Test Scores [44.18094253540039, 59.75432205200195, 17.142915725708008]\n",
      "Epoch 262 Loss 39.6442 Current Train,Val,Test Scores [23.72478675842285, 29.76522445678711, 12.36446475982666]\n",
      "Epoch 263 Loss 39.4225 Current Train,Val,Test Scores [109.2425537109375, 89.87235260009766, 31.581083297729492]\n",
      "Epoch 264 Loss 39.2852 Current Train,Val,Test Scores [51.47700500488281, 42.56636428833008, 17.87452507019043]\n",
      "Epoch 265 Loss 39.2634 Current Train,Val,Test Scores [173.65399169921875, 155.03619384765625, 47.77610778808594]\n",
      "Epoch 266 Loss 39.0018 Current Train,Val,Test Scores [104.40563201904297, 92.40917205810547, 29.975183486938477]\n",
      "Epoch 267 Loss 38.8925 Current Train,Val,Test Scores [65.68663024902344, 78.4110336303711, 24.370956420898438]\n",
      "Epoch 268 Loss 38.8215 Current Train,Val,Test Scores [72.22740936279297, 76.52899932861328, 25.51441764831543]\n",
      "Epoch 269 Loss 38.6912 Current Train,Val,Test Scores [90.18765258789062, 107.1707992553711, 29.645845413208008]\n",
      "Epoch 270 Loss 38.5519 Current Train,Val,Test Scores [56.353763580322266, 71.93267059326172, 20.12225914001465]\n",
      "Epoch 271 Loss 38.4176 Current Train,Val,Test Scores [22.092880249023438, 38.24393844604492, 11.431917190551758]\n",
      "Epoch 272 Loss 38.2751 Current Train,Val,Test Scores [22.440998077392578, 38.474422454833984, 11.544139862060547]\n",
      "Epoch 273 Loss 38.2535 Current Train,Val,Test Scores [25.959646224975586, 31.372705459594727, 13.509817123413086]\n",
      "Epoch 274 Loss 38.1271 Current Train,Val,Test Scores [14.908870697021484, 34.12477111816406, 11.15205192565918]\n",
      "Epoch 275 Loss 37.9711 Current Train,Val,Test Scores [53.57643127441406, 43.996063232421875, 18.45075225830078]\n",
      "Epoch 276 Loss 37.8271 Current Train,Val,Test Scores [42.09529113769531, 57.45441436767578, 18.455907821655273]\n",
      "Epoch 277 Loss 37.6847 Current Train,Val,Test Scores [74.81977081298828, 90.95136260986328, 26.029264450073242]\n",
      "Epoch 278 Loss 37.5425 Current Train,Val,Test Scores [66.01557159423828, 79.52479553222656, 22.693784713745117]\n",
      "Epoch 279 Loss 37.4629 Current Train,Val,Test Scores [26.66559410095215, 39.52665328979492, 12.217129707336426]\n",
      "Epoch 280 Loss 37.3170 Current Train,Val,Test Scores [28.15755844116211, 41.087989807128906, 12.635252952575684]\n",
      "Epoch 281 Loss 37.2095 Current Train,Val,Test Scores [29.456756591796875, 42.3797607421875, 12.978636741638184]\n",
      "Epoch 282 Loss 37.1387 Current Train,Val,Test Scores [25.977262496948242, 38.86883544921875, 12.055437088012695]\n",
      "Epoch 283 Loss 37.0611 Current Train,Val,Test Scores [28.646160125732422, 41.531803131103516, 12.763158798217773]\n",
      "Epoch 284 Loss 37.0106 Current Train,Val,Test Scores [27.5972957611084, 40.50605392456055, 12.50613021850586]\n",
      "Epoch 285 Loss 37.1871 Current Train,Val,Test Scores [32.991817474365234, 46.0326042175293, 13.935373306274414]\n",
      "Epoch 286 Loss 37.9985 Current Train,Val,Test Scores [13.666670799255371, 31.99481773376465, 11.279336929321289]\n",
      "Epoch 287 Loss 39.0997 Current Train,Val,Test Scores [24.1701602935791, 37.18681335449219, 11.608675003051758]\n",
      "Epoch 288 Loss 41.8524 Current Train,Val,Test Scores [112.4796142578125, 93.76123046875, 32.252017974853516]\n",
      "Epoch 289 Loss 40.1997 Current Train,Val,Test Scores [26.643983840942383, 39.874961853027344, 12.14709186553955]\n",
      "Epoch 290 Loss 39.3240 Current Train,Val,Test Scores [18.460853576660156, 31.179418563842773, 10.17072868347168]\n",
      "Epoch 291 Loss 36.7005 Current Train,Val,Test Scores [17.37513542175293, 29.937496185302734, 9.867671966552734]\n",
      "Epoch 292 Loss 36.7657 Current Train,Val,Test Scores [56.01932144165039, 68.9342041015625, 19.908058166503906]\n",
      "Epoch 293 Loss 38.4717 Current Train,Val,Test Scores [17.771469116210938, 31.090932846069336, 10.333203315734863]\n",
      "Epoch 294 Loss 38.4715 Current Train,Val,Test Scores [18.084373474121094, 30.926069259643555, 10.122580528259277]\n",
      "Epoch 295 Loss 37.6922 Current Train,Val,Test Scores [19.908456802368164, 30.934370040893555, 10.580476760864258]\n",
      "Epoch 296 Loss 35.8549 Current Train,Val,Test Scores [22.0200252532959, 33.10177993774414, 11.19876480102539]\n",
      "Epoch 297 Loss 35.4980 Current Train,Val,Test Scores [22.374225616455078, 33.44211196899414, 11.339334487915039]\n",
      "Epoch 298 Loss 35.9801 Current Train,Val,Test Scores [27.11347198486328, 38.66755294799805, 12.876813888549805]\n",
      "Epoch 299 Loss 36.3737 Current Train,Val,Test Scores [21.18973159790039, 32.2537841796875, 11.00642204284668]\n",
      "Epoch 300 Loss 37.3025 Current Train,Val,Test Scores [26.90517234802246, 38.29473114013672, 12.644282341003418]\n",
      "Epoch 301 Loss 37.0236 Current Train,Val,Test Scores [17.26707649230957, 29.883203506469727, 9.94771671295166]\n",
      "Epoch 302 Loss 37.1924 Current Train,Val,Test Scores [18.518606185913086, 31.27130699157715, 10.35200309753418]\n",
      "Epoch 303 Loss 35.9782 Current Train,Val,Test Scores [53.69499588012695, 66.27925109863281, 19.435794830322266]\n",
      "Epoch 304 Loss 35.3700 Current Train,Val,Test Scores [96.74179077148438, 109.00205993652344, 30.797285079956055]\n",
      "Epoch 305 Loss 34.5488 Current Train,Val,Test Scores [49.018463134765625, 61.2447395324707, 18.177202224731445]\n",
      "Epoch 306 Loss 34.8689 Current Train,Val,Test Scores [18.179153442382812, 29.981834411621094, 10.250244140625]\n",
      "Epoch 307 Loss 35.4507 Current Train,Val,Test Scores [25.48122787475586, 36.62839889526367, 12.330180168151855]\n",
      "Epoch 308 Loss 35.2220 Current Train,Val,Test Scores [24.68732261657715, 35.80677032470703, 12.099809646606445]\n",
      "Epoch 309 Loss 35.4100 Current Train,Val,Test Scores [26.400650024414062, 38.06033706665039, 13.790215492248535]\n",
      "Epoch 310 Loss 35.6936 Current Train,Val,Test Scores [86.6672592163086, 98.74576568603516, 28.094825744628906]\n",
      "Epoch 311 Loss 37.2391 Current Train,Val,Test Scores [37.286746978759766, 51.88196563720703, 15.877211570739746]\n",
      "Epoch 312 Loss 37.0742 Current Train,Val,Test Scores [23.36101722717285, 34.345272064208984, 11.7499418258667]\n",
      "Epoch 313 Loss 38.6823 Current Train,Val,Test Scores [45.3980712890625, 57.12965393066406, 17.22477912902832]\n",
      "Epoch 314 Loss 37.1448 Current Train,Val,Test Scores [137.6721649169922, 150.20147705078125, 41.66239547729492]\n",
      "Epoch 315 Loss 35.3217 Current Train,Val,Test Scores [148.56973266601562, 160.68399047851562, 44.544368743896484]\n",
      "Epoch 316 Loss 33.4315 Current Train,Val,Test Scores [34.175628662109375, 46.17819595336914, 14.348132133483887]\n",
      "Epoch 317 Loss 33.7456 Current Train,Val,Test Scores [43.698402404785156, 54.9329719543457, 17.117216110229492]\n",
      "Epoch 318 Loss 34.8096 Current Train,Val,Test Scores [80.61569213867188, 90.63812255859375, 26.419734954833984]\n",
      "Epoch 319 Loss 35.0532 Current Train,Val,Test Scores [43.10661315917969, 54.248043060302734, 16.952457427978516]\n",
      "Epoch 320 Loss 36.0878 Current Train,Val,Test Scores [66.4389419555664, 77.95001220703125, 23.065704345703125]\n",
      "Epoch 321 Loss 35.6505 Current Train,Val,Test Scores [40.9773063659668, 52.15549850463867, 16.416440963745117]\n",
      "Epoch 322 Loss 35.8275 Current Train,Val,Test Scores [51.891536712646484, 63.77440643310547, 19.46483612060547]\n",
      "Epoch 323 Loss 34.2689 Current Train,Val,Test Scores [44.814876556396484, 55.75908660888672, 17.411746978759766]\n",
      "Epoch 324 Loss 33.5539 Current Train,Val,Test Scores [41.796207427978516, 53.77659225463867, 17.036060333251953]\n",
      "Epoch 325 Loss 32.7332 Current Train,Val,Test Scores [122.43592071533203, 134.85800170898438, 37.7063102722168]\n",
      "Epoch 326 Loss 31.9220 Current Train,Val,Test Scores [50.6331672668457, 61.76570510864258, 18.934602737426758]\n",
      "Epoch 327 Loss 31.7961 Current Train,Val,Test Scores [73.34159088134766, 84.1618881225586, 24.839433670043945]\n",
      "Epoch 328 Loss 32.1457 Current Train,Val,Test Scores [59.79050064086914, 71.0124282836914, 21.401325225830078]\n",
      "Epoch 329 Loss 32.3178 Current Train,Val,Test Scores [42.629390716552734, 54.94990158081055, 17.340194702148438]\n",
      "Epoch 330 Loss 32.4478 Current Train,Val,Test Scores [109.65162658691406, 122.20922088623047, 34.265567779541016]\n",
      "Epoch 331 Loss 33.4332 Current Train,Val,Test Scores [87.53437805175781, 97.79209899902344, 28.77285385131836]\n",
      "Epoch 332 Loss 34.6726 Current Train,Val,Test Scores [44.228675842285156, 55.177146911621094, 17.225187301635742]\n",
      "Epoch 333 Loss 40.8523 Current Train,Val,Test Scores [131.81515502929688, 138.3893280029297, 40.42613220214844]\n",
      "Epoch 334 Loss 43.1128 Current Train,Val,Test Scores [442.8512878417969, 455.384033203125, 122.39156341552734]\n",
      "Epoch 335 Loss 56.9068 Current Train,Val,Test Scores [70.20377349853516, 81.23395538330078, 23.94952964782715]\n",
      "Epoch 336 Loss 38.6819 Current Train,Val,Test Scores [27.960172653198242, 38.72072982788086, 13.069441795349121]\n",
      "Epoch 337 Loss 36.8177 Current Train,Val,Test Scores [189.55264282226562, 202.06863403320312, 55.43641662597656]\n",
      "Epoch 338 Loss 38.2142 Current Train,Val,Test Scores [124.71455383300781, 134.6629638671875, 38.45936584472656]\n",
      "Epoch 339 Loss 41.2563 Current Train,Val,Test Scores [46.77876663208008, 59.283390045166016, 17.64649200439453]\n",
      "Epoch 340 Loss 43.0851 Current Train,Val,Test Scores [100.04061126708984, 109.67990112304688, 31.956748962402344]\n",
      "Epoch 341 Loss 30.8302 Current Train,Val,Test Scores [140.13536071777344, 151.5452423095703, 43.272499084472656]\n",
      "Epoch 342 Loss 38.9137 Current Train,Val,Test Scores [218.9506072998047, 231.11190795898438, 63.054386138916016]\n",
      "Epoch 343 Loss 45.0880 Current Train,Val,Test Scores [210.31475830078125, 212.87408447265625, 61.69203186035156]\n",
      "Epoch 344 Loss 33.4954 Current Train,Val,Test Scores [107.18718719482422, 115.6326675415039, 34.164024353027344]\n",
      "Epoch 345 Loss 39.6565 Current Train,Val,Test Scores [57.279022216796875, 67.9166488647461, 20.589370727539062]\n",
      "Epoch 346 Loss 36.1093 Current Train,Val,Test Scores [77.53268432617188, 87.9068832397461, 25.906030654907227]\n",
      "Epoch 347 Loss 35.3815 Current Train,Val,Test Scores [86.60169982910156, 99.44200134277344, 28.218891143798828]\n",
      "Epoch 348 Loss 47.0709 Current Train,Val,Test Scores [44.92715835571289, 55.84832000732422, 17.252410888671875]\n",
      "Epoch 349 Loss 29.9636 Current Train,Val,Test Scores [77.78048706054688, 88.16162109375, 26.0410213470459]\n",
      "Epoch 350 Loss 39.6194 Current Train,Val,Test Scores [234.61163330078125, 246.8456268310547, 67.2826919555664]\n",
      "Epoch 351 Loss 47.7225 Current Train,Val,Test Scores [84.64513397216797, 95.417724609375, 27.906293869018555]\n",
      "Epoch 352 Loss 45.3608 Current Train,Val,Test Scores [73.39927673339844, 84.01493072509766, 24.873933792114258]\n",
      "Epoch 353 Loss 43.1587 Current Train,Val,Test Scores [176.3893585205078, 180.40724182128906, 52.25727462768555]\n",
      "Epoch 354 Loss 33.1780 Current Train,Val,Test Scores [315.05975341796875, 306.8916931152344, 88.16890716552734]\n",
      "Epoch 355 Loss 47.5656 Current Train,Val,Test Scores [182.1546173095703, 185.57203674316406, 53.795143127441406]\n",
      "Epoch 356 Loss 40.3170 Current Train,Val,Test Scores [364.2803955078125, 353.7770080566406, 101.40702056884766]\n",
      "Epoch 357 Loss 42.2160 Current Train,Val,Test Scores [291.42913818359375, 287.370361328125, 82.47535705566406]\n",
      "Epoch 358 Loss 39.1253 Current Train,Val,Test Scores [243.9175262451172, 242.19223022460938, 69.76374053955078]\n",
      "Epoch 359 Loss 30.9556 Current Train,Val,Test Scores [276.2518310546875, 271.2933044433594, 78.31298065185547]\n",
      "Epoch 360 Loss 42.6943 Current Train,Val,Test Scores [193.8159942626953, 197.8740692138672, 57.0104866027832]\n",
      "Epoch 361 Loss 38.7928 Current Train,Val,Test Scores [337.96990966796875, 328.6780700683594, 94.08851623535156]\n",
      "Epoch 362 Loss 42.7483 Current Train,Val,Test Scores [102.30986022949219, 110.69536590576172, 32.75970458984375]\n",
      "Epoch 363 Loss 33.6266 Current Train,Val,Test Scores [110.43364715576172, 122.71495819091797, 34.42506790161133]\n",
      "Epoch 364 Loss 31.7117 Current Train,Val,Test Scores [584.546630859375, 595.6834106445312, 159.547119140625]\n",
      "Epoch 365 Loss 35.0508 Current Train,Val,Test Scores [666.2947998046875, 678.7236328125, 181.27667236328125]\n",
      "Epoch 366 Loss 31.4752 Current Train,Val,Test Scores [361.7953796386719, 351.2093200683594, 100.52684020996094]\n",
      "Epoch 367 Loss 33.5010 Current Train,Val,Test Scores [378.80712890625, 367.2621765136719, 104.87254333496094]\n",
      "Epoch 368 Loss 27.1358 Current Train,Val,Test Scores [287.201416015625, 281.2486572265625, 80.74443817138672]\n",
      "Epoch 369 Loss 30.7386 Current Train,Val,Test Scores [182.78750610351562, 183.72604370117188, 53.27799987792969]\n",
      "Epoch 370 Loss 28.0245 Current Train,Val,Test Scores [91.63128662109375, 100.66777801513672, 29.630634307861328]\n",
      "Epoch 371 Loss 29.2259 Current Train,Val,Test Scores [32.7225341796875, 44.93619918823242, 14.028562545776367]\n",
      "Epoch 372 Loss 27.9146 Current Train,Val,Test Scores [489.5083312988281, 502.3419189453125, 134.69508361816406]\n",
      "Epoch 373 Loss 25.4631 Current Train,Val,Test Scores [1142.2550048828125, 1155.2894287109375, 307.2707214355469]\n",
      "Epoch 374 Loss 27.7389 Current Train,Val,Test Scores [41.247657775878906, 54.29903030395508, 16.251649856567383]\n",
      "Epoch 375 Loss 25.6736 Current Train,Val,Test Scores [149.12863159179688, 154.5920867919922, 44.64785385131836]\n",
      "Epoch 376 Loss 27.9530 Current Train,Val,Test Scores [167.28143310546875, 170.35708618164062, 49.29887390136719]\n",
      "Epoch 377 Loss 26.2431 Current Train,Val,Test Scores [74.82038116455078, 84.99049377441406, 24.752960205078125]\n",
      "Epoch 378 Loss 25.5871 Current Train,Val,Test Scores [194.36911010742188, 195.7692413330078, 56.29694747924805]\n",
      "Epoch 379 Loss 25.0615 Current Train,Val,Test Scores [212.26724243164062, 213.48460388183594, 61.25341796875]\n",
      "Epoch 380 Loss 24.0153 Current Train,Val,Test Scores [412.7635803222656, 426.1030578613281, 114.30901336669922]\n",
      "Epoch 381 Loss 23.3609 Current Train,Val,Test Scores [812.6642456054688, 825.3633422851562, 220.0728302001953]\n",
      "Epoch 382 Loss 23.8352 Current Train,Val,Test Scores [145.70152282714844, 158.94520568847656, 43.55447769165039]\n",
      "Epoch 383 Loss 22.7134 Current Train,Val,Test Scores [337.25616455078125, 328.92926025390625, 93.81593322753906]\n",
      "Epoch 384 Loss 22.5816 Current Train,Val,Test Scores [301.9498596191406, 295.6155700683594, 84.43898010253906]\n",
      "Epoch 385 Loss 22.7270 Current Train,Val,Test Scores [303.5094909667969, 297.4944763183594, 84.80933380126953]\n",
      "Epoch 386 Loss 22.3543 Current Train,Val,Test Scores [222.5888671875, 234.805419921875, 63.63878631591797]\n",
      "Epoch 387 Loss 21.9237 Current Train,Val,Test Scores [781.0159912109375, 792.6323852539062, 211.40069580078125]\n",
      "Epoch 388 Loss 22.1175 Current Train,Val,Test Scores [1013.4659423828125, 1026.2025146484375, 272.9888610839844]\n",
      "Epoch 389 Loss 23.8197 Current Train,Val,Test Scores [342.7830810546875, 331.2223205566406, 94.47419738769531]\n",
      "Epoch 390 Loss 27.4289 Current Train,Val,Test Scores [292.7768859863281, 287.6457214355469, 81.95661163330078]\n",
      "Epoch 391 Loss 42.8767 Current Train,Val,Test Scores [555.81689453125, 546.1630859375, 149.75494384765625]\n",
      "Epoch 392 Loss 58.4831 Current Train,Val,Test Scores [51.661014556884766, 62.935394287109375, 18.131990432739258]\n",
      "Epoch 393 Loss 93.0319 Current Train,Val,Test Scores [40.930137634277344, 54.416316986083984, 16.352449417114258]\n",
      "Epoch 394 Loss 36.2280 Current Train,Val,Test Scores [171.0196075439453, 152.1597442626953, 44.80972671508789]\n",
      "Epoch 395 Loss 45.5866 Current Train,Val,Test Scores [436.5944519042969, 464.2950439453125, 122.59634399414062]\n",
      "Epoch 396 Loss 80.1106 Current Train,Val,Test Scores [75.1893539428711, 86.2347412109375, 24.132841110229492]\n",
      "Epoch 397 Loss 72.6444 Current Train,Val,Test Scores [36.0699577331543, 47.62129211425781, 13.980269432067871]\n",
      "Epoch 398 Loss 64.3773 Current Train,Val,Test Scores [51.51774215698242, 63.51224136352539, 17.942747116088867]\n",
      "Epoch 399 Loss 48.3634 Current Train,Val,Test Scores [86.82775115966797, 98.7647476196289, 28.044498443603516]\n",
      "Epoch 400 Loss 47.7313 Current Train,Val,Test Scores [261.9982604980469, 274.39837646484375, 74.49100494384766]\n",
      "Epoch 401 Loss 68.6200 Current Train,Val,Test Scores [29.457561492919922, 41.590248107910156, 12.891517639160156]\n",
      "Epoch 402 Loss 37.7605 Current Train,Val,Test Scores [350.82867431640625, 342.417236328125, 97.27669525146484]\n",
      "Epoch 403 Loss 46.8200 Current Train,Val,Test Scores [183.30711364746094, 188.13705444335938, 53.52912521362305]\n",
      "Epoch 404 Loss 43.0951 Current Train,Val,Test Scores [317.4997863769531, 318.57879638671875, 89.9243392944336]\n",
      "Epoch 405 Loss 51.7387 Current Train,Val,Test Scores [358.548828125, 354.1170654296875, 100.1026840209961]\n",
      "Epoch 406 Loss 45.2027 Current Train,Val,Test Scores [127.72233581542969, 136.21185302734375, 39.10463333129883]\n",
      "Epoch 407 Loss 32.9629 Current Train,Val,Test Scores [85.27783966064453, 97.3133544921875, 27.82965850830078]\n",
      "Epoch 408 Loss 34.3838 Current Train,Val,Test Scores [208.66259765625, 220.84698486328125, 60.47328186035156]\n",
      "Epoch 409 Loss 39.1736 Current Train,Val,Test Scores [54.40842056274414, 66.21875762939453, 19.573942184448242]\n",
      "Epoch 410 Loss 34.5226 Current Train,Val,Test Scores [40.915531158447266, 52.260799407958984, 15.370750427246094]\n",
      "Epoch 411 Loss 33.1565 Current Train,Val,Test Scores [18.363046646118164, 30.541929244995117, 9.533150672912598]\n",
      "Epoch 412 Loss 27.7492 Current Train,Val,Test Scores [86.38848876953125, 99.73712158203125, 28.600296020507812]\n",
      "Epoch 413 Loss 33.8988 Current Train,Val,Test Scores [839.8458251953125, 804.4530029296875, 222.46841430664062]\n",
      "Epoch 414 Loss 30.7769 Current Train,Val,Test Scores [699.9046020507812, 671.6810913085938, 185.8685760498047]\n",
      "Epoch 415 Loss 30.6654 Current Train,Val,Test Scores [114.01908874511719, 128.7709197998047, 35.78395080566406]\n",
      "Epoch 416 Loss 27.0662 Current Train,Val,Test Scores [18.34415054321289, 30.5376033782959, 9.441446304321289]\n",
      "Epoch 417 Loss 30.8376 Current Train,Val,Test Scores [31.48973846435547, 43.091758728027344, 13.054838180541992]\n",
      "Epoch 418 Loss 26.5906 Current Train,Val,Test Scores [21.722904205322266, 33.84490203857422, 11.136310577392578]\n",
      "Epoch 419 Loss 26.0587 Current Train,Val,Test Scores [94.34754943847656, 106.01998138427734, 30.280595779418945]\n",
      "Epoch 420 Loss 25.7781 Current Train,Val,Test Scores [129.86622619628906, 141.54388427734375, 39.725181579589844]\n",
      "Epoch 421 Loss 27.1325 Current Train,Val,Test Scores [88.05244445800781, 102.0842056274414, 28.55550193786621]\n",
      "Epoch 422 Loss 24.2802 Current Train,Val,Test Scores [345.4418640136719, 324.8543701171875, 89.35415649414062]\n",
      "Epoch 423 Loss 23.4472 Current Train,Val,Test Scores [1071.2066650390625, 1055.9840087890625, 279.9588928222656]\n",
      "Epoch 424 Loss 35.4657 Current Train,Val,Test Scores [2464.89306640625, 2453.105224609375, 648.7418212890625]\n",
      "Epoch 425 Loss 42.4994 Current Train,Val,Test Scores [4894.26708984375, 4881.6494140625, 1293.7618408203125]\n",
      "Epoch 426 Loss 28.9864 Current Train,Val,Test Scores [5750.35595703125, 5737.8505859375, 1520.3614501953125]\n",
      "Epoch 427 Loss 32.4645 Current Train,Val,Test Scores [4891.14794921875, 4879.43798828125, 1292.4132080078125]\n",
      "Epoch 428 Loss 27.9605 Current Train,Val,Test Scores [4605.65576171875, 4595.48291015625, 1215.927490234375]\n",
      "Epoch 429 Loss 30.2555 Current Train,Val,Test Scores [3726.890869140625, 3716.5068359375, 982.3246459960938]\n",
      "Epoch 430 Loss 24.5256 Current Train,Val,Test Scores [2218.584716796875, 2208.34814453125, 582.5633544921875]\n",
      "Epoch 431 Loss 29.5984 Current Train,Val,Test Scores [1473.4320068359375, 1463.6795654296875, 385.70159912109375]\n",
      "Epoch 432 Loss 28.2658 Current Train,Val,Test Scores [1073.5086669921875, 1062.09423828125, 280.11224365234375]\n",
      "Epoch 433 Loss 24.3299 Current Train,Val,Test Scores [1361.9443359375, 1350.100341796875, 356.6945495605469]\n",
      "Epoch 434 Loss 26.9101 Current Train,Val,Test Scores [1463.9747314453125, 1452.531494140625, 383.89422607421875]\n",
      "Epoch 435 Loss 23.6063 Current Train,Val,Test Scores [1255.691162109375, 1242.54833984375, 329.1198425292969]\n",
      "Epoch 436 Loss 25.1952 Current Train,Val,Test Scores [1240.1051025390625, 1211.145751953125, 327.245361328125]\n",
      "Epoch 437 Loss 24.8773 Current Train,Val,Test Scores [149.29180908203125, 166.88812255859375, 47.19721221923828]\n",
      "Epoch 438 Loss 21.5610 Current Train,Val,Test Scores [312.14605712890625, 325.79150390625, 88.4930648803711]\n",
      "Epoch 439 Loss 23.9960 Current Train,Val,Test Scores [386.2309875488281, 399.9208984375, 108.1572036743164]\n",
      "Epoch 440 Loss 22.3903 Current Train,Val,Test Scores [409.10772705078125, 422.5721435546875, 114.3311767578125]\n",
      "Epoch 441 Loss 21.5847 Current Train,Val,Test Scores [452.94329833984375, 466.5714416503906, 126.06878662109375]\n",
      "Epoch 442 Loss 22.1277 Current Train,Val,Test Scores [504.8836975097656, 520.12255859375, 139.84466552734375]\n",
      "Epoch 443 Loss 20.4279 Current Train,Val,Test Scores [587.0870971679688, 612.7467041015625, 162.9423065185547]\n",
      "Epoch 444 Loss 21.2890 Current Train,Val,Test Scores [624.4810180664062, 654.1561279296875, 173.81385803222656]\n",
      "Epoch 445 Loss 19.9525 Current Train,Val,Test Scores [762.6307983398438, 790.0697631835938, 209.83116149902344]\n",
      "Epoch 446 Loss 20.0005 Current Train,Val,Test Scores [836.5009765625, 861.9893188476562, 228.6698455810547]\n",
      "Epoch 447 Loss 19.8662 Current Train,Val,Test Scores [687.8547973632812, 709.3151245117188, 188.15113830566406]\n",
      "Epoch 448 Loss 19.0390 Current Train,Val,Test Scores [706.0708618164062, 726.1758422851562, 192.44058227539062]\n",
      "Epoch 449 Loss 19.1443 Current Train,Val,Test Scores [710.6883544921875, 730.0473022460938, 193.70428466796875]\n",
      "Epoch 450 Loss 18.7375 Current Train,Val,Test Scores [585.723388671875, 603.94873046875, 162.6566162109375]\n",
      "Epoch 451 Loss 18.6850 Current Train,Val,Test Scores [773.8181762695312, 791.90966796875, 210.13804626464844]\n",
      "Epoch 452 Loss 18.3242 Current Train,Val,Test Scores [843.7478637695312, 861.0065307617188, 228.1358642578125]\n",
      "Epoch 453 Loss 17.7820 Current Train,Val,Test Scores [977.66064453125, 996.1483154296875, 263.9013977050781]\n",
      "Epoch 454 Loss 18.0027 Current Train,Val,Test Scores [1417.48974609375, 1434.673583984375, 380.3379821777344]\n",
      "Epoch 455 Loss 17.6443 Current Train,Val,Test Scores [1349.4156494140625, 1363.6251220703125, 363.7887878417969]\n",
      "Epoch 456 Loss 17.3657 Current Train,Val,Test Scores [875.66259765625, 891.7867431640625, 239.92889404296875]\n",
      "Epoch 457 Loss 17.1648 Current Train,Val,Test Scores [627.0691528320312, 644.7987670898438, 172.9883270263672]\n",
      "Epoch 458 Loss 16.8924 Current Train,Val,Test Scores [470.5039367675781, 485.684326171875, 133.58106994628906]\n",
      "Epoch 459 Loss 16.8917 Current Train,Val,Test Scores [486.9906005859375, 503.3526916503906, 137.2444305419922]\n",
      "Epoch 460 Loss 16.5194 Current Train,Val,Test Scores [722.5516357421875, 740.4068603515625, 197.8577423095703]\n",
      "Epoch 461 Loss 16.2733 Current Train,Val,Test Scores [998.766357421875, 1016.162109375, 270.41571044921875]\n",
      "Epoch 462 Loss 16.1833 Current Train,Val,Test Scores [1109.1246337890625, 1126.5850830078125, 298.6342468261719]\n",
      "Epoch 463 Loss 15.9797 Current Train,Val,Test Scores [965.5620727539062, 983.3189697265625, 260.31884765625]\n",
      "Epoch 464 Loss 15.6717 Current Train,Val,Test Scores [825.5664672851562, 842.5231323242188, 222.6454620361328]\n",
      "Epoch 465 Loss 15.6086 Current Train,Val,Test Scores [812.809814453125, 829.750732421875, 219.19204711914062]\n",
      "Epoch 466 Loss 15.3997 Current Train,Val,Test Scores [857.1762084960938, 874.6943969726562, 230.9862060546875]\n",
      "Epoch 467 Loss 15.2416 Current Train,Val,Test Scores [974.9561157226562, 992.3414916992188, 262.2154846191406]\n",
      "Epoch 468 Loss 15.0759 Current Train,Val,Test Scores [878.3460083007812, 895.6907348632812, 236.6423797607422]\n",
      "Epoch 469 Loss 14.8298 Current Train,Val,Test Scores [733.8160400390625, 750.7120971679688, 198.32730102539062]\n",
      "Epoch 470 Loss 14.7450 Current Train,Val,Test Scores [696.57421875, 713.3959350585938, 188.47787475585938]\n",
      "Epoch 471 Loss 14.6224 Current Train,Val,Test Scores [664.7274169921875, 681.6056518554688, 180.05075073242188]\n",
      "Epoch 472 Loss 14.3549 Current Train,Val,Test Scores [655.659912109375, 672.9076538085938, 177.69451904296875]\n",
      "Epoch 473 Loss 14.1930 Current Train,Val,Test Scores [670.4342041015625, 687.5861206054688, 181.60324096679688]\n",
      "Epoch 474 Loss 14.0332 Current Train,Val,Test Scores [569.3756713867188, 584.1396484375, 154.97833251953125]\n",
      "Epoch 475 Loss 13.7921 Current Train,Val,Test Scores [513.3012084960938, 527.2716064453125, 140.16635131835938]\n",
      "Epoch 476 Loss 13.6321 Current Train,Val,Test Scores [462.5960693359375, 476.4946594238281, 126.75481414794922]\n",
      "Epoch 477 Loss 13.5184 Current Train,Val,Test Scores [426.02117919921875, 440.51446533203125, 117.05064392089844]\n",
      "Epoch 478 Loss 13.3666 Current Train,Val,Test Scores [381.0532531738281, 395.117919921875, 105.1933364868164]\n",
      "Epoch 479 Loss 13.1965 Current Train,Val,Test Scores [320.0367126464844, 334.2876281738281, 89.02279663085938]\n",
      "Epoch 480 Loss 13.0681 Current Train,Val,Test Scores [283.68475341796875, 298.18829345703125, 79.40827178955078]\n",
      "Epoch 481 Loss 12.9341 Current Train,Val,Test Scores [264.316162109375, 278.6727600097656, 74.25054168701172]\n",
      "Epoch 482 Loss 12.6967 Current Train,Val,Test Scores [246.19540405273438, 260.558837890625, 69.4225082397461]\n",
      "Epoch 483 Loss 12.4716 Current Train,Val,Test Scores [208.410400390625, 222.4966583251953, 59.35792541503906]\n",
      "Epoch 484 Loss 12.2609 Current Train,Val,Test Scores [199.6796875, 213.65167236328125, 57.01100540161133]\n",
      "Epoch 485 Loss 12.6691 Current Train,Val,Test Scores [166.5800323486328, 180.29702758789062, 48.19196319580078]\n",
      "Epoch 486 Loss 13.2473 Current Train,Val,Test Scores [272.6773986816406, 287.345703125, 76.44334411621094]\n",
      "Epoch 487 Loss 13.2476 Current Train,Val,Test Scores [252.6731719970703, 267.0365295410156, 71.25559997558594]\n",
      "Epoch 488 Loss 13.8201 Current Train,Val,Test Scores [189.27552795410156, 203.0124969482422, 54.600799560546875]\n",
      "Epoch 489 Loss 15.3426 Current Train,Val,Test Scores [232.0932159423828, 246.32318115234375, 65.9830322265625]\n",
      "Epoch 490 Loss 16.4901 Current Train,Val,Test Scores [367.039794921875, 382.3871154785156, 101.94248962402344]\n",
      "Epoch 491 Loss 19.9117 Current Train,Val,Test Scores [339.8487243652344, 357.40887451171875, 94.20889282226562]\n",
      "Epoch 492 Loss 25.4416 Current Train,Val,Test Scores [259.5265808105469, 275.792724609375, 73.19778442382812]\n",
      "Epoch 493 Loss 28.5547 Current Train,Val,Test Scores [181.41787719726562, 198.35899353027344, 52.20223617553711]\n",
      "Epoch 494 Loss 37.4932 Current Train,Val,Test Scores [417.6005554199219, 435.1549072265625, 115.11773681640625]\n",
      "Epoch 495 Loss 37.3384 Current Train,Val,Test Scores [228.3221435546875, 245.08966064453125, 64.4231185913086]\n",
      "Epoch 496 Loss 37.4332 Current Train,Val,Test Scores [277.8388671875, 295.9082946777344, 77.63807678222656]\n",
      "Epoch 497 Loss 22.5040 Current Train,Val,Test Scores [684.3659057617188, 702.2432250976562, 185.69493103027344]\n",
      "Epoch 498 Loss 12.5855 Current Train,Val,Test Scores [593.3357543945312, 611.7278442382812, 162.0747528076172]\n",
      "Epoch 499 Loss 20.0262 Current Train,Val,Test Scores [245.78668212890625, 263.3693542480469, 70.17221069335938]\n",
      "Best Train,Val,Test Scores [14.96496295928955, 28.01516342163086, 9.199520111083984]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creates a HeteroGNN model from the previously constructed hetero graph and trains it.\n",
    "\"\"\"\n",
    "\n",
    "best_model = None\n",
    "best_tvt_scores = (float(\"inf\"),float(\"inf\"),float(\"inf\"))\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    # Train\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    # Test for the accuracy of the model\n",
    "    cur_tvt_scores, best_tvt_scores, best_model = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_tvt_scores)\n",
    "    print(f\"Epoch {epoch} Loss {loss:.4f} Current Train,Val,Test Scores {[score.item() for score in cur_tvt_scores]}\")\n",
    "\n",
    "print(\"Best Train,Val,Test Scores\", [score.item() for score in best_tvt_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Score: 27.796846389770508\n",
      "tensor([5.4196], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([6.6848], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([113.8790], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([13.4972], device='cuda:0', grad_fn=<SelectBackward0>) tensor([94.], device='cuda:0')\n",
      "tensor([159.8505], device='cuda:0', grad_fn=<SelectBackward0>) tensor([48.], device='cuda:0')\n",
      "tensor([10.2517], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([210.1763], device='cuda:0', grad_fn=<SelectBackward0>) tensor([111.], device='cuda:0')\n",
      "tensor([6.9525], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([9.7624], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([3.3793], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([8.1168], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([7.4244], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([6.5812], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([13.7725], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([5.0211], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.4440], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([8.7366], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([6.6658], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([14.5443], device='cuda:0', grad_fn=<SelectBackward0>) tensor([58.], device='cuda:0')\n",
      "tensor([5.8443], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([5.6624], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.4335], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([8.7706], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([17.5087], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.6897], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([7.1575], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([12.3165], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([8.3295], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([127.6720], device='cuda:0', grad_fn=<SelectBackward0>) tensor([32.], device='cuda:0')\n",
      "tensor([8.5569], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([9.6141], device='cuda:0', grad_fn=<SelectBackward0>) tensor([19.], device='cuda:0')\n",
      "tensor([10.7454], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([15.5242], device='cuda:0', grad_fn=<SelectBackward0>) tensor([144.], device='cuda:0')\n",
      "tensor([96.6329], device='cuda:0', grad_fn=<SelectBackward0>) tensor([79.], device='cuda:0')\n",
      "tensor([6.6782], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([5.9421], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([9.3092], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([6.1768], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([5.8904], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([10.2667], device='cuda:0', grad_fn=<SelectBackward0>) tensor([51.], device='cuda:0')\n",
      "tensor([6.5926], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([5.4267], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([65.8458], device='cuda:0', grad_fn=<SelectBackward0>) tensor([40.], device='cuda:0')\n",
      "tensor([9.6650], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([5.3782], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([198.5020], device='cuda:0', grad_fn=<SelectBackward0>) tensor([607.], device='cuda:0')\n",
      "tensor([6.4949], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([6.7239], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([48.1061], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "model.load_state_dict(torch.load('./best_model.pkl'))\n",
    "preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "# mask = preds['event'] > 0\n",
    "# preds['event'][preds['event'] > 0].shape\n",
    "\n",
    "# print(preds['event'][0], hetero_graph.node_target['event'][0]) \n",
    "\n",
    "\n",
    "#print(hetero_graph.node_feature['event'])\n",
    "\n",
    "# Assuming val_idx is the dictionary containing the validation indices for 'event' node type\n",
    "\n",
    "# TODO: Best validation score doesn't match the best validation score from the training loop\n",
    "# best_val = torch.sum(torch.abs(preds['event'][val_idx['event']] - hetero_graph.node_target['event'][val_idx['event']])) / val_idx['event'].shape[0]\n",
    "cur_tvt_scores, best_tvt_scores, best_model = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_tvt_scores)\n",
    "\n",
    "\n",
    "# Print the validation loss\n",
    "print(f\"Best Validation Score: {cur_tvt_scores[1].item()}\")\n",
    "\n",
    "# for i in range(3000):\n",
    "#     if hetero_graph.node_target['event'][i] != -1: # concepts have node target -1\n",
    "#         print(preds['event'][i], hetero_graph.node_target['event'][i])\n",
    "        \n",
    "    \n",
    "for i in range(1000):    \n",
    "    if hetero_graph.node_target['event'][test_idx['event']][i] != -1:\n",
    "        print(preds['event'][test_idx['event']][i], hetero_graph.node_target['event'][test_idx['event']][i])\n",
    "\n",
    "\n",
    "# for i in range(1000):\n",
    "#     # Extract the target value and check if it is not equal to -1\n",
    "#     target = hetero_graph.node_target['event'][test_idx['event']][i].item()\n",
    "#     if target != -1:\n",
    "#         # Extract the prediction value\n",
    "#         pred = preds['event'][test_idx['event']][i].item()\n",
    "#         print(f\"Prediction: {pred:.4f}, Target: {target:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2023-11-16 19:24:41,988] A new study created in memory with name: no-name-a9486f72-8c8d-4b6e-871e-d613a7ac1dc9\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamgrobelnik\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\Heterogeneus\\wandb\\run-20231116_192445-qp9hpx2l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qp9hpx2l' target=\"_blank\">fiery-dew-1</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qp9hpx2l' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qp9hpx2l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>██▇▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂▁▁▁▆▃▂▁▁▂▃▇▂▂█▁▄▁▁▇▁▃▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>20.93139</td></tr><tr><td>epoch</td><td>234</td></tr><tr><td>train_loss</td><td>60.27063</td></tr><tr><td>val_score</td><td>2397.53101</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-dew-1</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qp9hpx2l' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qp9hpx2l</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231116_192445-qp9hpx2l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-16 19:25:20,680] Trial 0 finished with value: 20.9313907623291 and parameters: {'lr': 0.0020225681861146304, 'weight_decay': 9.684740233016735e-05, 'hidden_size': 92, 'epochs': 235}. Best is trial 0 with value: 20.9313907623291.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\Heterogeneus\\wandb\\run-20231116_192520-qnrfo5ay</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qnrfo5ay' target=\"_blank\">woven-rain-2</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qnrfo5ay' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qnrfo5ay</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>█▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>███▇▇▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▃▂▄▄▃▄▂▂▅▆▃▂▂▂▆▁▃▅█▇▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>29.49602</td></tr><tr><td>epoch</td><td>222</td></tr><tr><td>train_loss</td><td>362.57751</td></tr><tr><td>val_score</td><td>326.71277</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">woven-rain-2</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qnrfo5ay' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qnrfo5ay</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231116_192520-qnrfo5ay\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-16 19:25:54,121] Trial 1 finished with value: 29.496017456054688 and parameters: {'lr': 0.0005514224960203741, 'weight_decay': 0.0003127735255466916, 'hidden_size': 84, 'epochs': 223}. Best is trial 0 with value: 20.9313907623291.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\Heterogeneus\\wandb\\run-20231116_192554-p9s9o4wk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/p9s9o4wk' target=\"_blank\">earthy-cloud-3</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/p9s9o4wk' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/p9s9o4wk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>█▆▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>███▇▇▇▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁▂▂▄▃▅▅█▇▄▄▅▄▂▃▂▃▅▁▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>27.50316</td></tr><tr><td>epoch</td><td>152</td></tr><tr><td>train_loss</td><td>577.40967</td></tr><tr><td>val_score</td><td>734.14221</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earthy-cloud-3</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/p9s9o4wk' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/p9s9o4wk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231116_192554-p9s9o4wk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-16 19:26:25,863] Trial 2 finished with value: 27.503164291381836 and parameters: {'lr': 0.0010050133630073253, 'weight_decay': 0.00015970709494371755, 'hidden_size': 74, 'epochs': 153}. Best is trial 0 with value: 20.9313907623291.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\Heterogeneus\\wandb\\run-20231116_192625-qrjj8zbr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qrjj8zbr' target=\"_blank\">fragrant-wind-4</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qrjj8zbr' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qrjj8zbr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>██████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>██▇▆▆▅▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▆█▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>28.3522</td></tr><tr><td>epoch</td><td>151</td></tr><tr><td>train_loss</td><td>53.5307</td></tr><tr><td>val_score</td><td>50.36627</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-wind-4</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qrjj8zbr' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/qrjj8zbr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231116_192625-qrjj8zbr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-16 19:26:58,407] Trial 3 finished with value: 28.352201461791992 and parameters: {'lr': 0.004532300090119492, 'weight_decay': 0.0004141326546776573, 'hidden_size': 62, 'epochs': 152}. Best is trial 0 with value: 20.9313907623291.\n",
      "[W 2023-11-16 19:27:00,724] Trial 4 failed with parameters: {'lr': 2.4244644502810926e-05, 'weight_decay': 0.0006158170455386171, 'hidden_size': 19, 'epochs': 271} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Adrian\\AppData\\Local\\Temp\\ipykernel_5708\\1583648187.py\", line 6, in objective\n",
      "    wandb.init(project=\"V2_MLG_PredEvents_GNN+LMM\", config={\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 1189, in init\n",
      "    raise e\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 1166, in init\n",
      "    run = wi.init()\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 752, in init\n",
      "    result = run_init_handle.wait(\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 283, in wait\n",
      "    found, abandoned = self._slot._get_and_clear(timeout=wait_timeout)\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 130, in _get_and_clear\n",
      "    if self._wait(timeout=timeout):\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 126, in _wait\n",
      "    return self._event.wait(timeout=timeout)\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\threading.py\", line 607, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\threading.py\", line 324, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "[W 2023-11-16 19:27:00,727] Trial 4 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: C:\\Users\\Adrian\\AppData\\Local\\Temp\\ipykernel_5708\\1583648187.py 6 objective\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Initialize wandb run\n",
    "    wandb.init(project=\"V2_MLG_PredEvents_GNN+LMM\", config={\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True),\n",
    "        \"hidden_size\": trial.suggest_int(\"hidden_size\", 16, 128),\n",
    "        \"attn_size\": 32,  # Fixed value\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 150, 300),\n",
    "        \"num_layers\": 2,  # Fixed value\n",
    "    })\n",
    "\n",
    "    # Use wandb config\n",
    "    config = wandb.config\n",
    "\n",
    "    # Initialize the model with the new hyperparameters\n",
    "    model = HeteroGNN(hetero_graph, {\n",
    "        'hidden_size': config.hidden_size,\n",
    "        'attn_size': config.attn_size,\n",
    "        'device': args['device']\n",
    "    }, num_layers=config.num_layers, aggr=\"mean\").to(args['device'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "    # Initialize best scores with infinity\n",
    "    best_tvt_scores = (float(\"inf\"), float(\"inf\"), float(\"inf\"))\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "        cur_tvt_scores, best_tvt_scores, _ = test(model, hetero_graph, [train_idx, val_idx, test_idx], None, best_tvt_scores)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_score\": cur_tvt_scores[1],\n",
    "            \"best_val_score\": best_tvt_scores[1],\n",
    "        })\n",
    "\n",
    "        # Update the best validation score\n",
    "        if cur_tvt_scores[1] < best_tvt_scores[1]:\n",
    "            best_tvt_scores = (cur_tvt_scores[0], cur_tvt_scores[1], cur_tvt_scores[2])\n",
    "\n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "    # The objective value is the best validation score\n",
    "    return best_tvt_scores[1]\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=500)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"Value: {trial.value}\")\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
