{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        self.lin_dst = nn.Linear(in_channels_dst, out_channels)\n",
    "        self.lin_src = nn.Linear(in_channels_src, out_channels)\n",
    "        self.lin_update = nn.Linear(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "        ):\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        \"\"\"\n",
    "        Initializes the HeteroGNNWrapperConv instance.\n",
    "\n",
    "        :param convs: Dictionary of convolution layers for each message type.\n",
    "        :param args: Arguments dictionary containing hyperparameters like hidden_size and attn_size.\n",
    "        :param aggr: Aggregation method, defaults to 'mean'.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_features: Dictionary of node features for each node type.\n",
    "        :param edge_indices: Dictionary of edge indices for each message type.\n",
    "        :return: Aggregated node embeddings for each node type.\n",
    "        \"\"\"\n",
    "        \n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        \n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        \n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "                \n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        \"\"\"\n",
    "        Aggregates node embeddings using the specified aggregation method.\n",
    "\n",
    "        :param xs: List of node embeddings to aggregate.\n",
    "        :return: Aggregated node embeddings as a torch.Tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates convolutional layers for each message type in a heterogeneous graph.\n",
    "\n",
    "    :param hetero_graph: The heterogeneous graph for which convolutions are to be created.\n",
    "    :param conv: The convolutional layer class or constructor.\n",
    "    :param hidden_size: The number of features in the hidden layer.\n",
    "    :param first_layer: Boolean indicating if this is the first layer in the network.\n",
    "    \n",
    "    :return: A dictionary of convolutional layers, keyed by message type.\n",
    "    \"\"\"\n",
    "\n",
    "    convs = {}\n",
    "    \n",
    "    # Extracting all types of messages/edges in the heterogeneous graph.\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        \n",
    "        # Determine the input feature size for source and destination nodes.\n",
    "        # If it's the first layer, use the feature size of the nodes.\n",
    "        # Otherwise, use the hidden size, since from there on the size of embeddings\n",
    "        # is the same for all nodes.\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        \n",
    "        # Create a convolutional layer for this message type and add it to the dictionary.\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, num_layers, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "        \n",
    "        # Initialize the graph convolutional layers\n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        # Initialize batch normalization, ReLU, and fully connected layers for each node type\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            \n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            \n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "            \n",
    "    def forward(self, node_feature, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_feature: Dictionary of node features for each node type.\n",
    "        :param edge_index: Dictionary of edge indices for each message type.\n",
    "        :return: The output embeddings for each node type after passing through the model.\n",
    "        \"\"\"\n",
    "        x = node_feature\n",
    "\n",
    "        # Apply graph convolutional, batch normalization, and ReLU layers\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        x = forward_op(x, self.fc)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        \"\"\"\n",
    "        Computes the loss for the model.\n",
    "\n",
    "        :param preds: Predictions made by the model.\n",
    "        :param y: Ground truth target values.\n",
    "        :param indices: Indices of nodes for which loss should be calculated.\n",
    "        \n",
    "        :return: The computed loss value.\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = 0\n",
    "        loss_func = torch.nn.MSELoss() \n",
    "             \n",
    "        mask = y['event'][indices['event'], 0] != -1\n",
    "        non_zero_idx = torch.masked_select(indices['event'], mask)\n",
    "                \n",
    "        loss += loss_func(preds['event'][non_zero_idx], y['event'][non_zero_idx])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    \"\"\"\n",
    "    Trains the model on the given heterogeneous graph using the specified indices.\n",
    "\n",
    "    :param model: The graph neural network model to train.\n",
    "    :param optimizer: The optimizer used for training the model.\n",
    "    :param hetero_graph: The heterogeneous graph data.\n",
    "    :param train_idx: Indices for training nodes.\n",
    "\n",
    "    :return: The training loss as a float.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "    optimizer.zero_grad() # Zero out any existing gradients \n",
    "    \n",
    "    # Compute predictions using the model\n",
    "    # TODO: Use only train_idx instead of edge_index\n",
    "    # TODO: Train only on events not on concepts\n",
    "    \n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    # Compute the loss using model's loss function\n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "\n",
    "    loss.backward() # Backward pass: compute gradient of the loss\n",
    "    optimizer.step() # Perform a single optimization step, updates parameters\n",
    "    \n",
    "    return loss.item() \n",
    "\n",
    "def test(model, graph, indices, best_model, best_tvt_scores):\n",
    "    \"\"\"\n",
    "    Tests the model on given indices and updates the best model based on validation loss.\n",
    "\n",
    "    :param model: The trained graph neural network model.\n",
    "    :param graph: The heterogeneous graph data.\n",
    "    :param indices: List of indices for training, validation, and testing nodes.\n",
    "    :param best_model: The current best model based on validation loss.\n",
    "    :param best_val: The current best validation loss.\n",
    "    \n",
    "    :return: A tuple containing the list of losses for each dataset, the best model, and the best validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    tvt_scores = []\n",
    "    \n",
    "    # Evaluate the model on each set of indices\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        \n",
    "        idx = index['event']\n",
    "        \n",
    "                     \n",
    "        # mask = y['event'][indices['event'], 0] != -1\n",
    "        # non_zero_idx = torch.masked_select(indices['event'], mask)\n",
    "        #preds['event'][non_zero_idx], y['event'][non_zero_idx]\n",
    "        \n",
    "        # non_zero_targets = torch.masked_select(graph.node_target['event'][indices['event']], mask)\n",
    "        # non_zero_truth = torch.masked_select(graph.node_target['event'][indices['event']], mask)\n",
    "        \n",
    "        mask = graph.node_target['event'][idx, 0] != -1\n",
    "        non_zero_idx = torch.masked_select(idx, mask)\n",
    "        \n",
    "        \n",
    "        L1 = torch.sum(torch.abs(preds['event'][non_zero_idx] - graph.node_target['event'][non_zero_idx])) / non_zero_idx.shape[0]\n",
    "        \n",
    "        tvt_scores.append(L1)\n",
    "    \n",
    "    # Update the best model and validation loss if the current model performs better\n",
    "    if tvt_scores[1] < best_tvt_scores[1]:\n",
    "        best_tvt_scores = tvt_scores\n",
    "        # torch.to_pickle(model, 'best_model.pkl')\n",
    "        # model.to_pickle('best_model.pkl')\n",
    "        # best_model = copy.deepcopy(model)\n",
    "        torch.save(model.state_dict(), './best_model.pkl')\n",
    "    \n",
    "    return tvt_scores, best_tvt_scores, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 48,\n",
    "    'epochs': 500,\n",
    "    'weight_decay': 0.0002930387278908051,\n",
    "    'lr': 0.05091434725288385,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell creates a small heterogeneous graph, primarily for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [1, 1, 1],   # event 0\n",
    "                [2, 2, 2]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([[50], [2000]], dtype=torch.float32),\n",
    "    # \"concept\": torch.tensor([[0], [0]], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}\n",
    "\n",
    "# Testing\n",
    "hetero_graph = HeteroGraph(\n",
    "    node_feature=S_node_feature,\n",
    "    node_target=S_node_targets,\n",
    "    edge_index=S_edge_index\n",
    ")\n",
    "\n",
    "train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./1_concepts_similar_llm.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "    # Convert to directed graph for compatibility with Deepsnap\n",
    "    # G = G.to_directed()\n",
    "   \n",
    "        \n",
    "hetero_graph = HeteroGraph(G, netlib=nx, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 769\n",
      "\t Feature 769\n",
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 769\n",
      "\t Feature 1\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 1\n",
      "\t Feature 769\n",
      "KEY ('event', 'similar', 'event') <class 'tuple'>\n",
      "KEY NUMS ('event', 'similar', 'event') 8487 8487\n",
      "MAX EDGES tensor(8283) tensor(8283) 8487 8487\n",
      "KEY ('event', 'related', 'concept') <class 'tuple'>\n",
      "KEY NUMS ('event', 'related', 'concept') 8487 8729\n",
      "MAX EDGES tensor(8265) tensor(8728) 8487 8729\n",
      "KEY ('concept', 'related', 'event') <class 'tuple'>\n",
      "KEY NUMS ('concept', 'related', 'event') 8729 8487\n",
      "MAX EDGES tensor(8728) tensor(8265) 8729 8487\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block ensures that all tensors in a heterogeneous graph are transferred to the same \n",
    "computational device, as specified in the 'args' dictionary.\n",
    "\"\"\"\n",
    "\n",
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "# Send node features to device\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "# Create a torch.SparseTensor from edge_index and send it to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key, type(key))\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    \n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max(), hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
    "    \n",
    "# Send node targets to device\n",
    "for key in hetero_graph.node_target:\n",
    "    hetero_graph.node_target[key] = hetero_graph.node_target[key].to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5940])\n",
      "torch.Size([1698])\n",
      "torch.Size([849])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block creates a basic split of a graph's nodes into training, validation, and testing sets. \n",
    "It uses predefined ratios to divide 'event' and 'concept' nodes in the heterogeneous graph for a simple \n",
    "dataset split, mainly for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "nEvents = hetero_graph.num_nodes(\"event\")\n",
    "nConcepts = hetero_graph.num_nodes(\"concept\")\n",
    "\n",
    "s1 = 0.7\n",
    "s2 = 0.8\n",
    "\n",
    "train_idx = {   \"event\": torch.tensor(range(0, int(nEvents * s1))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(0, int(nConcepts * s1))).to(args[\"device\"])\n",
    "            }\n",
    "val_idx = {   \"event\": torch.tensor(range(int(nEvents * s1), int(nEvents * s2))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s1), int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "test_idx = {   \"event\": torch.tensor(range(int(nEvents * s2), nEvents)).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s2), nConcepts)).to(args[\"device\"])\n",
    "            }\n",
    "\n",
    "print(train_idx[\"event\"].shape)\n",
    "print(test_idx[\"event\"].shape)\n",
    "print(val_idx[\"event\"].shape)\n",
    "\n",
    "\n",
    "# TODO: Add node labels to the nodes and try to make the deepsnap split work even for regression!\n",
    "\n",
    "# dataset = deepsnap.dataset.GraphDataset([hetero_graph], task='node')\n",
    "\n",
    "# dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
    "# datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2813.7439 Current Train,Val,Test Scores [373.6999206542969, 362.2122497558594, 305.9820861816406]\n",
      "Epoch 1 Loss 2710.2632 Current Train,Val,Test Scores [1550.159423828125, 1537.192626953125, 596.01220703125]\n",
      "Epoch 2 Loss 2392.2263 Current Train,Val,Test Scores [883.7342529296875, 873.007568359375, 283.9662780761719]\n",
      "Epoch 3 Loss 2311.0391 Current Train,Val,Test Scores [2086.635009765625, 2077.3955078125, 565.779541015625]\n",
      "Epoch 4 Loss 1912.1632 Current Train,Val,Test Scores [1644.73681640625, 1637.889404296875, 438.6648254394531]\n",
      "Epoch 5 Loss 1747.6449 Current Train,Val,Test Scores [850.1151123046875, 845.3836669921875, 247.95516967773438]\n",
      "Epoch 6 Loss 1703.8121 Current Train,Val,Test Scores [398.1546325683594, 392.571533203125, 136.44036865234375]\n",
      "Epoch 7 Loss 1730.8505 Current Train,Val,Test Scores [199.00094604492188, 199.64639282226562, 86.96051788330078]\n",
      "Epoch 8 Loss 1771.8457 Current Train,Val,Test Scores [133.33621215820312, 140.39471435546875, 69.69786071777344]\n",
      "Epoch 9 Loss 1771.1981 Current Train,Val,Test Scores [55.90563201904297, 69.71168518066406, 48.89112854003906]\n",
      "Epoch 10 Loss 1724.9542 Current Train,Val,Test Scores [35.86857223510742, 45.88459396362305, 45.29351043701172]\n",
      "Epoch 11 Loss 1659.8552 Current Train,Val,Test Scores [67.42311096191406, 78.78929901123047, 53.2418212890625]\n",
      "Epoch 12 Loss 1621.2576 Current Train,Val,Test Scores [72.32766723632812, 84.1843490600586, 53.82027053833008]\n",
      "Epoch 13 Loss 1625.4683 Current Train,Val,Test Scores [23.890609741210938, 34.17919921875, 39.32438278198242]\n",
      "Epoch 14 Loss 1609.8846 Current Train,Val,Test Scores [330.44525146484375, 321.9644470214844, 116.64408874511719]\n",
      "Epoch 15 Loss 1560.9805 Current Train,Val,Test Scores [822.6910400390625, 813.3507690429688, 243.46173095703125]\n",
      "Epoch 16 Loss 1508.7303 Current Train,Val,Test Scores [854.487548828125, 845.3309326171875, 247.60040283203125]\n",
      "Epoch 17 Loss 1463.6248 Current Train,Val,Test Scores [579.5154418945312, 569.6484985351562, 169.2836456298828]\n",
      "Epoch 18 Loss 1388.5020 Current Train,Val,Test Scores [224.4418182373047, 226.3345489501953, 71.04896545410156]\n",
      "Epoch 19 Loss 1297.5895 Current Train,Val,Test Scores [255.6501922607422, 254.6805877685547, 74.89390563964844]\n",
      "Epoch 20 Loss 1225.4861 Current Train,Val,Test Scores [1788.783935546875, 1781.3673095703125, 476.9757080078125]\n",
      "Epoch 21 Loss 1173.6514 Current Train,Val,Test Scores [1197.3905029296875, 1188.146484375, 318.14111328125]\n",
      "Epoch 22 Loss 1006.4754 Current Train,Val,Test Scores [390.1759033203125, 402.8190612792969, 107.07723236083984]\n",
      "Epoch 23 Loss 1305.3597 Current Train,Val,Test Scores [348.1815490722656, 360.5570373535156, 96.00233459472656]\n",
      "Epoch 24 Loss 906.0990 Current Train,Val,Test Scores [127.98319244384766, 135.5667724609375, 38.546199798583984]\n",
      "Epoch 25 Loss 1105.6338 Current Train,Val,Test Scores [291.078125, 287.27056884765625, 83.20116424560547]\n",
      "Epoch 26 Loss 1010.4117 Current Train,Val,Test Scores [252.08033752441406, 242.57220458984375, 73.97669219970703]\n",
      "Epoch 27 Loss 849.4855 Current Train,Val,Test Scores [91.10840606689453, 101.26726531982422, 38.506710052490234]\n",
      "Epoch 28 Loss 885.5807 Current Train,Val,Test Scores [688.8328857421875, 676.7490234375, 197.4553680419922]\n",
      "Epoch 29 Loss 905.8021 Current Train,Val,Test Scores [2103.18896484375, 2091.6591796875, 569.75146484375]\n",
      "Epoch 30 Loss 734.9404 Current Train,Val,Test Scores [2563.0546875, 2550.698486328125, 688.527099609375]\n",
      "Epoch 31 Loss 756.4514 Current Train,Val,Test Scores [2062.058349609375, 2049.2685546875, 553.1002807617188]\n",
      "Epoch 32 Loss 790.5468 Current Train,Val,Test Scores [1353.3499755859375, 1340.17626953125, 363.4775390625]\n",
      "Epoch 33 Loss 656.9942 Current Train,Val,Test Scores [824.93603515625, 811.491943359375, 222.25283813476562]\n",
      "Epoch 34 Loss 669.8281 Current Train,Val,Test Scores [344.5204162597656, 335.4978942871094, 95.84098815917969]\n",
      "Epoch 35 Loss 705.2999 Current Train,Val,Test Scores [46.172454833984375, 56.77194595336914, 16.40086555480957]\n",
      "Epoch 36 Loss 618.4490 Current Train,Val,Test Scores [989.2747192382812, 985.8909912109375, 261.25408935546875]\n",
      "Epoch 37 Loss 609.5680 Current Train,Val,Test Scores [1446.68408203125, 1444.462646484375, 382.3561096191406]\n",
      "Epoch 38 Loss 648.1913 Current Train,Val,Test Scores [380.78759765625, 378.0572509765625, 101.2444839477539]\n",
      "Epoch 39 Loss 582.8823 Current Train,Val,Test Scores [182.86761474609375, 187.10562133789062, 54.97658157348633]\n",
      "Epoch 40 Loss 564.0042 Current Train,Val,Test Scores [744.7593383789062, 732.9533081054688, 201.82305908203125]\n",
      "Epoch 41 Loss 588.8807 Current Train,Val,Test Scores [1362.5345458984375, 1350.3443603515625, 365.4459228515625]\n",
      "Epoch 42 Loss 540.2333 Current Train,Val,Test Scores [1745.205810546875, 1732.6668701171875, 466.8385314941406]\n",
      "Epoch 43 Loss 524.1758 Current Train,Val,Test Scores [1990.8282470703125, 1978.30517578125, 531.8644409179688]\n",
      "Epoch 44 Loss 537.1875 Current Train,Val,Test Scores [2471.487548828125, 2459.007080078125, 658.987060546875]\n",
      "Epoch 45 Loss 499.4017 Current Train,Val,Test Scores [2715.406494140625, 2702.85595703125, 723.5147094726562]\n",
      "Epoch 46 Loss 507.8133 Current Train,Val,Test Scores [1065.0279541015625, 1052.8360595703125, 287.10784912109375]\n",
      "Epoch 47 Loss 473.3478 Current Train,Val,Test Scores [185.0719757080078, 189.64022827148438, 56.306053161621094]\n",
      "Epoch 48 Loss 451.4412 Current Train,Val,Test Scores [8439.5859375, 8438.6865234375, 2232.47021484375]\n",
      "Epoch 49 Loss 452.4162 Current Train,Val,Test Scores [8951.4677734375, 8949.8818359375, 2367.6181640625]\n",
      "Epoch 50 Loss 443.9993 Current Train,Val,Test Scores [133.60845947265625, 140.29747009277344, 42.32099533081055]\n",
      "Epoch 51 Loss 417.7371 Current Train,Val,Test Scores [194.0394744873047, 197.31478881835938, 58.41945266723633]\n",
      "Epoch 52 Loss 408.0832 Current Train,Val,Test Scores [451.9984436035156, 439.4300537109375, 125.45391845703125]\n",
      "Epoch 53 Loss 390.9542 Current Train,Val,Test Scores [718.7809448242188, 706.247802734375, 194.76515197753906]\n",
      "Epoch 54 Loss 379.3972 Current Train,Val,Test Scores [838.3991088867188, 826.004638671875, 226.17141723632812]\n",
      "Epoch 55 Loss 366.5309 Current Train,Val,Test Scores [779.8720092773438, 767.538818359375, 210.53375244140625]\n",
      "Epoch 56 Loss 354.5330 Current Train,Val,Test Scores [472.2901611328125, 459.860595703125, 129.95162963867188]\n",
      "Epoch 57 Loss 336.7292 Current Train,Val,Test Scores [260.3490905761719, 258.6717224121094, 74.93548583984375]\n",
      "Epoch 58 Loss 322.8377 Current Train,Val,Test Scores [223.6338653564453, 224.85421752929688, 65.228515625]\n",
      "Epoch 59 Loss 302.2383 Current Train,Val,Test Scores [167.19284057617188, 172.27328491210938, 50.16193389892578]\n",
      "Epoch 60 Loss 289.2680 Current Train,Val,Test Scores [95.37416076660156, 104.79133605957031, 30.598648071289062]\n",
      "Epoch 61 Loss 291.0555 Current Train,Val,Test Scores [47.570960998535156, 58.54747009277344, 17.84093475341797]\n",
      "Epoch 62 Loss 440.8175 Current Train,Val,Test Scores [56.197391510009766, 66.97571563720703, 20.163225173950195]\n",
      "Epoch 63 Loss 697.3061 Current Train,Val,Test Scores [4548.68896484375, 4554.8798828125, 1201.5721435546875]\n",
      "Epoch 64 Loss 308.6754 Current Train,Val,Test Scores [10732.6396484375, 10736.3349609375, 2836.972900390625]\n",
      "Epoch 65 Loss 345.9788 Current Train,Val,Test Scores [5333.140625, 5337.09326171875, 1409.1502685546875]\n",
      "Epoch 66 Loss 395.7035 Current Train,Val,Test Scores [10769.9990234375, 10770.4130859375, 2847.215576171875]\n",
      "Epoch 67 Loss 298.6407 Current Train,Val,Test Scores [17693.1796875, 17685.244140625, 4679.84228515625]\n",
      "Epoch 68 Loss 307.7534 Current Train,Val,Test Scores [23806.962890625, 23779.818359375, 6298.9169921875]\n",
      "Epoch 69 Loss 291.6950 Current Train,Val,Test Scores [2560.24267578125, 2530.49267578125, 680.058349609375]\n",
      "Epoch 70 Loss 255.0589 Current Train,Val,Test Scores [146.3806610107422, 152.7767791748047, 44.16202163696289]\n",
      "Epoch 71 Loss 288.1863 Current Train,Val,Test Scores [184.20054626464844, 188.3513641357422, 54.284393310546875]\n",
      "Epoch 72 Loss 220.6670 Current Train,Val,Test Scores [347.1029968261719, 338.2711486816406, 96.961181640625]\n",
      "Epoch 73 Loss 268.5690 Current Train,Val,Test Scores [544.6387329101562, 531.884521484375, 147.913330078125]\n",
      "Epoch 74 Loss 217.3616 Current Train,Val,Test Scores [470.8766784667969, 458.1248474121094, 128.9102783203125]\n",
      "Epoch 75 Loss 230.7238 Current Train,Val,Test Scores [75.84030151367188, 86.37054443359375, 24.76597785949707]\n",
      "Epoch 76 Loss 186.8114 Current Train,Val,Test Scores [1296.735595703125, 1301.353759765625, 339.3442077636719]\n",
      "Epoch 77 Loss 187.2058 Current Train,Val,Test Scores [12884.783203125, 12882.08984375, 3404.893310546875]\n",
      "Epoch 78 Loss 173.2906 Current Train,Val,Test Scores [20881.9375, 20874.73828125, 5519.48876953125]\n",
      "Epoch 79 Loss 184.4827 Current Train,Val,Test Scores [24309.587890625, 24298.435546875, 6426.09033203125]\n",
      "Epoch 80 Loss 168.3049 Current Train,Val,Test Scores [24174.4921875, 24162.826171875, 6390.00537109375]\n",
      "Epoch 81 Loss 177.4720 Current Train,Val,Test Scores [26014.525390625, 25998.224609375, 6876.6591796875]\n",
      "Epoch 82 Loss 171.4704 Current Train,Val,Test Scores [8445.3994140625, 8444.421875, 2228.687744140625]\n",
      "Epoch 83 Loss 241.5737 Current Train,Val,Test Scores [27.614482879638672, 40.43181610107422, 11.619111061096191]\n",
      "Epoch 84 Loss 496.4071 Current Train,Val,Test Scores [50.49951171875, 63.3728141784668, 17.631755828857422]\n",
      "Epoch 85 Loss 190.2759 Current Train,Val,Test Scores [67.16669464111328, 80.06267547607422, 22.020719528198242]\n",
      "Epoch 86 Loss 268.8536 Current Train,Val,Test Scores [63.74860382080078, 76.59583282470703, 21.110654830932617]\n",
      "Epoch 87 Loss 329.4605 Current Train,Val,Test Scores [71.36421203613281, 84.19186401367188, 23.09151840209961]\n",
      "Epoch 88 Loss 331.6062 Current Train,Val,Test Scores [62.643070220947266, 75.44236755371094, 20.746150970458984]\n",
      "Epoch 89 Loss 350.6385 Current Train,Val,Test Scores [29.893970489501953, 41.03792953491211, 12.492687225341797]\n",
      "Epoch 90 Loss 341.3706 Current Train,Val,Test Scores [130.1405487060547, 137.6936798095703, 39.10213088989258]\n",
      "Epoch 91 Loss 329.6778 Current Train,Val,Test Scores [266.3883972167969, 264.7361755371094, 75.33834075927734]\n",
      "Epoch 92 Loss 314.4298 Current Train,Val,Test Scores [365.84161376953125, 355.7496032714844, 101.22869110107422]\n",
      "Epoch 93 Loss 282.9057 Current Train,Val,Test Scores [423.30841064453125, 410.6866149902344, 116.11438751220703]\n",
      "Epoch 94 Loss 266.3110 Current Train,Val,Test Scores [378.6753845214844, 367.3597412109375, 104.55573272705078]\n",
      "Epoch 95 Loss 254.4501 Current Train,Val,Test Scores [271.8051452636719, 269.5478210449219, 76.74612426757812]\n",
      "Epoch 96 Loss 236.1414 Current Train,Val,Test Scores [181.3754119873047, 185.75181579589844, 52.89085388183594]\n",
      "Epoch 97 Loss 233.4097 Current Train,Val,Test Scores [145.35794067382812, 151.8190155029297, 43.218990325927734]\n",
      "Epoch 98 Loss 232.7480 Current Train,Val,Test Scores [144.33389282226562, 150.85951232910156, 42.94228744506836]\n",
      "Epoch 99 Loss 234.3144 Current Train,Val,Test Scores [121.17176818847656, 129.41615295410156, 36.660369873046875]\n",
      "Epoch 100 Loss 234.1790 Current Train,Val,Test Scores [74.49237060546875, 85.11181640625, 23.993309020996094]\n",
      "Epoch 101 Loss 267.0738 Current Train,Val,Test Scores [18.05047607421875, 30.67601776123047, 9.055435180664062]\n",
      "Epoch 102 Loss 276.6717 Current Train,Val,Test Scores [54.84733963012695, 67.58489227294922, 18.641376495361328]\n",
      "Epoch 103 Loss 371.2211 Current Train,Val,Test Scores [2124.67919921875, 2144.77978515625, 557.9918212890625]\n",
      "Epoch 104 Loss 180.6586 Current Train,Val,Test Scores [3287.67578125, 3309.884765625, 865.13671875]\n",
      "Epoch 105 Loss 245.6775 Current Train,Val,Test Scores [70.394287109375, 83.20716857910156, 22.873558044433594]\n",
      "Epoch 106 Loss 459.3743 Current Train,Val,Test Scores [73.3149642944336, 86.03126525878906, 23.729978561401367]\n",
      "Epoch 107 Loss 160.4027 Current Train,Val,Test Scores [5485.2666015625, 5508.03173828125, 1446.662109375]\n",
      "Epoch 108 Loss 686.5361 Current Train,Val,Test Scores [73.12959289550781, 83.74686431884766, 24.06187629699707]\n",
      "Epoch 109 Loss 701.2981 Current Train,Val,Test Scores [182.8317413330078, 187.15130615234375, 53.821807861328125]\n",
      "Epoch 110 Loss 987.6429 Current Train,Val,Test Scores [204.7165985107422, 207.58848571777344, 59.728843688964844]\n",
      "Epoch 111 Loss 573.8725 Current Train,Val,Test Scores [28.178285598754883, 39.2679443359375, 12.683963775634766]\n",
      "Epoch 112 Loss 303.3968 Current Train,Val,Test Scores [119.97515106201172, 132.869140625, 36.47507858276367]\n",
      "Epoch 113 Loss 582.6069 Current Train,Val,Test Scores [179.21542358398438, 192.4795379638672, 52.10255813598633]\n",
      "Epoch 114 Loss 388.9887 Current Train,Val,Test Scores [204.5042724609375, 218.110107421875, 58.73390197753906]\n",
      "Epoch 115 Loss 697.7612 Current Train,Val,Test Scores [137.01905822753906, 149.76028442382812, 41.046913146972656]\n",
      "Epoch 116 Loss 452.6906 Current Train,Val,Test Scores [4748.22607421875, 4755.73046875, 1254.796875]\n",
      "Epoch 117 Loss 244.4442 Current Train,Val,Test Scores [15668.83203125, 15685.32421875, 4140.72119140625]\n",
      "Epoch 118 Loss 451.3809 Current Train,Val,Test Scores [15081.6259765625, 15102.24609375, 3984.6259765625]\n",
      "Epoch 119 Loss 217.4466 Current Train,Val,Test Scores [12143.0224609375, 12162.0751953125, 3207.755126953125]\n",
      "Epoch 120 Loss 352.1002 Current Train,Val,Test Scores [12045.0546875, 12061.48046875, 3182.075439453125]\n",
      "Epoch 121 Loss 411.9386 Current Train,Val,Test Scores [15042.6533203125, 15049.59375, 3975.561767578125]\n",
      "Epoch 122 Loss 222.5105 Current Train,Val,Test Scores [19979.330078125, 19979.23828125, 5281.427734375]\n",
      "Epoch 123 Loss 216.3737 Current Train,Val,Test Scores [22095.3671875, 22090.005859375, 5841.2158203125]\n",
      "Epoch 124 Loss 336.3232 Current Train,Val,Test Scores [19437.08984375, 19434.494140625, 5137.76708984375]\n",
      "Epoch 125 Loss 147.9972 Current Train,Val,Test Scores [16351.755859375, 16351.6181640625, 4321.58056640625]\n",
      "Epoch 126 Loss 220.7860 Current Train,Val,Test Scores [14494.447265625, 14494.1171875, 3830.3017578125]\n",
      "Epoch 127 Loss 281.1457 Current Train,Val,Test Scores [13861.369140625, 13858.90625, 3663.03955078125]\n",
      "Epoch 128 Loss 180.2851 Current Train,Val,Test Scores [14037.826171875, 14032.1865234375, 3709.920654296875]\n",
      "Epoch 129 Loss 140.3507 Current Train,Val,Test Scores [13836.5087890625, 13828.447265625, 3656.849365234375]\n",
      "Epoch 130 Loss 227.7640 Current Train,Val,Test Scores [12438.0068359375, 12429.8994140625, 3287.052001953125]\n",
      "Epoch 131 Loss 169.4045 Current Train,Val,Test Scores [10761.1064453125, 10754.4384765625, 2843.52880859375]\n",
      "Epoch 132 Loss 126.8393 Current Train,Val,Test Scores [9692.76171875, 9687.3935546875, 2560.9033203125]\n",
      "Epoch 133 Loss 173.2004 Current Train,Val,Test Scores [9124.7783203125, 9119.6669921875, 2410.636474609375]\n",
      "Epoch 134 Loss 175.4822 Current Train,Val,Test Scores [8669.48046875, 8663.232421875, 2290.2685546875]\n",
      "Epoch 135 Loss 126.1361 Current Train,Val,Test Scores [8107.81103515625, 8099.46533203125, 2141.85546875]\n",
      "Epoch 136 Loss 126.2208 Current Train,Val,Test Scores [7116.009765625, 7108.31005859375, 1880.0557861328125]\n",
      "Epoch 137 Loss 155.2934 Current Train,Val,Test Scores [5627.29248046875, 5619.6064453125, 1486.3519287109375]\n",
      "Epoch 138 Loss 123.1980 Current Train,Val,Test Scores [4134.548828125, 4128.1103515625, 1091.4583740234375]\n",
      "Epoch 139 Loss 107.8777 Current Train,Val,Test Scores [2994.05810546875, 2988.2265625, 789.8007202148438]\n",
      "Epoch 140 Loss 127.3403 Current Train,Val,Test Scores [2265.479248046875, 2258.9287109375, 597.677978515625]\n",
      "Epoch 141 Loss 122.4193 Current Train,Val,Test Scores [1808.214111328125, 1799.6678466796875, 477.5166015625]\n",
      "Epoch 142 Loss 97.9439 Current Train,Val,Test Scores [1144.364013671875, 1130.383544921875, 301.08258056640625]\n",
      "Epoch 143 Loss 101.8406 Current Train,Val,Test Scores [225.58489990234375, 215.91357421875, 60.20335388183594]\n",
      "Epoch 144 Loss 110.8648 Current Train,Val,Test Scores [17.584218978881836, 30.227455139160156, 9.113150596618652]\n",
      "Epoch 145 Loss 91.3126 Current Train,Val,Test Scores [17.43099594116211, 29.549413681030273, 9.07512092590332]\n",
      "Epoch 146 Loss 88.6694 Current Train,Val,Test Scores [17.536575317382812, 29.774499893188477, 8.966529846191406]\n",
      "Epoch 147 Loss 98.5828 Current Train,Val,Test Scores [18.262025833129883, 30.635108947753906, 9.093106269836426]\n",
      "Epoch 148 Loss 87.5854 Current Train,Val,Test Scores [20.155441284179688, 32.96894073486328, 9.517366409301758]\n",
      "Epoch 149 Loss 77.5559 Current Train,Val,Test Scores [22.49644660949707, 35.60441970825195, 10.14775562286377]\n",
      "Epoch 150 Loss 87.4850 Current Train,Val,Test Scores [21.49131202697754, 35.3372802734375, 10.103084564208984]\n",
      "Epoch 151 Loss 80.7175 Current Train,Val,Test Scores [82.69055938720703, 91.675537109375, 24.657621383666992]\n",
      "Epoch 152 Loss 72.0301 Current Train,Val,Test Scores [134.75332641601562, 139.95790100097656, 39.283382415771484]\n",
      "Epoch 153 Loss 79.1466 Current Train,Val,Test Scores [130.38418579101562, 136.08529663085938, 38.294708251953125]\n",
      "Epoch 154 Loss 75.1036 Current Train,Val,Test Scores [138.14013671875, 140.888916015625, 40.26310348510742]\n",
      "Epoch 155 Loss 67.5145 Current Train,Val,Test Scores [152.31207275390625, 148.41455078125, 43.749332427978516]\n",
      "Epoch 156 Loss 72.8803 Current Train,Val,Test Scores [150.56285095214844, 146.85804748535156, 42.58799743652344]\n",
      "Epoch 157 Loss 68.6938 Current Train,Val,Test Scores [64.99897003173828, 70.87442779541016, 21.39069938659668]\n",
      "Epoch 158 Loss 64.5305 Current Train,Val,Test Scores [16.25330352783203, 31.671144485473633, 9.378405570983887]\n",
      "Epoch 159 Loss 68.1056 Current Train,Val,Test Scores [40.14188766479492, 52.983646392822266, 14.865202903747559]\n",
      "Epoch 160 Loss 64.0269 Current Train,Val,Test Scores [41.255699157714844, 54.103145599365234, 15.160197257995605]\n",
      "Epoch 161 Loss 61.2207 Current Train,Val,Test Scores [39.321163177490234, 52.164405822753906, 14.651678085327148]\n",
      "Epoch 162 Loss 63.8794 Current Train,Val,Test Scores [30.88746452331543, 43.680179595947266, 12.414267539978027]\n",
      "Epoch 163 Loss 59.2983 Current Train,Val,Test Scores [18.88886260986328, 31.586097717285156, 9.268115043640137]\n",
      "Epoch 164 Loss 59.2300 Current Train,Val,Test Scores [17.804704666137695, 29.673011779785156, 9.276278495788574]\n",
      "Epoch 165 Loss 59.7872 Current Train,Val,Test Scores [17.358142852783203, 29.592117309570312, 9.069689750671387]\n",
      "Epoch 166 Loss 56.0186 Current Train,Val,Test Scores [23.759050369262695, 36.62504196166992, 10.55634880065918]\n",
      "Epoch 167 Loss 56.7990 Current Train,Val,Test Scores [33.75904846191406, 46.7011833190918, 13.206727027893066]\n",
      "Epoch 168 Loss 55.7371 Current Train,Val,Test Scores [33.153438568115234, 46.1429557800293, 13.039892196655273]\n",
      "Epoch 169 Loss 53.4907 Current Train,Val,Test Scores [23.324926376342773, 36.28800582885742, 10.437259674072266]\n",
      "Epoch 170 Loss 54.4211 Current Train,Val,Test Scores [17.836198806762695, 30.386226654052734, 9.07140827178955]\n",
      "Epoch 171 Loss 52.3535 Current Train,Val,Test Scores [22.644777297973633, 35.53363037109375, 10.281880378723145]\n",
      "Epoch 172 Loss 51.5792 Current Train,Val,Test Scores [38.66072082519531, 51.59829330444336, 14.52462100982666]\n",
      "Epoch 173 Loss 51.6622 Current Train,Val,Test Scores [55.51670455932617, 68.53813171386719, 18.97882843017578]\n",
      "Epoch 174 Loss 49.7005 Current Train,Val,Test Scores [60.9583740234375, 74.00757598876953, 20.413814544677734]\n",
      "Epoch 175 Loss 49.9258 Current Train,Val,Test Scores [55.544551849365234, 68.54755401611328, 18.98996353149414]\n",
      "Epoch 176 Loss 49.0887 Current Train,Val,Test Scores [47.096805572509766, 60.02519607543945, 16.766685485839844]\n",
      "Epoch 177 Loss 47.9347 Current Train,Val,Test Scores [45.902767181396484, 58.809757232666016, 16.457168579101562]\n",
      "Epoch 178 Loss 48.1068 Current Train,Val,Test Scores [55.596710205078125, 68.57488250732422, 19.018840789794922]\n",
      "Epoch 179 Loss 46.7989 Current Train,Val,Test Scores [65.68565368652344, 78.73773193359375, 21.68354034423828]\n",
      "Epoch 180 Loss 46.5122 Current Train,Val,Test Scores [68.8401870727539, 81.89620208740234, 22.5205020904541]\n",
      "Epoch 181 Loss 46.0350 Current Train,Val,Test Scores [65.36856079101562, 78.35415649414062, 21.61166763305664]\n",
      "Epoch 182 Loss 45.0407 Current Train,Val,Test Scores [69.02311706542969, 81.97219848632812, 22.587352752685547]\n",
      "Epoch 183 Loss 45.0042 Current Train,Val,Test Scores [81.30399322509766, 94.28765106201172, 25.837512969970703]\n",
      "Epoch 184 Loss 44.0647 Current Train,Val,Test Scores [94.0291976928711, 107.08824157714844, 29.198598861694336]\n",
      "Epoch 185 Loss 43.7848 Current Train,Val,Test Scores [86.90567016601562, 99.97067260742188, 27.313016891479492]\n",
      "Epoch 186 Loss 43.2560 Current Train,Val,Test Scores [69.50972747802734, 82.47914123535156, 22.716325759887695]\n",
      "Epoch 187 Loss 42.5405 Current Train,Val,Test Scores [62.36698532104492, 75.27082824707031, 20.8172664642334]\n",
      "Epoch 188 Loss 42.4256 Current Train,Val,Test Scores [77.25616455078125, 90.23554229736328, 24.75165557861328]\n",
      "Epoch 189 Loss 41.7146 Current Train,Val,Test Scores [83.59760284423828, 96.64749145507812, 26.4167423248291]\n",
      "Epoch 190 Loss 41.4685 Current Train,Val,Test Scores [72.12232971191406, 85.09114074707031, 23.373146057128906]\n",
      "Epoch 191 Loss 40.9305 Current Train,Val,Test Scores [69.13116455078125, 82.00008392333984, 22.575408935546875]\n",
      "Epoch 192 Loss 40.5364 Current Train,Val,Test Scores [76.68750762939453, 89.52596282958984, 24.584476470947266]\n",
      "Epoch 193 Loss 40.2143 Current Train,Val,Test Scores [82.54826354980469, 95.48670959472656, 26.1251163482666]\n",
      "Epoch 194 Loss 39.6182 Current Train,Val,Test Scores [83.76254272460938, 96.63780212402344, 26.451156616210938]\n",
      "Epoch 195 Loss 39.4784 Current Train,Val,Test Scores [106.21961975097656, 119.35662841796875, 32.38508987426758]\n",
      "Epoch 196 Loss 39.0008 Current Train,Val,Test Scores [80.92406463623047, 93.9509506225586, 25.689056396484375]\n",
      "Epoch 197 Loss 38.5190 Current Train,Val,Test Scores [81.509765625, 94.33695983886719, 25.856473922729492]\n",
      "Epoch 198 Loss 38.3509 Current Train,Val,Test Scores [124.55824279785156, 137.5688018798828, 37.25278854370117]\n",
      "Epoch 199 Loss 37.9292 Current Train,Val,Test Scores [103.56670379638672, 116.7176513671875, 31.688554763793945]\n",
      "Epoch 200 Loss 37.6340 Current Train,Val,Test Scores [86.62786102294922, 99.69844055175781, 27.197568893432617]\n",
      "Epoch 201 Loss 37.1563 Current Train,Val,Test Scores [97.56089782714844, 110.48434448242188, 30.09773826599121]\n",
      "Epoch 202 Loss 36.9452 Current Train,Val,Test Scores [121.40013122558594, 134.32485961914062, 36.40024185180664]\n",
      "Epoch 203 Loss 36.3851 Current Train,Val,Test Scores [157.7281951904297, 170.74789428710938, 46.01834487915039]\n",
      "Epoch 204 Loss 35.9648 Current Train,Val,Test Scores [151.1297149658203, 164.37722778320312, 44.31391906738281]\n",
      "Epoch 205 Loss 35.6155 Current Train,Val,Test Scores [146.26934814453125, 160.75157165527344, 42.87284851074219]\n",
      "Epoch 206 Loss 35.3449 Current Train,Val,Test Scores [132.4871063232422, 149.218505859375, 39.03459930419922]\n",
      "Epoch 207 Loss 34.8189 Current Train,Val,Test Scores [132.95408630371094, 149.8046417236328, 39.14963150024414]\n",
      "Epoch 208 Loss 34.7040 Current Train,Val,Test Scores [151.23500061035156, 165.6719207763672, 44.1850471496582]\n",
      "Epoch 209 Loss 34.3984 Current Train,Val,Test Scores [158.46908569335938, 171.9138946533203, 46.22755813598633]\n",
      "Epoch 210 Loss 33.9610 Current Train,Val,Test Scores [180.28843688964844, 196.3684844970703, 51.80326843261719]\n",
      "Epoch 211 Loss 33.6481 Current Train,Val,Test Scores [160.45291137695312, 174.42630004882812, 46.68759536743164]\n",
      "Epoch 212 Loss 33.4625 Current Train,Val,Test Scores [142.8786163330078, 156.27490234375, 42.033203125]\n",
      "Epoch 213 Loss 32.9932 Current Train,Val,Test Scores [120.70111083984375, 134.21934509277344, 36.13991165161133]\n",
      "Epoch 214 Loss 32.7665 Current Train,Val,Test Scores [124.33151245117188, 137.5996551513672, 37.1022834777832]\n",
      "Epoch 215 Loss 32.5202 Current Train,Val,Test Scores [180.52484130859375, 196.53738403320312, 51.78456497192383]\n",
      "Epoch 216 Loss 32.3108 Current Train,Val,Test Scores [145.56625366210938, 159.07769775390625, 42.680782318115234]\n",
      "Epoch 217 Loss 32.0451 Current Train,Val,Test Scores [139.25941467285156, 152.73138427734375, 41.01701354980469]\n",
      "Epoch 218 Loss 31.5884 Current Train,Val,Test Scores [153.01536560058594, 166.7093963623047, 44.663673400878906]\n",
      "Epoch 219 Loss 31.3299 Current Train,Val,Test Scores [110.91572570800781, 124.59248352050781, 33.527252197265625]\n",
      "Epoch 220 Loss 31.0863 Current Train,Val,Test Scores [117.88418579101562, 131.5775909423828, 35.36989212036133]\n",
      "Epoch 221 Loss 30.8550 Current Train,Val,Test Scores [125.62169647216797, 139.61325073242188, 37.37546157836914]\n",
      "Epoch 222 Loss 30.5878 Current Train,Val,Test Scores [142.54270935058594, 156.3937225341797, 41.876163482666016]\n",
      "Epoch 223 Loss 30.2942 Current Train,Val,Test Scores [83.51986694335938, 97.52229309082031, 26.20168113708496]\n",
      "Epoch 224 Loss 29.9825 Current Train,Val,Test Scores [60.696170806884766, 74.76039123535156, 20.18773651123047]\n",
      "Epoch 225 Loss 29.8426 Current Train,Val,Test Scores [44.31174087524414, 58.17367172241211, 15.846152305603027]\n",
      "Epoch 226 Loss 29.6778 Current Train,Val,Test Scores [35.34768295288086, 49.23283767700195, 13.480031967163086]\n",
      "Epoch 227 Loss 29.7157 Current Train,Val,Test Scores [33.38214874267578, 47.01287078857422, 13.073142051696777]\n",
      "Epoch 228 Loss 30.4755 Current Train,Val,Test Scores [22.425989151000977, 35.951881408691406, 10.034151077270508]\n",
      "Epoch 229 Loss 31.4642 Current Train,Val,Test Scores [15.360517501831055, 28.454687118530273, 8.702488899230957]\n",
      "Epoch 230 Loss 29.9960 Current Train,Val,Test Scores [14.752171516418457, 27.82378387451172, 8.549697875976562]\n",
      "Epoch 231 Loss 28.5377 Current Train,Val,Test Scores [31.195585250854492, 44.54925537109375, 12.418209075927734]\n",
      "Epoch 232 Loss 29.7323 Current Train,Val,Test Scores [23.720542907714844, 34.25326919555664, 10.896156311035156]\n",
      "Epoch 233 Loss 28.6722 Current Train,Val,Test Scores [24.801618576049805, 34.90424346923828, 11.318282127380371]\n",
      "Epoch 234 Loss 28.0888 Current Train,Val,Test Scores [41.61202621459961, 50.81964874267578, 16.33464241027832]\n",
      "Epoch 235 Loss 28.0980 Current Train,Val,Test Scores [43.40549850463867, 52.092098236083984, 16.929481506347656]\n",
      "Epoch 236 Loss 29.2130 Current Train,Val,Test Scores [33.56631851196289, 47.3491325378418, 12.948698997497559]\n",
      "Epoch 237 Loss 29.2115 Current Train,Val,Test Scores [37.25598907470703, 50.77366256713867, 14.105125427246094]\n",
      "Epoch 238 Loss 27.9899 Current Train,Val,Test Scores [34.76895523071289, 48.46384811401367, 13.42115306854248]\n",
      "Epoch 239 Loss 27.3517 Current Train,Val,Test Scores [28.6328125, 42.28482437133789, 11.655144691467285]\n",
      "Epoch 240 Loss 27.9253 Current Train,Val,Test Scores [56.96696472167969, 70.60685729980469, 18.999906539916992]\n",
      "Epoch 241 Loss 26.8014 Current Train,Val,Test Scores [142.35926818847656, 159.76638793945312, 41.309669494628906]\n",
      "Epoch 242 Loss 26.4206 Current Train,Val,Test Scores [57.67613220214844, 75.6217269897461, 18.634756088256836]\n",
      "Epoch 243 Loss 26.8617 Current Train,Val,Test Scores [33.44204330444336, 47.29489517211914, 12.411604881286621]\n",
      "Epoch 244 Loss 26.8527 Current Train,Val,Test Scores [58.13488006591797, 72.21553039550781, 19.341487884521484]\n",
      "Epoch 245 Loss 27.5340 Current Train,Val,Test Scores [44.19328308105469, 57.62702178955078, 15.99858283996582]\n",
      "Epoch 246 Loss 26.5051 Current Train,Val,Test Scores [23.953664779663086, 37.13256072998047, 10.575236320495605]\n",
      "Epoch 247 Loss 26.1672 Current Train,Val,Test Scores [32.72849655151367, 46.4133415222168, 12.608428955078125]\n",
      "Epoch 248 Loss 26.3437 Current Train,Val,Test Scores [31.015457153320312, 44.84762191772461, 12.013541221618652]\n",
      "Epoch 249 Loss 26.3260 Current Train,Val,Test Scores [42.85991287231445, 59.8330192565918, 14.559653282165527]\n",
      "Epoch 250 Loss 24.9210 Current Train,Val,Test Scores [110.96537017822266, 127.0244369506836, 33.0411376953125]\n",
      "Epoch 251 Loss 24.4016 Current Train,Val,Test Scores [46.63595962524414, 60.16435241699219, 16.467605590820312]\n",
      "Epoch 252 Loss 25.1046 Current Train,Val,Test Scores [60.70380401611328, 74.53030395507812, 20.080900192260742]\n",
      "Epoch 253 Loss 27.9385 Current Train,Val,Test Scores [66.4285888671875, 80.38783264160156, 21.615440368652344]\n",
      "Epoch 254 Loss 26.1904 Current Train,Val,Test Scores [38.63506317138672, 52.42864990234375, 14.266900062561035]\n",
      "Epoch 255 Loss 24.3454 Current Train,Val,Test Scores [29.144001007080078, 42.75252914428711, 11.771093368530273]\n",
      "Epoch 256 Loss 25.2661 Current Train,Val,Test Scores [41.861690521240234, 55.49092483520508, 15.174874305725098]\n",
      "Epoch 257 Loss 24.6493 Current Train,Val,Test Scores [34.9151496887207, 48.59760284423828, 13.316654205322266]\n",
      "Epoch 258 Loss 23.7381 Current Train,Val,Test Scores [35.93596267700195, 49.75743103027344, 13.559612274169922]\n",
      "Epoch 259 Loss 23.5315 Current Train,Val,Test Scores [49.73691177368164, 63.500980377197266, 17.240650177001953]\n",
      "Epoch 260 Loss 24.4460 Current Train,Val,Test Scores [60.523231506347656, 74.21110534667969, 20.081878662109375]\n",
      "Epoch 261 Loss 28.2369 Current Train,Val,Test Scores [68.42687225341797, 81.97750854492188, 22.320344924926758]\n",
      "Epoch 262 Loss 28.3378 Current Train,Val,Test Scores [16.69585609436035, 29.506366729736328, 8.72110652923584]\n",
      "Epoch 263 Loss 25.3722 Current Train,Val,Test Scores [54.264625549316406, 67.10108184814453, 18.821603775024414]\n",
      "Epoch 264 Loss 25.2262 Current Train,Val,Test Scores [100.42912292480469, 100.9428939819336, 30.641605377197266]\n",
      "Epoch 265 Loss 24.3683 Current Train,Val,Test Scores [851.92724609375, 809.49609375, 229.2216796875]\n",
      "Epoch 266 Loss 23.1946 Current Train,Val,Test Scores [1125.7257080078125, 1123.8858642578125, 294.3722229003906]\n",
      "Epoch 267 Loss 24.4538 Current Train,Val,Test Scores [802.4486694335938, 772.0333862304688, 214.2067413330078]\n",
      "Epoch 268 Loss 22.3039 Current Train,Val,Test Scores [75.3310775756836, 75.6245346069336, 24.180891036987305]\n",
      "Epoch 269 Loss 24.0662 Current Train,Val,Test Scores [74.4071273803711, 83.87693786621094, 25.46037483215332]\n",
      "Epoch 270 Loss 21.8309 Current Train,Val,Test Scores [292.2791748046875, 269.7330322265625, 82.95882415771484]\n",
      "Epoch 271 Loss 22.9024 Current Train,Val,Test Scores [266.8642578125, 251.39285278320312, 76.22032165527344]\n",
      "Epoch 272 Loss 21.4894 Current Train,Val,Test Scores [223.2196807861328, 216.3797149658203, 64.65664672851562]\n",
      "Epoch 273 Loss 22.1363 Current Train,Val,Test Scores [429.8865051269531, 405.77880859375, 118.2717056274414]\n",
      "Epoch 274 Loss 21.2555 Current Train,Val,Test Scores [617.830322265625, 589.3700561523438, 168.04345703125]\n",
      "Epoch 275 Loss 21.1242 Current Train,Val,Test Scores [399.43609619140625, 375.0389099121094, 110.53699493408203]\n",
      "Epoch 276 Loss 21.1197 Current Train,Val,Test Scores [29.179725646972656, 40.3830451965332, 13.361713409423828]\n",
      "Epoch 277 Loss 20.4571 Current Train,Val,Test Scores [209.33724975585938, 229.1306610107422, 58.938575744628906]\n",
      "Epoch 278 Loss 20.7094 Current Train,Val,Test Scores [93.01725006103516, 112.81558990478516, 28.143688201904297]\n",
      "Epoch 279 Loss 19.9753 Current Train,Val,Test Scores [62.453636169433594, 68.44158935546875, 22.312602996826172]\n",
      "Epoch 280 Loss 20.1223 Current Train,Val,Test Scores [46.277774810791016, 54.46629333496094, 17.876110076904297]\n",
      "Epoch 281 Loss 19.4797 Current Train,Val,Test Scores [52.13312530517578, 71.49870300292969, 17.110612869262695]\n",
      "Epoch 282 Loss 19.7638 Current Train,Val,Test Scores [256.16876220703125, 275.6022644042969, 71.43143463134766]\n",
      "Epoch 283 Loss 19.2718 Current Train,Val,Test Scores [161.10231018066406, 176.09201049804688, 46.62268829345703]\n",
      "Epoch 284 Loss 20.0107 Current Train,Val,Test Scores [144.6249237060547, 159.6077423095703, 42.32307815551758]\n",
      "Epoch 285 Loss 19.5810 Current Train,Val,Test Scores [438.4959716796875, 457.6080322265625, 119.95065307617188]\n",
      "Epoch 286 Loss 21.7865 Current Train,Val,Test Scores [325.8738708496094, 342.3668518066406, 90.05266571044922]\n",
      "Epoch 287 Loss 29.5530 Current Train,Val,Test Scores [259.8767395019531, 277.50274658203125, 72.37004852294922]\n",
      "Epoch 288 Loss 32.9881 Current Train,Val,Test Scores [262.34881591796875, 280.02752685546875, 73.1417007446289]\n",
      "Epoch 289 Loss 30.8689 Current Train,Val,Test Scores [103.34305572509766, 100.0669174194336, 34.104000091552734]\n",
      "Epoch 290 Loss 27.6231 Current Train,Val,Test Scores [2281.431396484375, 2242.304931640625, 608.7852172851562]\n",
      "Epoch 291 Loss 25.8634 Current Train,Val,Test Scores [2923.905517578125, 2926.978515625, 769.1530151367188]\n",
      "Epoch 292 Loss 25.1541 Current Train,Val,Test Scores [3616.263916015625, 3619.967529296875, 951.7689208984375]\n",
      "Epoch 293 Loss 27.0707 Current Train,Val,Test Scores [3279.505615234375, 3284.797607421875, 862.2991943359375]\n",
      "Epoch 294 Loss 25.3301 Current Train,Val,Test Scores [1066.6507568359375, 1070.7413330078125, 278.4910583496094]\n",
      "Epoch 295 Loss 23.2277 Current Train,Val,Test Scores [17.150819778442383, 30.13670539855957, 9.152562141418457]\n",
      "Epoch 296 Loss 22.7692 Current Train,Val,Test Scores [74.63273620605469, 88.29331970214844, 24.224782943725586]\n",
      "Epoch 297 Loss 23.1009 Current Train,Val,Test Scores [108.93220520019531, 122.49472045898438, 33.303627014160156]\n",
      "Epoch 298 Loss 22.3912 Current Train,Val,Test Scores [108.29973602294922, 121.74127960205078, 33.16230773925781]\n",
      "Epoch 299 Loss 20.9870 Current Train,Val,Test Scores [120.39347076416016, 133.8219451904297, 36.40575408935547]\n",
      "Epoch 300 Loss 20.9837 Current Train,Val,Test Scores [130.09716796875, 143.46299743652344, 38.99833679199219]\n",
      "Epoch 301 Loss 19.9834 Current Train,Val,Test Scores [172.5128631591797, 185.94943237304688, 50.24068069458008]\n",
      "Epoch 302 Loss 20.2695 Current Train,Val,Test Scores [179.9617462158203, 193.48475646972656, 52.23092269897461]\n",
      "Epoch 303 Loss 19.7698 Current Train,Val,Test Scores [161.04905700683594, 174.47262573242188, 47.253700256347656]\n",
      "Epoch 304 Loss 18.8640 Current Train,Val,Test Scores [192.53248596191406, 206.0251007080078, 55.591495513916016]\n",
      "Epoch 305 Loss 19.1261 Current Train,Val,Test Scores [732.8648071289062, 749.6315307617188, 198.04957580566406]\n",
      "Epoch 306 Loss 18.9248 Current Train,Val,Test Scores [589.521728515625, 608.6224975585938, 159.91734313964844]\n",
      "Epoch 307 Loss 18.0206 Current Train,Val,Test Scores [296.2667541503906, 315.7357177734375, 82.2585678100586]\n",
      "Epoch 308 Loss 18.7805 Current Train,Val,Test Scores [126.70014953613281, 140.56712341308594, 38.101261138916016]\n",
      "Epoch 309 Loss 18.5721 Current Train,Val,Test Scores [138.3155517578125, 152.46041870117188, 41.168697357177734]\n",
      "Epoch 310 Loss 18.6704 Current Train,Val,Test Scores [48.83757019042969, 62.64255142211914, 17.51500701904297]\n",
      "Epoch 311 Loss 18.1455 Current Train,Val,Test Scores [38.91178512573242, 52.55354309082031, 14.904927253723145]\n",
      "Epoch 312 Loss 17.9593 Current Train,Val,Test Scores [84.20169067382812, 98.0078353881836, 26.891231536865234]\n",
      "Epoch 313 Loss 18.2457 Current Train,Val,Test Scores [64.95873260498047, 78.83735656738281, 21.80231285095215]\n",
      "Epoch 314 Loss 18.2234 Current Train,Val,Test Scores [68.99430847167969, 82.7879867553711, 22.871593475341797]\n",
      "Epoch 315 Loss 16.5744 Current Train,Val,Test Scores [96.71127319335938, 110.88980865478516, 30.064668655395508]\n",
      "Epoch 316 Loss 17.2979 Current Train,Val,Test Scores [80.44296264648438, 94.47452545166016, 25.834157943725586]\n",
      "Epoch 317 Loss 16.8706 Current Train,Val,Test Scores [90.88268280029297, 104.72772216796875, 28.5972957611084]\n",
      "Epoch 318 Loss 18.0995 Current Train,Val,Test Scores [81.27072143554688, 95.60942840576172, 25.880197525024414]\n",
      "Epoch 319 Loss 18.4456 Current Train,Val,Test Scores [68.70684051513672, 82.61846923828125, 22.608741760253906]\n",
      "Epoch 320 Loss 17.9121 Current Train,Val,Test Scores [45.394989013671875, 59.42866516113281, 16.484525680541992]\n",
      "Epoch 321 Loss 16.1498 Current Train,Val,Test Scores [65.8967056274414, 80.04438018798828, 21.830825805664062]\n",
      "Epoch 322 Loss 16.2037 Current Train,Val,Test Scores [208.13467407226562, 224.11375427246094, 59.414268493652344]\n",
      "Epoch 323 Loss 16.9509 Current Train,Val,Test Scores [60.601295471191406, 80.03787994384766, 19.51476287841797]\n",
      "Epoch 324 Loss 16.1568 Current Train,Val,Test Scores [135.74940490722656, 155.48501586914062, 39.49101638793945]\n",
      "Epoch 325 Loss 16.1416 Current Train,Val,Test Scores [94.66310119628906, 114.52100372314453, 28.73020362854004]\n",
      "Epoch 326 Loss 15.1714 Current Train,Val,Test Scores [138.68833923339844, 128.29345703125, 44.22965621948242]\n",
      "Epoch 327 Loss 14.9907 Current Train,Val,Test Scores [1019.0167846679688, 977.972900390625, 276.9700012207031]\n",
      "Epoch 328 Loss 15.1323 Current Train,Val,Test Scores [835.5816040039062, 794.5338134765625, 228.29331970214844]\n",
      "Epoch 329 Loss 14.5266 Current Train,Val,Test Scores [411.4765930175781, 384.2759094238281, 113.78289794921875]\n",
      "Epoch 330 Loss 14.8550 Current Train,Val,Test Scores [689.4744262695312, 651.4201049804688, 190.56358337402344]\n",
      "Epoch 331 Loss 14.1316 Current Train,Val,Test Scores [304.32672119140625, 271.876220703125, 88.4267807006836]\n",
      "Epoch 332 Loss 13.5818 Current Train,Val,Test Scores [204.73268127441406, 181.8765106201172, 61.24858474731445]\n",
      "Epoch 333 Loss 13.5924 Current Train,Val,Test Scores [262.1822814941406, 229.5653533935547, 78.19652557373047]\n",
      "Epoch 334 Loss 13.8595 Current Train,Val,Test Scores [248.3657989501953, 218.34783935546875, 72.92601013183594]\n",
      "Epoch 335 Loss 13.6891 Current Train,Val,Test Scores [817.6292114257812, 787.0462036132812, 223.09678649902344]\n",
      "Epoch 336 Loss 14.1566 Current Train,Val,Test Scores [928.3038940429688, 893.2011108398438, 253.12220764160156]\n",
      "Epoch 337 Loss 13.0784 Current Train,Val,Test Scores [1038.2239990234375, 1002.0211791992188, 282.8588562011719]\n",
      "Epoch 338 Loss 13.2314 Current Train,Val,Test Scores [726.5343627929688, 693.4993286132812, 199.46536254882812]\n",
      "Epoch 339 Loss 15.4719 Current Train,Val,Test Scores [145.0192413330078, 130.17897033691406, 46.1510009765625]\n",
      "Epoch 340 Loss 14.0219 Current Train,Val,Test Scores [105.28165435791016, 102.83016967773438, 35.9273681640625]\n",
      "Epoch 341 Loss 17.7978 Current Train,Val,Test Scores [23.39125633239746, 40.69984817504883, 12.30122184753418]\n",
      "Epoch 342 Loss 21.1755 Current Train,Val,Test Scores [266.72900390625, 239.5205535888672, 75.05770111083984]\n",
      "Epoch 343 Loss 24.1891 Current Train,Val,Test Scores [222.149169921875, 224.44064331054688, 57.5109748840332]\n",
      "Epoch 344 Loss 26.7160 Current Train,Val,Test Scores [1971.350341796875, 1980.9407958984375, 514.7423706054688]\n",
      "Epoch 345 Loss 21.3043 Current Train,Val,Test Scores [3145.677978515625, 3151.941162109375, 827.0599365234375]\n",
      "Epoch 346 Loss 17.0180 Current Train,Val,Test Scores [3233.71923828125, 3240.39404296875, 850.3116455078125]\n",
      "Epoch 347 Loss 22.9007 Current Train,Val,Test Scores [2430.322265625, 2417.1650390625, 642.7775268554688]\n",
      "Epoch 348 Loss 21.7564 Current Train,Val,Test Scores [20.218595504760742, 33.38369369506836, 10.167006492614746]\n",
      "Epoch 349 Loss 21.2582 Current Train,Val,Test Scores [98.96942138671875, 112.7320327758789, 30.847457885742188]\n",
      "Epoch 350 Loss 20.8810 Current Train,Val,Test Scores [50.90778732299805, 60.76165771484375, 19.60568618774414]\n",
      "Epoch 351 Loss 16.4192 Current Train,Val,Test Scores [1069.34912109375, 1030.489013671875, 289.4408264160156]\n",
      "Epoch 352 Loss 18.3184 Current Train,Val,Test Scores [795.47314453125, 772.1792602539062, 213.8409423828125]\n",
      "Epoch 353 Loss 31.4565 Current Train,Val,Test Scores [78.83059692382812, 86.4455795288086, 26.628244400024414]\n",
      "Epoch 354 Loss 41.3047 Current Train,Val,Test Scores [21.317697525024414, 33.74460220336914, 10.399627685546875]\n",
      "Epoch 355 Loss 45.6013 Current Train,Val,Test Scores [29.745603561401367, 45.51667785644531, 13.10980224609375]\n",
      "Epoch 356 Loss 46.7817 Current Train,Val,Test Scores [1081.9898681640625, 1088.1787109375, 280.6985168457031]\n",
      "Epoch 357 Loss 44.6020 Current Train,Val,Test Scores [3072.86083984375, 3076.747802734375, 808.0438842773438]\n",
      "Epoch 358 Loss 39.2649 Current Train,Val,Test Scores [4279.8759765625, 4283.74755859375, 1126.9075927734375]\n",
      "Epoch 359 Loss 36.5042 Current Train,Val,Test Scores [5596.36083984375, 5599.8681640625, 1475.2060546875]\n",
      "Epoch 360 Loss 36.6661 Current Train,Val,Test Scores [5664.015625, 5666.42529296875, 1493.287841796875]\n",
      "Epoch 361 Loss 35.8106 Current Train,Val,Test Scores [5167.86083984375, 5170.55615234375, 1362.0494384765625]\n",
      "Epoch 362 Loss 36.2010 Current Train,Val,Test Scores [3704.20458984375, 3707.123291015625, 974.865966796875]\n",
      "Epoch 363 Loss 36.3887 Current Train,Val,Test Scores [2319.92822265625, 2323.955078125, 608.7100830078125]\n",
      "Epoch 364 Loss 36.3042 Current Train,Val,Test Scores [42.79360580444336, 62.7235221862793, 16.88978385925293]\n",
      "Epoch 365 Loss 34.3912 Current Train,Val,Test Scores [29.0989990234375, 40.7755126953125, 12.47122859954834]\n",
      "Epoch 366 Loss 31.9581 Current Train,Val,Test Scores [36.90630340576172, 48.31696319580078, 14.504321098327637]\n",
      "Epoch 367 Loss 31.4117 Current Train,Val,Test Scores [45.72392654418945, 56.887935638427734, 16.726823806762695]\n",
      "Epoch 368 Loss 30.9186 Current Train,Val,Test Scores [37.84727096557617, 49.073692321777344, 14.796784400939941]\n",
      "Epoch 369 Loss 31.5063 Current Train,Val,Test Scores [30.192014694213867, 41.33921813964844, 12.892175674438477]\n",
      "Epoch 370 Loss 30.7241 Current Train,Val,Test Scores [25.82830238342285, 36.87041473388672, 11.70166301727295]\n",
      "Epoch 371 Loss 30.0479 Current Train,Val,Test Scores [22.309133529663086, 33.7360725402832, 10.730857849121094]\n",
      "Epoch 372 Loss 29.4881 Current Train,Val,Test Scores [317.4739074707031, 294.9962463378906, 84.21725463867188]\n",
      "Epoch 373 Loss 29.2221 Current Train,Val,Test Scores [1125.6619873046875, 1129.264892578125, 293.09490966796875]\n",
      "Epoch 374 Loss 29.2574 Current Train,Val,Test Scores [1535.037841796875, 1539.25390625, 400.85040283203125]\n",
      "Epoch 375 Loss 28.5174 Current Train,Val,Test Scores [1615.1842041015625, 1619.0660400390625, 422.25006103515625]\n",
      "Epoch 376 Loss 28.4380 Current Train,Val,Test Scores [1254.6121826171875, 1257.082275390625, 327.1043395996094]\n",
      "Epoch 377 Loss 28.5250 Current Train,Val,Test Scores [171.71096801757812, 157.34765625, 46.666229248046875]\n",
      "Epoch 378 Loss 28.2494 Current Train,Val,Test Scores [63.819576263427734, 77.45562744140625, 21.337371826171875]\n",
      "Epoch 379 Loss 28.4616 Current Train,Val,Test Scores [57.87577819824219, 71.14506530761719, 19.8222713470459]\n",
      "Epoch 380 Loss 30.3538 Current Train,Val,Test Scores [115.30975341796875, 128.56246948242188, 35.01179122924805]\n",
      "Epoch 381 Loss 39.0256 Current Train,Val,Test Scores [124.00752258300781, 137.52931213378906, 37.25372314453125]\n",
      "Epoch 382 Loss 65.3184 Current Train,Val,Test Scores [153.04995727539062, 166.1614532470703, 44.98735427856445]\n",
      "Epoch 383 Loss 161.6189 Current Train,Val,Test Scores [246.10067749023438, 258.9446716308594, 69.6705322265625]\n",
      "Epoch 384 Loss 461.9485 Current Train,Val,Test Scores [182.01087951660156, 194.21958923339844, 52.811805725097656]\n",
      "Epoch 385 Loss 166.4796 Current Train,Val,Test Scores [3761.32861328125, 3807.701904296875, 982.7444458007812]\n",
      "Epoch 386 Loss 252.9758 Current Train,Val,Test Scores [1701.7872314453125, 1736.4229736328125, 442.580078125]\n",
      "Epoch 387 Loss 310.9171 Current Train,Val,Test Scores [5852.0380859375, 5904.796875, 1536.2216796875]\n",
      "Epoch 388 Loss 281.1291 Current Train,Val,Test Scores [10172.1474609375, 10206.919921875, 2681.283935546875]\n",
      "Epoch 389 Loss 337.4221 Current Train,Val,Test Scores [1331.0064697265625, 1368.842529296875, 343.6388854980469]\n",
      "Epoch 390 Loss 207.8297 Current Train,Val,Test Scores [80.17301940917969, 90.40699768066406, 25.78200340270996]\n",
      "Epoch 391 Loss 244.3819 Current Train,Val,Test Scores [171.32559204101562, 175.92587280273438, 50.70490264892578]\n",
      "Epoch 392 Loss 244.7149 Current Train,Val,Test Scores [287.1106262207031, 283.6348571777344, 81.26649475097656]\n",
      "Epoch 393 Loss 154.2628 Current Train,Val,Test Scores [433.9905700683594, 421.2738952636719, 119.40863037109375]\n",
      "Epoch 394 Loss 166.2243 Current Train,Val,Test Scores [535.9351196289062, 522.6278076171875, 145.82080078125]\n",
      "Epoch 395 Loss 121.7323 Current Train,Val,Test Scores [485.4464111328125, 471.96343994140625, 132.90699768066406]\n",
      "Epoch 396 Loss 165.3727 Current Train,Val,Test Scores [341.8537902832031, 333.66229248046875, 95.73663330078125]\n",
      "Epoch 397 Loss 128.5066 Current Train,Val,Test Scores [202.2021942138672, 205.18292236328125, 59.08620071411133]\n",
      "Epoch 398 Loss 129.8381 Current Train,Val,Test Scores [164.1186065673828, 169.42758178710938, 48.94443130493164]\n",
      "Epoch 399 Loss 111.3777 Current Train,Val,Test Scores [163.0410614013672, 168.46229553222656, 48.657169342041016]\n",
      "Epoch 400 Loss 114.5160 Current Train,Val,Test Scores [30.80800437927246, 42.29563522338867, 13.348047256469727]\n",
      "Epoch 401 Loss 103.8083 Current Train,Val,Test Scores [8439.6015625, 8447.0361328125, 2228.531494140625]\n",
      "Epoch 402 Loss 84.9455 Current Train,Val,Test Scores [12967.3828125, 12976.0234375, 3424.556640625]\n",
      "Epoch 403 Loss 90.0934 Current Train,Val,Test Scores [15827.8935546875, 15835.8681640625, 4180.9013671875]\n",
      "Epoch 404 Loss 78.7374 Current Train,Val,Test Scores [16321.1064453125, 16328.58203125, 4311.35498046875]\n",
      "Epoch 405 Loss 89.9864 Current Train,Val,Test Scores [13592.302734375, 13597.7861328125, 3590.021484375]\n",
      "Epoch 406 Loss 69.3165 Current Train,Val,Test Scores [9925.880859375, 9927.0673828125, 2621.667236328125]\n",
      "Epoch 407 Loss 80.2293 Current Train,Val,Test Scores [7721.171875, 7722.88134765625, 2039.147705078125]\n",
      "Epoch 408 Loss 68.1544 Current Train,Val,Test Scores [5342.5927734375, 5291.0966796875, 1422.8560791015625]\n",
      "Epoch 409 Loss 74.4038 Current Train,Val,Test Scores [198.68447875976562, 165.0732421875, 58.480525970458984]\n",
      "Epoch 410 Loss 58.0439 Current Train,Val,Test Scores [709.1176147460938, 723.8697509765625, 192.38546752929688]\n",
      "Epoch 411 Loss 63.0362 Current Train,Val,Test Scores [835.9164428710938, 849.7208251953125, 225.79029846191406]\n",
      "Epoch 412 Loss 54.4039 Current Train,Val,Test Scores [883.2507934570312, 896.8124389648438, 238.37254333496094]\n",
      "Epoch 413 Loss 57.8431 Current Train,Val,Test Scores [956.501953125, 969.95458984375, 257.72369384765625]\n",
      "Epoch 414 Loss 52.9454 Current Train,Val,Test Scores [1100.620361328125, 1114.4871826171875, 295.768798828125]\n",
      "Epoch 415 Loss 59.1818 Current Train,Val,Test Scores [1138.1298828125, 1152.1632080078125, 305.6761169433594]\n",
      "Epoch 416 Loss 51.6538 Current Train,Val,Test Scores [967.4730834960938, 983.793212890625, 260.2076721191406]\n",
      "Epoch 417 Loss 52.1319 Current Train,Val,Test Scores [774.420654296875, 790.563232421875, 209.14614868164062]\n",
      "Epoch 418 Loss 47.6723 Current Train,Val,Test Scores [571.3841552734375, 528.6107788085938, 159.35472106933594]\n",
      "Epoch 419 Loss 47.0941 Current Train,Val,Test Scores [3155.827880859375, 3162.00341796875, 831.2783203125]\n",
      "Epoch 420 Loss 44.9959 Current Train,Val,Test Scores [3673.796875, 3678.745361328125, 967.4822387695312]\n",
      "Epoch 421 Loss 45.3900 Current Train,Val,Test Scores [4805.01806640625, 4809.42529296875, 1266.40869140625]\n",
      "Epoch 422 Loss 42.5535 Current Train,Val,Test Scores [6466.38720703125, 6471.4599609375, 1705.2520751953125]\n",
      "Epoch 423 Loss 40.1651 Current Train,Val,Test Scores [8486.154296875, 8491.2421875, 2239.146240234375]\n",
      "Epoch 424 Loss 39.3447 Current Train,Val,Test Scores [8680.86328125, 8685.06640625, 2290.68310546875]\n",
      "Epoch 425 Loss 37.3087 Current Train,Val,Test Scores [5841.4853515625, 5844.73095703125, 1540.2269287109375]\n",
      "Epoch 426 Loss 36.6477 Current Train,Val,Test Scores [1854.623291015625, 1806.243408203125, 499.57110595703125]\n",
      "Epoch 427 Loss 35.3361 Current Train,Val,Test Scores [594.1708984375, 547.69482421875, 166.79039001464844]\n",
      "Epoch 428 Loss 35.3681 Current Train,Val,Test Scores [654.8070068359375, 671.2223510742188, 177.4823760986328]\n",
      "Epoch 429 Loss 33.6475 Current Train,Val,Test Scores [765.4165649414062, 780.0540771484375, 207.06661987304688]\n",
      "Epoch 430 Loss 33.7994 Current Train,Val,Test Scores [717.7293701171875, 732.1399536132812, 194.46034240722656]\n",
      "Epoch 431 Loss 31.6753 Current Train,Val,Test Scores [635.1649780273438, 649.353271484375, 172.62545776367188]\n",
      "Epoch 432 Loss 32.3839 Current Train,Val,Test Scores [616.2931518554688, 630.4633178710938, 167.697509765625]\n",
      "Epoch 433 Loss 30.6801 Current Train,Val,Test Scores [598.3283081054688, 612.6795043945312, 162.94996643066406]\n",
      "Epoch 434 Loss 31.5368 Current Train,Val,Test Scores [512.0288696289062, 526.3491821289062, 140.09890747070312]\n",
      "Epoch 435 Loss 30.0496 Current Train,Val,Test Scores [424.1521911621094, 438.28436279296875, 116.89388275146484]\n",
      "Epoch 436 Loss 30.4313 Current Train,Val,Test Scores [350.7530212402344, 365.3976745605469, 97.27181243896484]\n",
      "Epoch 437 Loss 29.0086 Current Train,Val,Test Scores [142.3618621826172, 155.37863159179688, 43.881675720214844]\n",
      "Epoch 438 Loss 28.9780 Current Train,Val,Test Scores [561.9283447265625, 554.4288330078125, 149.21653747558594]\n",
      "Epoch 439 Loss 28.0744 Current Train,Val,Test Scores [1024.1483154296875, 1019.801513671875, 269.76605224609375]\n",
      "Epoch 440 Loss 28.0068 Current Train,Val,Test Scores [1325.0899658203125, 1319.4691162109375, 349.7277526855469]\n",
      "Epoch 441 Loss 27.6103 Current Train,Val,Test Scores [1241.830322265625, 1236.8128662109375, 327.69622802734375]\n",
      "Epoch 442 Loss 27.3614 Current Train,Val,Test Scores [927.4146118164062, 926.7531127929688, 244.27810668945312]\n",
      "Epoch 443 Loss 27.1853 Current Train,Val,Test Scores [600.8563842773438, 597.743408203125, 158.2530517578125]\n",
      "Epoch 444 Loss 26.7922 Current Train,Val,Test Scores [303.0610656738281, 300.8622741699219, 83.10018157958984]\n",
      "Epoch 445 Loss 26.7718 Current Train,Val,Test Scores [54.13321304321289, 70.78933715820312, 20.87952423095703]\n",
      "Epoch 446 Loss 26.1190 Current Train,Val,Test Scores [176.5101776123047, 189.32801818847656, 51.28436279296875]\n",
      "Epoch 447 Loss 26.1567 Current Train,Val,Test Scores [216.9434814453125, 231.7352752685547, 61.80499267578125]\n",
      "Epoch 448 Loss 25.5495 Current Train,Val,Test Scores [241.79373168945312, 256.9625244140625, 68.16887664794922]\n",
      "Epoch 449 Loss 25.5623 Current Train,Val,Test Scores [103.34992218017578, 130.46102905273438, 30.013103485107422]\n",
      "Epoch 450 Loss 25.0702 Current Train,Val,Test Scores [264.0565185546875, 243.43946838378906, 77.01060485839844]\n",
      "Epoch 451 Loss 24.9291 Current Train,Val,Test Scores [639.2699584960938, 608.5054321289062, 176.2958526611328]\n",
      "Epoch 452 Loss 24.5644 Current Train,Val,Test Scores [1295.233642578125, 1263.7083740234375, 347.69696044921875]\n",
      "Epoch 453 Loss 24.3215 Current Train,Val,Test Scores [1580.546630859375, 1575.39892578125, 419.03912353515625]\n",
      "Epoch 454 Loss 24.0423 Current Train,Val,Test Scores [1741.3389892578125, 1746.9725341796875, 458.4225769042969]\n",
      "Epoch 455 Loss 23.7034 Current Train,Val,Test Scores [2028.545166015625, 2039.12353515625, 532.2867431640625]\n",
      "Epoch 456 Loss 23.5252 Current Train,Val,Test Scores [2094.1640625, 2104.947998046875, 548.8474731445312]\n",
      "Epoch 457 Loss 23.1799 Current Train,Val,Test Scores [2014.1456298828125, 2024.879638671875, 527.949951171875]\n",
      "Epoch 458 Loss 23.0360 Current Train,Val,Test Scores [1843.711181640625, 1825.730712890625, 490.52996826171875]\n",
      "Epoch 459 Loss 22.6897 Current Train,Val,Test Scores [1236.3868408203125, 1193.9835205078125, 336.7636413574219]\n",
      "Epoch 460 Loss 22.5292 Current Train,Val,Test Scores [708.0818481445312, 668.0426635742188, 197.0989532470703]\n",
      "Epoch 461 Loss 22.1990 Current Train,Val,Test Scores [230.33221435546875, 194.51968383789062, 68.96466827392578]\n",
      "Epoch 462 Loss 21.9761 Current Train,Val,Test Scores [260.5401916503906, 283.0634765625, 71.02204132080078]\n",
      "Epoch 463 Loss 21.7072 Current Train,Val,Test Scores [327.24322509765625, 343.13861083984375, 90.44889068603516]\n",
      "Epoch 464 Loss 21.4454 Current Train,Val,Test Scores [320.9821472167969, 337.3480224609375, 88.80831909179688]\n",
      "Epoch 465 Loss 21.2216 Current Train,Val,Test Scores [324.904296875, 341.25164794921875, 89.95838165283203]\n",
      "Epoch 466 Loss 20.9319 Current Train,Val,Test Scores [323.11529541015625, 339.07415771484375, 89.55559539794922]\n",
      "Epoch 467 Loss 20.7173 Current Train,Val,Test Scores [288.9274597167969, 304.4811096191406, 80.58338165283203]\n",
      "Epoch 468 Loss 20.3969 Current Train,Val,Test Scores [251.84716796875, 266.9466857910156, 70.89466094970703]\n",
      "Epoch 469 Loss 20.1631 Current Train,Val,Test Scores [233.57359313964844, 248.44432067871094, 66.08320617675781]\n",
      "Epoch 470 Loss 19.8531 Current Train,Val,Test Scores [212.05458068847656, 227.00758361816406, 60.42686462402344]\n",
      "Epoch 471 Loss 19.6037 Current Train,Val,Test Scores [186.20565795898438, 205.64028930664062, 53.09909439086914]\n",
      "Epoch 472 Loss 19.3236 Current Train,Val,Test Scores [191.04391479492188, 216.2907257080078, 53.57444381713867]\n",
      "Epoch 473 Loss 19.0546 Current Train,Val,Test Scores [227.8831329345703, 252.8236541748047, 63.055057525634766]\n",
      "Epoch 474 Loss 18.8068 Current Train,Val,Test Scores [236.8596954345703, 264.4798889160156, 65.02630615234375]\n",
      "Epoch 475 Loss 18.5162 Current Train,Val,Test Scores [312.1988220214844, 331.9401550292969, 85.74278259277344]\n",
      "Epoch 476 Loss 18.2632 Current Train,Val,Test Scores [421.5184020996094, 436.108642578125, 115.86638641357422]\n",
      "Epoch 477 Loss 17.9904 Current Train,Val,Test Scores [494.3609313964844, 509.0199890136719, 135.09805297851562]\n",
      "Epoch 478 Loss 17.7289 Current Train,Val,Test Scores [544.0740966796875, 558.9207153320312, 148.24302673339844]\n",
      "Epoch 479 Loss 17.4884 Current Train,Val,Test Scores [598.1507568359375, 613.0781860351562, 162.60154724121094]\n",
      "Epoch 480 Loss 17.2175 Current Train,Val,Test Scores [650.05908203125, 664.9620361328125, 176.29881286621094]\n",
      "Epoch 481 Loss 16.9918 Current Train,Val,Test Scores [675.9144287109375, 690.8795776367188, 183.11477661132812]\n",
      "Epoch 482 Loss 16.7142 Current Train,Val,Test Scores [682.0222778320312, 696.9458618164062, 184.72998046875]\n",
      "Epoch 483 Loss 16.4991 Current Train,Val,Test Scores [675.009765625, 689.86279296875, 182.87347412109375]\n",
      "Epoch 484 Loss 16.2654 Current Train,Val,Test Scores [651.3876953125, 666.2291870117188, 176.60653686523438]\n",
      "Epoch 485 Loss 16.0745 Current Train,Val,Test Scores [626.1617431640625, 641.0885620117188, 169.9234161376953]\n",
      "Epoch 486 Loss 15.8942 Current Train,Val,Test Scores [602.2988891601562, 617.2352905273438, 163.60256958007812]\n",
      "Epoch 487 Loss 15.6731 Current Train,Val,Test Scores [560.3583374023438, 575.2799682617188, 152.48834228515625]\n",
      "Epoch 488 Loss 15.4804 Current Train,Val,Test Scores [509.72772216796875, 524.5917358398438, 139.09120178222656]\n",
      "Epoch 489 Loss 15.2829 Current Train,Val,Test Scores [465.9867858886719, 481.24798583984375, 127.43714904785156]\n",
      "Epoch 490 Loss 15.1182 Current Train,Val,Test Scores [412.008056640625, 426.9301452636719, 113.15991973876953]\n",
      "Epoch 491 Loss 14.9339 Current Train,Val,Test Scores [324.312744140625, 339.4671630859375, 89.8847427368164]\n",
      "Epoch 492 Loss 14.7504 Current Train,Val,Test Scores [221.71592712402344, 248.79739379882812, 60.94905090332031]\n",
      "Epoch 493 Loss 14.5223 Current Train,Val,Test Scores [91.44110870361328, 103.72627258300781, 33.73918914794922]\n",
      "Epoch 494 Loss 14.3847 Current Train,Val,Test Scores [339.7182312011719, 301.71002197265625, 96.24185180664062]\n",
      "Epoch 495 Loss 14.2145 Current Train,Val,Test Scores [514.0040893554688, 478.2774353027344, 142.86160278320312]\n",
      "Epoch 496 Loss 14.0506 Current Train,Val,Test Scores [512.052490234375, 476.2545471191406, 142.29299926757812]\n",
      "Epoch 497 Loss 13.8769 Current Train,Val,Test Scores [254.1966094970703, 219.2751922607422, 73.13299560546875]\n",
      "Epoch 498 Loss 13.7082 Current Train,Val,Test Scores [92.08987426757812, 116.69774627685547, 32.1479377746582]\n",
      "Epoch 499 Loss 13.6558 Current Train,Val,Test Scores [210.14695739746094, 242.26365661621094, 56.84961700439453]\n",
      "Best Train,Val,Test Scores [14.752171516418457, 27.82378387451172, 8.549697875976562]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creates a HeteroGNN model from the previously constructed hetero graph and trains it.\n",
    "\"\"\"\n",
    "\n",
    "best_model = None\n",
    "best_tvt_scores = (float(\"inf\"),float(\"inf\"),float(\"inf\"))\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    # Train\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    # Test for the accuracy of the model\n",
    "    cur_tvt_scores, best_tvt_scores, best_model = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_tvt_scores)\n",
    "    print(f\"Epoch {epoch} Loss {loss:.4f} Current Train,Val,Test Scores {[score.item() for score in cur_tvt_scores]}\")\n",
    "\n",
    "print(\"Best Train,Val,Test Scores\", [score.item() for score in best_tvt_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Score: 28.550447463989258\n",
      "tensor([4.3030], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([7.5159], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([138.2642], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([18.7437], device='cuda:0', grad_fn=<SelectBackward0>) tensor([94.], device='cuda:0')\n",
      "tensor([183.1452], device='cuda:0', grad_fn=<SelectBackward0>) tensor([48.], device='cuda:0')\n",
      "tensor([8.6889], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([189.9236], device='cuda:0', grad_fn=<SelectBackward0>) tensor([111.], device='cuda:0')\n",
      "tensor([8.4420], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([11.8422], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([3.1953], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([7.5582], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([9.0252], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([6.2847], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([21.8396], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([5.2563], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([5.3602], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([55.9655], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([8.6489], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([19.4363], device='cuda:0', grad_fn=<SelectBackward0>) tensor([58.], device='cuda:0')\n",
      "tensor([6.9434], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.2969], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([7.2991], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([5.4638], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([20.5239], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([7.8595], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([5.6486], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([12.3066], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([12.2452], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([232.3356], device='cuda:0', grad_fn=<SelectBackward0>) tensor([32.], device='cuda:0')\n",
      "tensor([11.2970], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([35.0841], device='cuda:0', grad_fn=<SelectBackward0>) tensor([19.], device='cuda:0')\n",
      "tensor([12.1949], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([19.3917], device='cuda:0', grad_fn=<SelectBackward0>) tensor([144.], device='cuda:0')\n",
      "tensor([72.7010], device='cuda:0', grad_fn=<SelectBackward0>) tensor([79.], device='cuda:0')\n",
      "tensor([2.6301], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([6.1514], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([9.7415], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([5.6345], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([4.8670], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([14.3348], device='cuda:0', grad_fn=<SelectBackward0>) tensor([51.], device='cuda:0')\n",
      "tensor([5.8146], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([7.0769], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([131.3581], device='cuda:0', grad_fn=<SelectBackward0>) tensor([40.], device='cuda:0')\n",
      "tensor([5.3878], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([8.9349], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([199.5417], device='cuda:0', grad_fn=<SelectBackward0>) tensor([607.], device='cuda:0')\n",
      "tensor([8.1611], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([7.1065], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([15.6425], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "model.load_state_dict(torch.load('./best_model.pkl'))\n",
    "preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "# mask = preds['event'] > 0\n",
    "# preds['event'][preds['event'] > 0].shape\n",
    "\n",
    "# print(preds['event'][0], hetero_graph.node_target['event'][0]) \n",
    "\n",
    "\n",
    "#print(hetero_graph.node_feature['event'])\n",
    "\n",
    "# Assuming val_idx is the dictionary containing the validation indices for 'event' node type\n",
    "\n",
    "# TODO: Best validation score doesn't match the best validation score from the training loop\n",
    "# best_val = torch.sum(torch.abs(preds['event'][val_idx['event']] - hetero_graph.node_target['event'][val_idx['event']])) / val_idx['event'].shape[0]\n",
    "cur_tvt_scores, best_tvt_scores, best_model = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_tvt_scores)\n",
    "\n",
    "\n",
    "# Print the validation loss\n",
    "print(f\"Best Validation Score: {cur_tvt_scores[1].item()}\")\n",
    "\n",
    "# for i in range(3000):\n",
    "#     if hetero_graph.node_target['event'][i] != -1: # concepts have node target -1\n",
    "#         print(preds['event'][i], hetero_graph.node_target['event'][i])\n",
    "        \n",
    "    \n",
    "for i in range(1000):    \n",
    "    if hetero_graph.node_target['event'][test_idx['event']][i] != -1:\n",
    "        print(preds['event'][test_idx['event']][i], hetero_graph.node_target['event'][test_idx['event']][i])\n",
    "\n",
    "\n",
    "# for i in range(1000):\n",
    "#     # Extract the target value and check if it is not equal to -1\n",
    "#     target = hetero_graph.node_target['event'][test_idx['event']][i].item()\n",
    "#     if target != -1:\n",
    "#         # Extract the prediction value\n",
    "#         pred = preds['event'][test_idx['event']][i].item()\n",
    "#         print(f\"Prediction: {pred:.4f}, Target: {target:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2023-11-17 15:42:53,067] A new study created in memory with name: no-name-8c7c805b-21f1-45da-951b-ccd6bb051623\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamgrobelnik\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\train\\gnn_llm\\wandb\\run-20231117_154255-m8h22vpw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/m8h22vpw' target=\"_blank\">grateful-butterfly-10</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/m8h22vpw' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/m8h22vpw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>█▇▅▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>███▇▇▇▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▆▃▄▄▆▅▂▅▄▁▁▁▁█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>28.58196</td></tr><tr><td>epoch</td><td>196</td></tr><tr><td>train_loss</td><td>960.5827</td></tr><tr><td>val_score</td><td>115.50377</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-butterfly-10</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/m8h22vpw' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/m8h22vpw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231117_154255-m8h22vpw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-17 15:43:21,016] Trial 0 finished with value: 28.58196258544922 and parameters: {'lr': 0.0005128759386601744, 'weight_decay': 0.00011143511561573684, 'hidden_size': 71, 'epochs': 197}. Best is trial 0 with value: 28.58196258544922.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\train\\gnn_llm\\wandb\\run-20231117_154321-40mnfh93</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/40mnfh93' target=\"_blank\">celestial-river-11</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/40mnfh93' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/40mnfh93</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>█▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>███████▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>val_score</td><td>█▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>28.34345</td></tr><tr><td>epoch</td><td>244</td></tr><tr><td>train_loss</td><td>2319.35352</td></tr><tr><td>val_score</td><td>28.34345</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">celestial-river-11</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/40mnfh93' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/40mnfh93</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231117_154321-40mnfh93\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-17 15:43:50,660] Trial 1 finished with value: 28.34345054626465 and parameters: {'lr': 3.506501759862614e-05, 'weight_decay': 0.0003395052165948722, 'hidden_size': 110, 'epochs': 245}. Best is trial 1 with value: 28.34345054626465.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\train\\gnn_llm\\wandb\\run-20231117_154350-2nb0tyns</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/2nb0tyns' target=\"_blank\">northern-field-12</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/2nb0tyns' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/2nb0tyns</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>████████████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>████████▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>val_score</td><td>███████████▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>30.39174</td></tr><tr><td>epoch</td><td>238</td></tr><tr><td>train_loss</td><td>2590.52368</td></tr><tr><td>val_score</td><td>30.39174</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">northern-field-12</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/2nb0tyns' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/2nb0tyns</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231117_154350-2nb0tyns\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-17 15:44:14,747] Trial 2 finished with value: 30.391735076904297 and parameters: {'lr': 2.911051528768232e-05, 'weight_decay': 1.0138805940007106e-05, 'hidden_size': 58, 'epochs': 239}. Best is trial 1 with value: 28.34345054626465.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\train\\gnn_llm\\wandb\\run-20231117_154414-7c96gxo2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/7c96gxo2' target=\"_blank\">visionary-planet-13</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/7c96gxo2' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/7c96gxo2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>████▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>val_score</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>34.08345</td></tr><tr><td>epoch</td><td>150</td></tr><tr><td>train_loss</td><td>2819.29419</td></tr><tr><td>val_score</td><td>34.11644</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">visionary-planet-13</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/7c96gxo2' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/7c96gxo2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231117_154414-7c96gxo2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-17 15:44:31,835] Trial 3 finished with value: 34.08344650268555 and parameters: {'lr': 2.1254522410732966e-05, 'weight_decay': 1.57423049376229e-05, 'hidden_size': 19, 'epochs': 151}. Best is trial 1 with value: 28.34345054626465.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\train\\gnn_llm\\wandb\\run-20231117_154431-eg43lsml</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/eg43lsml' target=\"_blank\">snowy-vortex-14</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/eg43lsml' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/eg43lsml</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>█▇▇▇▇▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>████▇▇▇▇▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_score</td><td>▂▂▁▁▁▁▁▁▁▁▂▂▃▄▄▃▃▂▂▂▂▂▂▂▃▃▄▄▄▄▅▆▇██▅▁▅▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>28.73788</td></tr><tr><td>epoch</td><td>165</td></tr><tr><td>train_loss</td><td>1547.87</td></tr><tr><td>val_score</td><td>56.0388</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-vortex-14</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/eg43lsml' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/eg43lsml</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231117_154431-eg43lsml\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-17 15:44:52,202] Trial 4 finished with value: 28.73788070678711 and parameters: {'lr': 0.00024145035285286874, 'weight_decay': 9.941609318037565e-05, 'hidden_size': 96, 'epochs': 166}. Best is trial 1 with value: 28.34345054626465.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\train\\gnn_llm\\wandb\\run-20231117_154452-j1n0qmxn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/j1n0qmxn' target=\"_blank\">rural-sponge-15</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/j1n0qmxn' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/j1n0qmxn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>█▇▆▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>████▇▇▇▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▂▄▂▄▁▄▆▂▃▄▂▁▂▃▁██▆▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>27.62709</td></tr><tr><td>epoch</td><td>231</td></tr><tr><td>train_loss</td><td>800.51306</td></tr><tr><td>val_score</td><td>34.79512</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rural-sponge-15</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/j1n0qmxn' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/j1n0qmxn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231117_154452-j1n0qmxn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-17 15:45:14,880] Trial 5 finished with value: 27.62708854675293 and parameters: {'lr': 0.0005830498619745771, 'weight_decay': 3.0750473636308886e-05, 'hidden_size': 57, 'epochs': 232}. Best is trial 5 with value: 27.62708854675293.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\train\\gnn_llm\\wandb\\run-20231117_154514-jdno9xui</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/jdno9xui' target=\"_blank\">prime-disco-16</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/jdno9xui' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/jdno9xui</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▃▃▃▃▂▂▂▂▁▁▂▂▂▅▆▇▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>val_score</td><td>▄▃▁▁▂▄▁▁▁▁▁▁▁▁▁▂▁▁█▂▁▂▁▁▁▄▅▃▁▁▁▁▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>31.53897</td></tr><tr><td>epoch</td><td>200</td></tr><tr><td>train_loss</td><td>208.72409</td></tr><tr><td>val_score</td><td>126.51942</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prime-disco-16</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/jdno9xui' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/jdno9xui</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231117_154514-jdno9xui\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-17 15:45:34,337] Trial 6 finished with value: 31.53897476196289 and parameters: {'lr': 0.07535688363628285, 'weight_decay': 0.0008031969490745124, 'hidden_size': 35, 'epochs': 201}. Best is trial 5 with value: 27.62708854675293.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\train\\gnn_llm\\wandb\\run-20231117_154534-fgllevea</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/fgllevea' target=\"_blank\">robust-moon-17</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/fgllevea' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/fgllevea</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>█▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>██▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂█▃▁▃▁▇▁█▂█▆▁▁▁▂▅▁▄▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>29.22615</td></tr><tr><td>epoch</td><td>248</td></tr><tr><td>train_loss</td><td>60.63571</td></tr><tr><td>val_score</td><td>202.49635</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">robust-moon-17</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/fgllevea' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/fgllevea</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231117_154534-fgllevea\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-17 15:45:54,768] Trial 7 finished with value: 29.226150512695312 and parameters: {'lr': 0.002646462803964806, 'weight_decay': 8.835572283522618e-05, 'hidden_size': 32, 'epochs': 249}. Best is trial 5 with value: 27.62708854675293.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\train\\gnn_llm\\wandb\\run-20231117_154554-poav3tyt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/poav3tyt' target=\"_blank\">smart-sun-18</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/poav3tyt' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/poav3tyt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>████████████████████████▇▇▇▆▆▅▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█████████▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>val_score</td><td>▆▇▇▇██████████████▇▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>30.21928</td></tr><tr><td>epoch</td><td>154</td></tr><tr><td>train_loss</td><td>2579.76538</td></tr><tr><td>val_score</td><td>30.21928</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smart-sun-18</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/poav3tyt' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/poav3tyt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231117_154554-poav3tyt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-17 15:46:17,180] Trial 8 finished with value: 30.219282150268555 and parameters: {'lr': 2.315772786035054e-05, 'weight_decay': 0.00012081525803156375, 'hidden_size': 116, 'epochs': 155}. Best is trial 5 with value: 27.62708854675293.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\train\\gnn_llm\\wandb\\run-20231117_154617-9nxls8jq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/9nxls8jq' target=\"_blank\">jumping-sky-19</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/9nxls8jq' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/9nxls8jq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>██████████████▇▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█████████▇▇▇▇▇▇▇▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>val_score</td><td>▇▇█████▇▇▇▇▇▆▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>28.1071</td></tr><tr><td>epoch</td><td>292</td></tr><tr><td>train_loss</td><td>2266.1582</td></tr><tr><td>val_score</td><td>28.1071</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jumping-sky-19</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/9nxls8jq' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/9nxls8jq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231117_154617-9nxls8jq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-17 15:46:43,635] Trial 9 finished with value: 28.107099533081055 and parameters: {'lr': 4.8317574052644374e-05, 'weight_decay': 4.429478041930544e-05, 'hidden_size': 64, 'epochs': 293}. Best is trial 5 with value: 27.62708854675293.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\train\\gnn_llm\\wandb\\run-20231117_154643-bpbb1xwi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/bpbb1xwi' target=\"_blank\">neat-universe-20</a></strong> to <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/bpbb1xwi' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/bpbb1xwi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>██████████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_score</td><td>▁▁▁▁▁▁▁▂▁▁▁▁▅▄▂█▁▁▇▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_score</td><td>23.98135</td></tr><tr><td>epoch</td><td>280</td></tr><tr><td>train_loss</td><td>99.29585</td></tr><tr><td>val_score</td><td>140.05194</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">neat-universe-20</strong> at: <a href='https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/bpbb1xwi' target=\"_blank\">https://wandb.ai/amgrobelnik/V2_MLG_PredEvents_GNN%2BLMM/runs/bpbb1xwi</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231117_154643-bpbb1xwi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-17 15:47:12,484] Trial 10 finished with value: 23.981351852416992 and parameters: {'lr': 0.0035050315664344163, 'weight_decay': 2.7717728682482574e-05, 'hidden_size': 84, 'epochs': 281}. Best is trial 10 with value: 23.981351852416992.\n",
      "[W 2023-11-17 15:47:15,227] Trial 11 failed with parameters: {'lr': 0.0030133056083382513, 'weight_decay': 2.366877322726593e-05, 'hidden_size': 92, 'epochs': 289} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Adrian\\AppData\\Local\\Temp\\ipykernel_148\\1583648187.py\", line 6, in objective\n",
      "    wandb.init(project=\"V2_MLG_PredEvents_GNN+LMM\", config={\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 1189, in init\n",
      "    raise e\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 1166, in init\n",
      "    run = wi.init()\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 798, in init\n",
      "    run._on_init()\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 2258, in _on_init\n",
      "    version_result = version_handle.wait(timeout=30)\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 283, in wait\n",
      "    found, abandoned = self._slot._get_and_clear(timeout=wait_timeout)\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 130, in _get_and_clear\n",
      "    if self._wait(timeout=timeout):\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 126, in _wait\n",
      "    return self._event.wait(timeout=timeout)\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\threading.py\", line 607, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"c:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\threading.py\", line 324, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "[W 2023-11-17 15:47:15,230] Trial 11 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: C:\\Users\\Adrian\\AppData\\Local\\Temp\\ipykernel_148\\1583648187.py 6 objective\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\train\\gnn_llm\\train_gnn_llm.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Create a study object and optimize the objective function\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Print the best hyperparameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32mc:\\Users\\Adrian\\AI Lab Dropbox\\Adrian Mladenić Grobelnik\\Areas\\Uni\\MLG\\MLG-Predicting-Impacful-Events\\train\\gnn_llm\\train_gnn_llm.ipynb Cell 15\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjective\u001b[39m(trial):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Initialize wandb run\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     wandb\u001b[39m.\u001b[39;49minit(project\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mV2_MLG_PredEvents_GNN+LMM\u001b[39;49m\u001b[39m\"\u001b[39;49m, config\u001b[39m=\u001b[39;49m{\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m: trial\u001b[39m.\u001b[39;49msuggest_float(\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1e-5\u001b[39;49m, \u001b[39m1e-1\u001b[39;49m, log\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m: trial\u001b[39m.\u001b[39;49msuggest_float(\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1e-5\u001b[39;49m, \u001b[39m1e-3\u001b[39;49m, log\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mhidden_size\u001b[39;49m\u001b[39m\"\u001b[39;49m: trial\u001b[39m.\u001b[39;49msuggest_int(\u001b[39m\"\u001b[39;49m\u001b[39mhidden_size\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m16\u001b[39;49m, \u001b[39m128\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mattn_size\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m32\u001b[39;49m,  \u001b[39m# Fixed value\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m\"\u001b[39;49m: trial\u001b[39m.\u001b[39;49msuggest_int(\u001b[39m\"\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m150\u001b[39;49m, \u001b[39m300\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mnum_layers\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m2\u001b[39;49m,  \u001b[39m# Fixed value\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     })\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Use wandb config\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adrian/AI%20Lab%20Dropbox/Adrian%20Mladeni%C4%87%20Grobelnik/Areas/Uni/MLG/MLG-Predicting-Impacful-Events/train/gnn_llm/train_gnn_llm.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     config \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39mconfig\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1189\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     \u001b[39massert\u001b[39;00m logger\n\u001b[0;32m   1188\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39minterrupted\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39me)\n\u001b[1;32m-> 1189\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m   1190\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1191\u001b[0m     error_seen \u001b[39m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1166\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1164\u001b[0m except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[0;32m   1165\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1166\u001b[0m     run \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39;49minit()\n\u001b[0;32m   1167\u001b[0m     except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[0;32m   1168\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:798\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    794\u001b[0m         tel\u001b[39m.\u001b[39mfeature\u001b[39m.\u001b[39mresumed \u001b[39m=\u001b[39m run_result\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mresumed\n\u001b[0;32m    796\u001b[0m run\u001b[39m.\u001b[39m_set_run_obj(run_result\u001b[39m.\u001b[39mrun)\n\u001b[1;32m--> 798\u001b[0m run\u001b[39m.\u001b[39;49m_on_init()\n\u001b[0;32m    800\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mstarting run threads in backend\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    801\u001b[0m \u001b[39m# initiate run (stats and metadata probing)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:2258\u001b[0m, in \u001b[0;36mRun._on_init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2254\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mcommunicating current version\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2255\u001b[0m version_handle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39minterface\u001b[39m.\u001b[39mdeliver_check_version(\n\u001b[0;32m   2256\u001b[0m     current_version\u001b[39m=\u001b[39mwandb\u001b[39m.\u001b[39m__version__\n\u001b[0;32m   2257\u001b[0m )\n\u001b[1;32m-> 2258\u001b[0m version_result \u001b[39m=\u001b[39m version_handle\u001b[39m.\u001b[39;49mwait(timeout\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n\u001b[0;32m   2259\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m version_result:\n\u001b[0;32m   2260\u001b[0m     version_handle\u001b[39m.\u001b[39mabandon()\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[1;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interface\u001b[39m.\u001b[39m_transport_keepalive_failed():\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m MailboxError(\u001b[39m\"\u001b[39m\u001b[39mtransport failed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 283\u001b[0m found, abandoned \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slot\u001b[39m.\u001b[39;49m_get_and_clear(timeout\u001b[39m=\u001b[39;49mwait_timeout)\n\u001b[0;32m    284\u001b[0m \u001b[39mif\u001b[39;00m found:\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Always update progress to 100% when done\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39mif\u001b[39;00m on_progress \u001b[39mand\u001b[39;00m progress_handle \u001b[39mand\u001b[39;00m progress_sent:\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_and_clear\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Optional[pb\u001b[39m.\u001b[39mResult], \u001b[39mbool\u001b[39m]:\n\u001b[0;32m    129\u001b[0m     found \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait(timeout\u001b[39m=\u001b[39;49mtimeout):\n\u001b[0;32m    131\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m             found \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wait\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout\u001b[39m=\u001b[39;49mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\mlg\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[0;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Initialize wandb run\n",
    "    wandb.init(project=\"V2_MLG_PredEvents_GNN+LMM\", config={\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True),\n",
    "        \"hidden_size\": trial.suggest_int(\"hidden_size\", 16, 128),\n",
    "        \"attn_size\": 32,  # Fixed value\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 150, 300),\n",
    "        \"num_layers\": 2,  # Fixed value\n",
    "    })\n",
    "\n",
    "    # Use wandb config\n",
    "    config = wandb.config\n",
    "\n",
    "    # Initialize the model with the new hyperparameters\n",
    "    model = HeteroGNN(hetero_graph, {\n",
    "        'hidden_size': config.hidden_size,\n",
    "        'attn_size': config.attn_size,\n",
    "        'device': args['device']\n",
    "    }, num_layers=config.num_layers, aggr=\"mean\").to(args['device'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "    # Initialize best scores with infinity\n",
    "    best_tvt_scores = (float(\"inf\"), float(\"inf\"), float(\"inf\"))\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "        cur_tvt_scores, best_tvt_scores, _ = test(model, hetero_graph, [train_idx, val_idx, test_idx], None, best_tvt_scores)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_score\": cur_tvt_scores[1],\n",
    "            \"best_val_score\": best_tvt_scores[1],\n",
    "        })\n",
    "\n",
    "        # Update the best validation score\n",
    "        if cur_tvt_scores[1] < best_tvt_scores[1]:\n",
    "            best_tvt_scores = (cur_tvt_scores[0], cur_tvt_scores[1], cur_tvt_scores[2])\n",
    "\n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "    # The objective value is the best validation score\n",
    "    return best_tvt_scores[1]\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=500)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"Value: {trial.value}\")\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
