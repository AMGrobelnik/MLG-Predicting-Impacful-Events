{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import deepsnap\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from deepsnap.hetero_gnn import forward_op\n",
    "from deepsnap.hetero_graph import HeteroGraph\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
    "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels_src = in_channels_src\n",
    "        self.in_channels_dst = in_channels_dst\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.lin_dst = None\n",
    "        self.lin_src = None\n",
    "\n",
    "        self.lin_update = None\n",
    "\n",
    "        self.lin_dst = nn.Linear(in_channels_dst, out_channels)\n",
    "        self.lin_src = nn.Linear(in_channels_src, out_channels)\n",
    "        self.lin_update = nn.Linear(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_feature_src,\n",
    "        node_feature_dst,\n",
    "        edge_index,\n",
    "        size=None,\n",
    "        res_n_id=None,\n",
    "        ):\n",
    "\n",
    "        return self.propagate(edge_index, node_feature_src=node_feature_src, \n",
    "                    node_feature_dst=node_feature_dst, size=size, res_n_id=res_n_id)\n",
    "\n",
    "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
    "\n",
    "        out = matmul(edge_index, node_feature_src, reduce='mean')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out, node_feature_dst, res_n_id):\n",
    "\n",
    "        dst_out = self.lin_dst(node_feature_dst)\n",
    "        aggr_out = self.lin_src(aggr_out)\n",
    "        aggr_out = torch.cat([dst_out, aggr_out], -1)\n",
    "        aggr_out = self.lin_update(aggr_out)\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
    "    def __init__(self, convs, args, aggr=\"mean\"):\n",
    "        \"\"\"\n",
    "        Initializes the HeteroGNNWrapperConv instance.\n",
    "\n",
    "        :param convs: Dictionary of convolution layers for each message type.\n",
    "        :param args: Arguments dictionary containing hyperparameters like hidden_size and attn_size.\n",
    "        :param aggr: Aggregation method, defaults to 'mean'.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
    "        self.aggr = aggr\n",
    "\n",
    "        # Map the index and message type\n",
    "        self.mapping = {}\n",
    "\n",
    "        # A numpy array that stores the final attention probability\n",
    "        self.alpha = None\n",
    "\n",
    "        self.attn_proj = None\n",
    "\n",
    "        if self.aggr == \"attn\":\n",
    "\n",
    "            self.attn_proj = nn.Sequential(\n",
    "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args['attn_size'], 1, bias=False)\n",
    "            )\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        super(HeteroGNNWrapperConv, self).reset_parameters()\n",
    "        if self.aggr == \"attn\":\n",
    "            for layer in self.attn_proj.children():\n",
    "                layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, node_features, edge_indices):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_features: Dictionary of node features for each node type.\n",
    "        :param edge_indices: Dictionary of edge indices for each message type.\n",
    "        :return: Aggregated node embeddings for each node type.\n",
    "        \"\"\"\n",
    "        \n",
    "        message_type_emb = {}\n",
    "        for message_key, message_type in edge_indices.items():\n",
    "            src_type, edge_type, dst_type = message_key\n",
    "            node_feature_src = node_features[src_type]\n",
    "            node_feature_dst = node_features[dst_type]\n",
    "            edge_index = edge_indices[message_key]\n",
    "            message_type_emb[message_key] = (\n",
    "                self.convs[message_key](\n",
    "                    node_feature_src,\n",
    "                    node_feature_dst,\n",
    "                    edge_index,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        \n",
    "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
    "        mapping = {}        \n",
    "        \n",
    "        for (src, edge_type, dst), item in message_type_emb.items():\n",
    "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
    "            node_emb[dst].append(item)\n",
    "        self.mapping = mapping\n",
    "        \n",
    "        for node_type, embs in node_emb.items():\n",
    "            if len(embs) == 1:\n",
    "                node_emb[node_type] = embs[0]\n",
    "            else:\n",
    "                node_emb[node_type] = self.aggregate(embs)\n",
    "                \n",
    "        return node_emb\n",
    "    \n",
    "    def aggregate(self, xs):\n",
    "        \"\"\"\n",
    "        Aggregates node embeddings using the specified aggregation method.\n",
    "\n",
    "        :param xs: List of node embeddings to aggregate.\n",
    "        :return: Aggregated node embeddings as a torch.Tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.aggr == \"mean\":\n",
    "            xs = torch.stack(xs)\n",
    "            out = torch.mean(xs, dim=0)\n",
    "            return out\n",
    "\n",
    "        elif self.aggr == \"attn\":\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            s = self.attn_proj(xs).squeeze(-1)\n",
    "            s = torch.mean(s, dim=-1)\n",
    "            self.alpha = torch.softmax(s, dim=0).detach()\n",
    "            out = self.alpha.reshape(-1, 1, 1) * xs\n",
    "            out = torch.sum(out, dim=0)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates convolutional layers for each message type in a heterogeneous graph.\n",
    "\n",
    "    :param hetero_graph: The heterogeneous graph for which convolutions are to be created.\n",
    "    :param conv: The convolutional layer class or constructor.\n",
    "    :param hidden_size: The number of features in the hidden layer.\n",
    "    :param first_layer: Boolean indicating if this is the first layer in the network.\n",
    "    \n",
    "    :return: A dictionary of convolutional layers, keyed by message type.\n",
    "    \"\"\"\n",
    "\n",
    "    convs = {}\n",
    "    \n",
    "    # Extracting all types of messages/edges in the heterogeneous graph.\n",
    "    all_messages_types = hetero_graph.message_types\n",
    "    for message_type in all_messages_types:\n",
    "        \n",
    "        # Determine the input feature size for source and destination nodes.\n",
    "        # If it's the first layer, use the feature size of the nodes.\n",
    "        # Otherwise, use the hidden size, since from there on the size of embeddings\n",
    "        # is the same for all nodes.\n",
    "        if first_layer:\n",
    "            in_channels_src = hetero_graph.num_node_features(message_type[0])\n",
    "            in_channels_dst = hetero_graph.num_node_features(message_type[2])\n",
    "        else:\n",
    "            in_channels_src = hidden_size\n",
    "            in_channels_dst = hidden_size\n",
    "        out_channels = hidden_size\n",
    "        \n",
    "        # Create a convolutional layer for this message type and add it to the dictionary.\n",
    "        convs[message_type] = conv(in_channels_src, in_channels_dst, out_channels)\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hetero_graph, args, num_layers, aggr=\"mean\"):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "\n",
    "        self.aggr = aggr\n",
    "        self.hidden_size = args['hidden_size']\n",
    "\n",
    "        self.bns1 = nn.ModuleDict()\n",
    "        self.bns2 = nn.ModuleDict()\n",
    "        self.relus1 = nn.ModuleDict()\n",
    "        self.relus2 = nn.ModuleDict()\n",
    "        self.post_mps = nn.ModuleDict()\n",
    "        self.fc = nn.ModuleDict()\n",
    "        \n",
    "        # Initialize the graph convolutional layers\n",
    "        self.convs1 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True), \n",
    "            args, self.aggr)\n",
    "        self.convs2 = HeteroGNNWrapperConv(\n",
    "            generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=False), \n",
    "            args, self.aggr)\n",
    "\n",
    "        # Initialize batch normalization, ReLU, and fully connected layers for each node type\n",
    "        all_node_types = hetero_graph.node_types\n",
    "        for node_type in all_node_types:\n",
    "            \n",
    "            self.bns1[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            self.bns2[node_type] = nn.BatchNorm1d(self.hidden_size, eps=1.0)\n",
    "            \n",
    "            self.relus1[node_type] = nn.LeakyReLU()\n",
    "            self.relus2[node_type] = nn.LeakyReLU()\n",
    "            self.fc[node_type] = nn.Linear(self.hidden_size, 1)\n",
    "            \n",
    "    def forward(self, node_feature, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        :param node_feature: Dictionary of node features for each node type.\n",
    "        :param edge_index: Dictionary of edge indices for each message type.\n",
    "        :return: The output embeddings for each node type after passing through the model.\n",
    "        \"\"\"\n",
    "        x = node_feature\n",
    "\n",
    "        # Apply graph convolutional, batch normalization, and ReLU layers\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = forward_op(x, self.bns1)\n",
    "        x = forward_op(x, self.relus1)\n",
    "\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = forward_op(x, self.bns2)\n",
    "        x = forward_op(x, self.relus2)\n",
    "        \n",
    "        x = forward_op(x, self.fc)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, preds, y, indices):\n",
    "        \"\"\"\n",
    "        Computes the loss for the model.\n",
    "\n",
    "        :param preds: Predictions made by the model.\n",
    "        :param y: Ground truth target values.\n",
    "        :param indices: Indices of nodes for which loss should be calculated.\n",
    "        \n",
    "        :return: The computed loss value.\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = 0\n",
    "        loss_func = torch.nn.MSELoss() \n",
    "             \n",
    "        mask = y['event'][indices['event'], 0] != -1\n",
    "        non_zero_idx = torch.masked_select(indices['event'], mask)\n",
    "                \n",
    "        loss += loss_func(preds['event'][non_zero_idx], y['event'][non_zero_idx])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, hetero_graph, train_idx):\n",
    "    \"\"\"\n",
    "    Trains the model on the given heterogeneous graph using the specified indices.\n",
    "\n",
    "    :param model: The graph neural network model to train.\n",
    "    :param optimizer: The optimizer used for training the model.\n",
    "    :param hetero_graph: The heterogeneous graph data.\n",
    "    :param train_idx: Indices for training nodes.\n",
    "\n",
    "    :return: The training loss as a float.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "    optimizer.zero_grad() # Zero out any existing gradients \n",
    "    \n",
    "    # Compute predictions using the model\n",
    "    # TODO: Use only train_idx instead of edge_index\n",
    "    # TODO: Train only on events not on concepts\n",
    "    \n",
    "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "    # Compute the loss using model's loss function\n",
    "    loss = model.loss(preds, hetero_graph.node_target, train_idx)\n",
    "\n",
    "    loss.backward() # Backward pass: compute gradient of the loss\n",
    "    optimizer.step() # Perform a single optimization step, updates parameters\n",
    "    \n",
    "    return loss.item() \n",
    "\n",
    "def test(model, graph, indices, best_model, best_tvt_scores):\n",
    "    \"\"\"\n",
    "    Tests the model on given indices and updates the best model based on validation loss.\n",
    "\n",
    "    :param model: The trained graph neural network model.\n",
    "    :param graph: The heterogeneous graph data.\n",
    "    :param indices: List of indices for training, validation, and testing nodes.\n",
    "    :param best_model: The current best model based on validation loss.\n",
    "    :param best_val: The current best validation loss.\n",
    "    \n",
    "    :return: A tuple containing the list of losses for each dataset, the best model, and the best validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    tvt_scores = []\n",
    "    \n",
    "    # Evaluate the model on each set of indices\n",
    "    for index in indices:\n",
    "        preds = model(graph.node_feature, graph.edge_index)\n",
    "        \n",
    "        idx = index['event']\n",
    "        \n",
    "                     \n",
    "        # mask = y['event'][indices['event'], 0] != -1\n",
    "        # non_zero_idx = torch.masked_select(indices['event'], mask)\n",
    "        #preds['event'][non_zero_idx], y['event'][non_zero_idx]\n",
    "        \n",
    "        # non_zero_targets = torch.masked_select(graph.node_target['event'][indices['event']], mask)\n",
    "        # non_zero_truth = torch.masked_select(graph.node_target['event'][indices['event']], mask)\n",
    "        \n",
    "        mask = graph.node_target['event'][idx, 0] != -1\n",
    "        non_zero_idx = torch.masked_select(idx, mask)\n",
    "        \n",
    "        \n",
    "        L1 = torch.sum(torch.abs(preds['event'][non_zero_idx] - graph.node_target['event'][non_zero_idx])) / non_zero_idx.shape[0]\n",
    "        \n",
    "        tvt_scores.append(L1)\n",
    "    \n",
    "    # Update the best model and validation loss if the current model performs better\n",
    "    if tvt_scores[1] < best_tvt_scores[1]:\n",
    "        best_tvt_scores = tvt_scores\n",
    "        # torch.to_pickle(model, 'best_model.pkl')\n",
    "        # model.to_pickle('best_model.pkl')\n",
    "        # best_model = copy.deepcopy(model)\n",
    "        torch.save(model.state_dict(), './best_model.pkl')\n",
    "    \n",
    "    return tvt_scores, best_tvt_scores, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'hidden_size': 48,\n",
    "    'epochs': 500,\n",
    "    'weight_decay': 0.0002930387278908051,\n",
    "    'lr': 0.05091434725288385,\n",
    "    'attn_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell creates a small heterogeneous graph, primarily for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "S_node_feature = {\n",
    "    \"event\": torch.tensor([\n",
    "                [1, 1, 1],   # event 0\n",
    "                [2, 2, 2]    # event 1\n",
    "    ], dtype=torch.float32),\n",
    "    \"concept\": torch.tensor([\n",
    "                [2, 2, 2],   # concept 0\n",
    "                [3, 3, 3]    # concept 1\n",
    "    ], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "# S_node_label = {\n",
    "#     \"event\": torch.tensor([0, 1], dtype=torch.long), # Class 0, Class 1\n",
    "#     \"concept\": torch.tensor([0, 1], dtype=torch.long)  # Class 0, Class 1\n",
    "# }\n",
    "\n",
    "S_node_targets = {\n",
    "    \"event\": torch.tensor([[50], [2000]], dtype=torch.float32),\n",
    "    # \"concept\": torch.tensor([[0], [0]], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "S_edge_index = {\n",
    "    (\"event\", \"similar\", \"event\"): torch.tensor([[0,1],[1,0]], dtype=torch.int64),\n",
    "    (\"event\", \"related\", \"concept\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64),\n",
    "    (\"concept\", \"related\", \"event\"): torch.tensor([[0,1],[0,1]], dtype=torch.int64)\n",
    "}\n",
    "\n",
    "# Testing\n",
    "hetero_graph = HeteroGraph(\n",
    "    node_feature=S_node_feature,\n",
    "    node_target=S_node_targets,\n",
    "    edge_index=S_edge_index\n",
    ")\n",
    "\n",
    "train_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "val_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}\n",
    "test_idx = {\"event\": torch.tensor([0, 1]).to(args['device']), \"concept\": torch.tensor([0, 1]).to(args['device'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./1_concepts_similar_llm.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "    # Convert to directed graph for compatibility with Deepsnap\n",
    "    # G = G.to_directed()\n",
    "   \n",
    "        \n",
    "hetero_graph = HeteroGraph(G, netlib=nx, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE ('event', 'similar', 'event')\n",
      "\t Feature 769\n",
      "\t Feature 769\n",
      "TYPE ('event', 'related', 'concept')\n",
      "\t Feature 769\n",
      "\t Feature 1\n",
      "TYPE ('concept', 'related', 'event')\n",
      "\t Feature 1\n",
      "\t Feature 769\n",
      "KEY ('event', 'similar', 'event') <class 'tuple'>\n",
      "KEY NUMS ('event', 'similar', 'event') 8487 8487\n",
      "MAX EDGES tensor(8283) tensor(8283) 8487 8487\n",
      "KEY ('event', 'related', 'concept') <class 'tuple'>\n",
      "KEY NUMS ('event', 'related', 'concept') 8487 8729\n",
      "MAX EDGES tensor(8265) tensor(8728) 8487 8729\n",
      "KEY ('concept', 'related', 'event') <class 'tuple'>\n",
      "KEY NUMS ('concept', 'related', 'event') 8729 8487\n",
      "MAX EDGES tensor(8728) tensor(8265) 8729 8487\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block ensures that all tensors in a heterogeneous graph are transferred to the same \n",
    "computational device, as specified in the 'args' dictionary.\n",
    "\"\"\"\n",
    "\n",
    "for message_type in hetero_graph.message_types:\n",
    "    print(\"TYPE\", message_type)\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[0]))\n",
    "    print(\"\\t Feature\", hetero_graph.num_node_features(message_type[2]))\n",
    "\n",
    "# Send node features to device\n",
    "for key in hetero_graph.node_feature:\n",
    "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
    "\n",
    "# for key in hetero_graph.node_label:\n",
    "#     hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
    "\n",
    "# Create a torch.SparseTensor from edge_index and send it to device\n",
    "for key in hetero_graph.edge_index:\n",
    "    print(\"KEY\", key, type(key))\n",
    "    print(\"KEY NUMS\", key, hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    \n",
    "    edge_index = hetero_graph.edge_index[key]\n",
    "\n",
    "    print(\"MAX EDGES\", edge_index[0].max(), edge_index[1].max(), hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2]))\n",
    "    adj = SparseTensor(row=edge_index[0].long(), col=edge_index[1].long(), sparse_sizes=(hetero_graph.num_nodes(key[0]), hetero_graph.num_nodes(key[2])))\n",
    "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
    "    \n",
    "# Send node targets to device\n",
    "for key in hetero_graph.node_target:\n",
    "    hetero_graph.node_target[key] = hetero_graph.node_target[key].to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5940])\n",
      "torch.Size([1698])\n",
      "torch.Size([849])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code block creates a basic split of a graph's nodes into training, validation, and testing sets. \n",
    "It uses predefined ratios to divide 'event' and 'concept' nodes in the heterogeneous graph for a simple \n",
    "dataset split, mainly for testing purposes.\n",
    "\"\"\"\n",
    "\n",
    "nEvents = hetero_graph.num_nodes(\"event\")\n",
    "nConcepts = hetero_graph.num_nodes(\"concept\")\n",
    "\n",
    "s1 = 0.7\n",
    "s2 = 0.8\n",
    "\n",
    "train_idx = {   \"event\": torch.tensor(range(0, int(nEvents * s1))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(0, int(nConcepts * s1))).to(args[\"device\"])\n",
    "            }\n",
    "val_idx = {   \"event\": torch.tensor(range(int(nEvents * s1), int(nEvents * s2))).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s1), int(nConcepts * s2))).to(args[\"device\"])\n",
    "            }\n",
    "test_idx = {   \"event\": torch.tensor(range(int(nEvents * s2), nEvents)).to(args[\"device\"]), \n",
    "                \"concept\": torch.tensor(range(int(nConcepts * s2), nConcepts)).to(args[\"device\"])\n",
    "            }\n",
    "\n",
    "print(train_idx[\"event\"].shape)\n",
    "print(test_idx[\"event\"].shape)\n",
    "print(val_idx[\"event\"].shape)\n",
    "\n",
    "\n",
    "# TODO: Add node labels to the nodes and try to make the deepsnap split work even for regression!\n",
    "\n",
    "# dataset = deepsnap.dataset.GraphDataset([hetero_graph], task='node')\n",
    "\n",
    "# dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.4, 0.3, 0.3])\n",
    "# datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2814.4502 Current Train,Val,Test Scores [951.3971557617188, 938.8259887695312, 493.22601318359375]\n",
      "Epoch 1 Loss 2432.2859 Current Train,Val,Test Scores [7026.90869140625, 7020.7880859375, 2313.303955078125]\n",
      "Epoch 2 Loss 2083.7654 Current Train,Val,Test Scores [4856.0947265625, 4852.65625, 1663.0482177734375]\n",
      "Epoch 3 Loss 1744.9337 Current Train,Val,Test Scores [2084.755615234375, 2085.794189453125, 781.117919921875]\n",
      "Epoch 4 Loss 1673.4425 Current Train,Val,Test Scores [639.6581420898438, 640.46875, 314.1707458496094]\n",
      "Epoch 5 Loss 1692.3171 Current Train,Val,Test Scores [382.3239440917969, 382.997314453125, 253.8238525390625]\n",
      "Epoch 6 Loss 1702.0675 Current Train,Val,Test Scores [234.04428100585938, 231.4447021484375, 203.26171875]\n",
      "Epoch 7 Loss 1628.2104 Current Train,Val,Test Scores [50.489505767822266, 63.394161224365234, 112.52481842041016]\n",
      "Epoch 8 Loss 1516.0288 Current Train,Val,Test Scores [210.1905517578125, 222.51959228515625, 108.86294555664062]\n",
      "Epoch 9 Loss 1480.3597 Current Train,Val,Test Scores [278.1770324707031, 289.9599304199219, 201.130615234375]\n",
      "Epoch 10 Loss 1430.1117 Current Train,Val,Test Scores [269.5999450683594, 281.44024658203125, 205.38836669921875]\n",
      "Epoch 11 Loss 1385.1553 Current Train,Val,Test Scores [210.29345703125, 222.0525360107422, 161.71470642089844]\n",
      "Epoch 12 Loss 1330.4307 Current Train,Val,Test Scores [164.9639434814453, 176.8892059326172, 114.4623794555664]\n",
      "Epoch 13 Loss 1263.0060 Current Train,Val,Test Scores [72.05235290527344, 84.08720397949219, 79.99940490722656]\n",
      "Epoch 14 Loss 1190.4309 Current Train,Val,Test Scores [396.262939453125, 384.6046447753906, 161.80783081054688]\n",
      "Epoch 15 Loss 1125.1699 Current Train,Val,Test Scores [222.80935668945312, 220.80810546875, 126.60230255126953]\n",
      "Epoch 16 Loss 1051.9592 Current Train,Val,Test Scores [1194.384521484375, 1186.8580322265625, 374.6657409667969]\n",
      "Epoch 17 Loss 1066.8619 Current Train,Val,Test Scores [441.7391052246094, 431.983642578125, 170.52828979492188]\n",
      "Epoch 18 Loss 871.2021 Current Train,Val,Test Scores [66.39523315429688, 76.54592895507812, 63.802791595458984]\n",
      "Epoch 19 Loss 873.5148 Current Train,Val,Test Scores [1180.1976318359375, 1177.6805419921875, 338.1968078613281]\n",
      "Epoch 20 Loss 1057.9332 Current Train,Val,Test Scores [283.05718994140625, 295.915771484375, 101.9689712524414]\n",
      "Epoch 21 Loss 717.5141 Current Train,Val,Test Scores [1533.8099365234375, 1547.5985107421875, 430.7245178222656]\n",
      "Epoch 22 Loss 913.1727 Current Train,Val,Test Scores [3318.20703125, 3330.503662109375, 894.5216674804688]\n",
      "Epoch 23 Loss 886.3004 Current Train,Val,Test Scores [3075.420166015625, 3088.2412109375, 818.4231567382812]\n",
      "Epoch 24 Loss 784.5217 Current Train,Val,Test Scores [2075.723876953125, 2088.698974609375, 552.9447631835938]\n",
      "Epoch 25 Loss 728.4547 Current Train,Val,Test Scores [633.1478881835938, 645.8538818359375, 171.4567108154297]\n",
      "Epoch 26 Loss 818.2800 Current Train,Val,Test Scores [224.89002990722656, 226.45126342773438, 64.54920959472656]\n",
      "Epoch 27 Loss 676.7614 Current Train,Val,Test Scores [457.0193786621094, 444.3656921386719, 125.41116333007812]\n",
      "Epoch 28 Loss 695.2548 Current Train,Val,Test Scores [456.32806396484375, 443.4927978515625, 129.14999389648438]\n",
      "Epoch 29 Loss 669.7701 Current Train,Val,Test Scores [215.4563751220703, 217.49444580078125, 68.3129653930664]\n",
      "Epoch 30 Loss 685.9077 Current Train,Val,Test Scores [18.422651290893555, 30.699975967407227, 12.792328834533691]\n",
      "Epoch 31 Loss 611.7244 Current Train,Val,Test Scores [21.86316680908203, 35.54482650756836, 11.287704467773438]\n",
      "Epoch 32 Loss 628.2104 Current Train,Val,Test Scores [1555.0274658203125, 1550.6595458984375, 412.99273681640625]\n",
      "Epoch 33 Loss 604.8047 Current Train,Val,Test Scores [2233.451416015625, 2228.00537109375, 592.4430541992188]\n",
      "Epoch 34 Loss 620.8423 Current Train,Val,Test Scores [2925.237060546875, 2920.6865234375, 774.7432861328125]\n",
      "Epoch 35 Loss 578.5477 Current Train,Val,Test Scores [2316.300048828125, 2313.80322265625, 612.9481811523438]\n",
      "Epoch 36 Loss 562.1856 Current Train,Val,Test Scores [17.891572952270508, 30.96160888671875, 9.313302040100098]\n",
      "Epoch 37 Loss 527.1013 Current Train,Val,Test Scores [17.79973030090332, 29.920557022094727, 9.214980125427246]\n",
      "Epoch 38 Loss 545.5583 Current Train,Val,Test Scores [46.198524475097656, 58.948524475097656, 16.499155044555664]\n",
      "Epoch 39 Loss 510.9776 Current Train,Val,Test Scores [221.54629516601562, 234.48048400878906, 62.85232925415039]\n",
      "Epoch 40 Loss 491.2272 Current Train,Val,Test Scores [540.18310546875, 553.2669677734375, 147.0358428955078]\n",
      "Epoch 41 Loss 500.8506 Current Train,Val,Test Scores [402.01898193359375, 415.0956726074219, 110.55326080322266]\n",
      "Epoch 42 Loss 472.8179 Current Train,Val,Test Scores [140.23133850097656, 153.15248107910156, 41.40205764770508]\n",
      "Epoch 43 Loss 454.0640 Current Train,Val,Test Scores [56.04721450805664, 67.00509643554688, 19.484052658081055]\n",
      "Epoch 44 Loss 452.8204 Current Train,Val,Test Scores [84.44955444335938, 94.55768585205078, 27.093841552734375]\n",
      "Epoch 45 Loss 435.1065 Current Train,Val,Test Scores [51.31481170654297, 62.26975631713867, 18.407976150512695]\n",
      "Epoch 46 Loss 416.1425 Current Train,Val,Test Scores [24.63665199279785, 37.53837203979492, 11.073831558227539]\n",
      "Epoch 47 Loss 416.5391 Current Train,Val,Test Scores [28.968528747558594, 41.87873077392578, 12.478527069091797]\n",
      "Epoch 48 Loss 418.7684 Current Train,Val,Test Scores [22.105812072753906, 34.99895095825195, 10.700624465942383]\n",
      "Epoch 49 Loss 394.6034 Current Train,Val,Test Scores [58.96810531616211, 65.1296157836914, 21.167816162109375]\n",
      "Epoch 50 Loss 364.1718 Current Train,Val,Test Scores [2387.65087890625, 2379.21044921875, 633.5957641601562]\n",
      "Epoch 51 Loss 352.2842 Current Train,Val,Test Scores [2076.737060546875, 2062.93115234375, 552.0991821289062]\n",
      "Epoch 52 Loss 347.5370 Current Train,Val,Test Scores [2516.27001953125, 2499.5234375, 669.1826782226562]\n",
      "Epoch 53 Loss 341.0211 Current Train,Val,Test Scores [26.75912857055664, 38.19593048095703, 12.739062309265137]\n",
      "Epoch 54 Loss 334.1890 Current Train,Val,Test Scores [442.4117736816406, 417.2378234863281, 121.82567596435547]\n",
      "Epoch 55 Loss 297.7347 Current Train,Val,Test Scores [809.5736694335938, 782.3405151367188, 218.6516571044922]\n",
      "Epoch 56 Loss 289.8828 Current Train,Val,Test Scores [122.09321594238281, 130.2088165283203, 37.99395751953125]\n",
      "Epoch 57 Loss 282.2549 Current Train,Val,Test Scores [137.95755004882812, 144.3522186279297, 42.41961669921875]\n",
      "Epoch 58 Loss 281.0305 Current Train,Val,Test Scores [147.08151245117188, 153.35232543945312, 44.692684173583984]\n",
      "Epoch 59 Loss 290.9167 Current Train,Val,Test Scores [183.80543518066406, 187.88644409179688, 54.4714241027832]\n",
      "Epoch 60 Loss 287.8395 Current Train,Val,Test Scores [235.88885498046875, 236.1864471435547, 68.1915512084961]\n",
      "Epoch 61 Loss 281.7634 Current Train,Val,Test Scores [129.22772216796875, 136.39662170410156, 39.73417282104492]\n",
      "Epoch 62 Loss 219.6852 Current Train,Val,Test Scores [6248.35986328125, 6246.1416015625, 1653.3277587890625]\n",
      "Epoch 63 Loss 254.8204 Current Train,Val,Test Scores [387.2178039550781, 374.89031982421875, 107.62029266357422]\n",
      "Epoch 64 Loss 504.0516 Current Train,Val,Test Scores [420.4410400390625, 407.43505859375, 116.15274047851562]\n",
      "Epoch 65 Loss 525.7371 Current Train,Val,Test Scores [405.1124267578125, 392.082275390625, 112.24877166748047]\n",
      "Epoch 66 Loss 466.6176 Current Train,Val,Test Scores [140.88494873046875, 147.21885681152344, 42.90503692626953]\n",
      "Epoch 67 Loss 562.2985 Current Train,Val,Test Scores [242.0347442626953, 254.82254028320312, 68.84938049316406]\n",
      "Epoch 68 Loss 620.8290 Current Train,Val,Test Scores [97.15669250488281, 106.6301040649414, 30.961942672729492]\n",
      "Epoch 69 Loss 430.2373 Current Train,Val,Test Scores [189.8754119873047, 193.56893920898438, 55.95035934448242]\n",
      "Epoch 70 Loss 437.8441 Current Train,Val,Test Scores [463.2777099609375, 452.3171081542969, 126.77771759033203]\n",
      "Epoch 71 Loss 442.9962 Current Train,Val,Test Scores [6027.80078125, 6068.513671875, 1589.9925537109375]\n",
      "Epoch 72 Loss 472.7112 Current Train,Val,Test Scores [18590.794921875, 18623.2578125, 4912.595703125]\n",
      "Epoch 73 Loss 690.7953 Current Train,Val,Test Scores [21697.958984375, 21736.099609375, 5733.052734375]\n",
      "Epoch 74 Loss 1626.4374 Current Train,Val,Test Scores [25465.623046875, 25497.642578125, 6729.93798828125]\n",
      "Epoch 75 Loss 1145.3325 Current Train,Val,Test Scores [30481.048828125, 30500.7890625, 8057.3369140625]\n",
      "Epoch 76 Loss 1494.1112 Current Train,Val,Test Scores [37859.64453125, 37873.6796875, 10008.7333984375]\n",
      "Epoch 77 Loss 602.6813 Current Train,Val,Test Scores [25366.8125, 25388.76953125, 6704.63623046875]\n",
      "Epoch 78 Loss 960.7610 Current Train,Val,Test Scores [11507.1337890625, 11528.908203125, 3039.993408203125]\n",
      "Epoch 79 Loss 575.2615 Current Train,Val,Test Scores [1648.09716796875, 1644.5157470703125, 437.5585021972656]\n",
      "Epoch 80 Loss 509.9275 Current Train,Val,Test Scores [152.9698028564453, 159.56065368652344, 45.91744613647461]\n",
      "Epoch 81 Loss 620.1949 Current Train,Val,Test Scores [191.54086303710938, 203.7117462158203, 55.67388153076172]\n",
      "Epoch 82 Loss 361.0138 Current Train,Val,Test Scores [233.6858673095703, 246.21435546875, 66.52224731445312]\n",
      "Epoch 83 Loss 538.3188 Current Train,Val,Test Scores [33.08491897583008, 46.004512786865234, 13.19619369506836]\n",
      "Epoch 84 Loss 517.2140 Current Train,Val,Test Scores [284.5133056640625, 280.7531433105469, 80.53251647949219]\n",
      "Epoch 85 Loss 343.6648 Current Train,Val,Test Scores [548.8121948242188, 535.3489990234375, 148.90353393554688]\n",
      "Epoch 86 Loss 431.3545 Current Train,Val,Test Scores [747.8222045898438, 734.157958984375, 201.13198852539062]\n",
      "Epoch 87 Loss 424.3054 Current Train,Val,Test Scores [859.009765625, 845.0650634765625, 230.4873504638672]\n",
      "Epoch 88 Loss 295.0582 Current Train,Val,Test Scores [776.5357666015625, 762.4769287109375, 208.64231872558594]\n",
      "Epoch 89 Loss 356.4066 Current Train,Val,Test Scores [545.6044311523438, 531.587890625, 147.93392944335938]\n",
      "Epoch 90 Loss 396.2239 Current Train,Val,Test Scores [297.037109375, 292.25262451171875, 83.67745208740234]\n",
      "Epoch 91 Loss 346.3680 Current Train,Val,Test Scores [107.5521011352539, 115.85842895507812, 33.16129684448242]\n",
      "Epoch 92 Loss 280.6413 Current Train,Val,Test Scores [32.653255462646484, 45.698699951171875, 12.886341094970703]\n",
      "Epoch 93 Loss 310.1591 Current Train,Val,Test Scores [405.5532531738281, 395.632080078125, 110.08555603027344]\n",
      "Epoch 94 Loss 320.2816 Current Train,Val,Test Scores [2865.725830078125, 2864.919189453125, 759.4241333007812]\n",
      "Epoch 95 Loss 288.0701 Current Train,Val,Test Scores [4986.001953125, 4987.404296875, 1319.7760009765625]\n",
      "Epoch 96 Loss 270.6941 Current Train,Val,Test Scores [5328.42529296875, 5327.439453125, 1408.546142578125]\n",
      "Epoch 97 Loss 252.6809 Current Train,Val,Test Scores [4276.1865234375, 4276.44189453125, 1129.7222900390625]\n",
      "Epoch 98 Loss 267.7602 Current Train,Val,Test Scores [3092.50048828125, 3077.547607421875, 817.096435546875]\n",
      "Epoch 99 Loss 274.5368 Current Train,Val,Test Scores [2568.559326171875, 2546.5869140625, 679.0520629882812]\n",
      "Epoch 100 Loss 241.0929 Current Train,Val,Test Scores [1574.5985107421875, 1536.4625244140625, 416.2607727050781]\n",
      "Epoch 101 Loss 230.7491 Current Train,Val,Test Scores [65.88102722167969, 77.57591247558594, 22.673686981201172]\n",
      "Epoch 102 Loss 230.1626 Current Train,Val,Test Scores [318.90362548828125, 314.13555908203125, 90.07917022705078]\n",
      "Epoch 103 Loss 217.3952 Current Train,Val,Test Scores [462.7227783203125, 450.8201599121094, 127.2419662475586]\n",
      "Epoch 104 Loss 223.7164 Current Train,Val,Test Scores [504.2630615234375, 492.1994934082031, 137.91909790039062]\n",
      "Epoch 105 Loss 207.9565 Current Train,Val,Test Scores [511.606201171875, 499.59014892578125, 139.81649780273438]\n",
      "Epoch 106 Loss 192.1713 Current Train,Val,Test Scores [560.3572387695312, 548.3016967773438, 152.3745574951172]\n",
      "Epoch 107 Loss 189.8087 Current Train,Val,Test Scores [659.8820190429688, 647.7822875976562, 178.49346923828125]\n",
      "Epoch 108 Loss 179.2151 Current Train,Val,Test Scores [745.0609741210938, 732.9550170898438, 201.02708435058594]\n",
      "Epoch 109 Loss 181.3699 Current Train,Val,Test Scores [753.759033203125, 741.7091674804688, 203.3202362060547]\n",
      "Epoch 110 Loss 171.7196 Current Train,Val,Test Scores [684.2693481445312, 672.3135986328125, 184.92813110351562]\n",
      "Epoch 111 Loss 161.3077 Current Train,Val,Test Scores [600.6712036132812, 588.7632446289062, 162.80618286132812]\n",
      "Epoch 112 Loss 157.0808 Current Train,Val,Test Scores [527.2340698242188, 515.2804565429688, 143.83355712890625]\n",
      "Epoch 113 Loss 149.4218 Current Train,Val,Test Scores [395.8131103515625, 384.331787109375, 109.96482849121094]\n",
      "Epoch 114 Loss 149.1176 Current Train,Val,Test Scores [226.2097625732422, 227.97483825683594, 65.74394989013672]\n",
      "Epoch 115 Loss 134.9250 Current Train,Val,Test Scores [130.93853759765625, 138.4988250732422, 40.270023345947266]\n",
      "Epoch 116 Loss 132.4007 Current Train,Val,Test Scores [102.42240142822266, 111.83164978027344, 32.5152473449707]\n",
      "Epoch 117 Loss 128.2594 Current Train,Val,Test Scores [79.42704772949219, 89.7632064819336, 26.280858993530273]\n",
      "Epoch 118 Loss 123.3108 Current Train,Val,Test Scores [63.58549499511719, 74.42560577392578, 22.127573013305664]\n",
      "Epoch 119 Loss 118.1036 Current Train,Val,Test Scores [69.42475128173828, 80.08279418945312, 23.63239860534668]\n",
      "Epoch 120 Loss 111.5952 Current Train,Val,Test Scores [81.79824829101562, 92.08826446533203, 26.923368453979492]\n",
      "Epoch 121 Loss 113.4816 Current Train,Val,Test Scores [77.79947662353516, 88.22154235839844, 25.85942840576172]\n",
      "Epoch 122 Loss 104.8390 Current Train,Val,Test Scores [89.29808807373047, 99.2763442993164, 28.97043800354004]\n",
      "Epoch 123 Loss 101.0278 Current Train,Val,Test Scores [114.10980987548828, 122.8548812866211, 35.74538040161133]\n",
      "Epoch 124 Loss 103.0150 Current Train,Val,Test Scores [107.49150085449219, 116.68518829345703, 33.93748474121094]\n",
      "Epoch 125 Loss 96.0449 Current Train,Val,Test Scores [110.69284057617188, 119.75013732910156, 34.80164337158203]\n",
      "Epoch 126 Loss 92.8392 Current Train,Val,Test Scores [116.1116714477539, 124.84686279296875, 36.25300598144531]\n",
      "Epoch 127 Loss 94.8733 Current Train,Val,Test Scores [83.53762817382812, 93.8780288696289, 27.361886978149414]\n",
      "Epoch 128 Loss 90.3630 Current Train,Val,Test Scores [63.038055419921875, 74.00176239013672, 21.958984375]\n",
      "Epoch 129 Loss 85.4604 Current Train,Val,Test Scores [46.18423080444336, 57.302528381347656, 17.504613876342773]\n",
      "Epoch 130 Loss 86.4586 Current Train,Val,Test Scores [28.615985870361328, 39.73200225830078, 13.051633834838867]\n",
      "Epoch 131 Loss 85.9248 Current Train,Val,Test Scores [25.1446590423584, 36.02432632446289, 12.087212562561035]\n",
      "Epoch 132 Loss 81.2477 Current Train,Val,Test Scores [20.463375091552734, 31.363500595092773, 10.73349380493164]\n",
      "Epoch 133 Loss 77.7090 Current Train,Val,Test Scores [23.231163024902344, 38.56116485595703, 11.793211936950684]\n",
      "Epoch 134 Loss 77.5263 Current Train,Val,Test Scores [224.72262573242188, 215.11907958984375, 60.886817932128906]\n",
      "Epoch 135 Loss 77.8084 Current Train,Val,Test Scores [1041.3536376953125, 1029.5399169921875, 275.17724609375]\n",
      "Epoch 136 Loss 78.6132 Current Train,Val,Test Scores [818.5570068359375, 807.09619140625, 216.18614196777344]\n",
      "Epoch 137 Loss 80.7458 Current Train,Val,Test Scores [1369.434326171875, 1357.4327392578125, 361.4386291503906]\n",
      "Epoch 138 Loss 81.1292 Current Train,Val,Test Scores [609.84765625, 597.9660034179688, 160.9796905517578]\n",
      "Epoch 139 Loss 79.0946 Current Train,Val,Test Scores [578.98828125, 571.126220703125, 155.2305145263672]\n",
      "Epoch 140 Loss 74.8676 Current Train,Val,Test Scores [18.556411743164062, 29.78245735168457, 10.19814395904541]\n",
      "Epoch 141 Loss 68.9036 Current Train,Val,Test Scores [20.889467239379883, 31.731096267700195, 10.836345672607422]\n",
      "Epoch 142 Loss 65.5910 Current Train,Val,Test Scores [26.00630760192871, 36.8835334777832, 12.306324005126953]\n",
      "Epoch 143 Loss 66.2723 Current Train,Val,Test Scores [35.909217834472656, 47.03783416748047, 14.864684104919434]\n",
      "Epoch 144 Loss 68.2716 Current Train,Val,Test Scores [43.00714111328125, 54.06898880004883, 16.63296127319336]\n",
      "Epoch 145 Loss 70.2024 Current Train,Val,Test Scores [55.2120246887207, 66.3118896484375, 19.82840919494629]\n",
      "Epoch 146 Loss 70.1057 Current Train,Val,Test Scores [57.60805892944336, 68.68543243408203, 20.459362030029297]\n",
      "Epoch 147 Loss 67.9771 Current Train,Val,Test Scores [64.16160583496094, 75.08467864990234, 22.160541534423828]\n",
      "Epoch 148 Loss 64.0539 Current Train,Val,Test Scores [59.2192268371582, 70.27403259277344, 20.878164291381836]\n",
      "Epoch 149 Loss 60.8818 Current Train,Val,Test Scores [58.00924301147461, 69.08080291748047, 20.554948806762695]\n",
      "Epoch 150 Loss 58.4612 Current Train,Val,Test Scores [54.695213317871094, 65.7783203125, 19.671785354614258]\n",
      "Epoch 151 Loss 56.7837 Current Train,Val,Test Scores [53.79203796386719, 64.83661651611328, 19.41965675354004]\n",
      "Epoch 152 Loss 55.9237 Current Train,Val,Test Scores [53.66405487060547, 64.71266174316406, 19.382658004760742]\n",
      "Epoch 153 Loss 55.4641 Current Train,Val,Test Scores [53.9511604309082, 64.9814453125, 19.45977783203125]\n",
      "Epoch 154 Loss 55.4674 Current Train,Val,Test Scores [55.48766326904297, 66.52172088623047, 19.861101150512695]\n",
      "Epoch 155 Loss 56.6065 Current Train,Val,Test Scores [57.047481536865234, 68.0330810546875, 20.279577255249023]\n",
      "Epoch 156 Loss 60.2692 Current Train,Val,Test Scores [61.179542541503906, 72.1311264038086, 21.350847244262695]\n",
      "Epoch 157 Loss 70.0448 Current Train,Val,Test Scores [63.05978775024414, 73.91075134277344, 21.846302032470703]\n",
      "Epoch 158 Loss 95.7142 Current Train,Val,Test Scores [69.47187805175781, 80.1061782836914, 23.495542526245117]\n",
      "Epoch 159 Loss 130.9169 Current Train,Val,Test Scores [80.11136627197266, 90.4519271850586, 26.329423904418945]\n",
      "Epoch 160 Loss 186.6947 Current Train,Val,Test Scores [111.18034362792969, 120.16790771484375, 34.82060241699219]\n",
      "Epoch 161 Loss 95.2489 Current Train,Val,Test Scores [133.99575805664062, 141.14817810058594, 41.01883316040039]\n",
      "Epoch 162 Loss 51.0786 Current Train,Val,Test Scores [128.81048583984375, 136.41287231445312, 39.602561950683594]\n",
      "Epoch 163 Loss 81.0655 Current Train,Val,Test Scores [126.77944946289062, 134.55392456054688, 39.04222869873047]\n",
      "Epoch 164 Loss 85.3039 Current Train,Val,Test Scores [100.13976287841797, 109.66099548339844, 31.76473045349121]\n",
      "Epoch 165 Loss 56.7473 Current Train,Val,Test Scores [77.07328033447266, 87.58358764648438, 25.513383865356445]\n",
      "Epoch 166 Loss 57.1449 Current Train,Val,Test Scores [57.69997024536133, 68.72200012207031, 20.429832458496094]\n",
      "Epoch 167 Loss 72.0189 Current Train,Val,Test Scores [47.1414794921875, 58.17629623413086, 17.614030838012695]\n",
      "Epoch 168 Loss 56.8545 Current Train,Val,Test Scores [35.67596435546875, 46.88005828857422, 14.736520767211914]\n",
      "Epoch 169 Loss 51.0682 Current Train,Val,Test Scores [55.90402603149414, 67.13954162597656, 20.159221649169922]\n",
      "Epoch 170 Loss 65.2808 Current Train,Val,Test Scores [33.0644645690918, 45.97849655151367, 14.184357643127441]\n",
      "Epoch 171 Loss 55.6798 Current Train,Val,Test Scores [31.680583953857422, 43.992919921875, 13.715259552001953]\n",
      "Epoch 172 Loss 48.2252 Current Train,Val,Test Scores [33.62675857543945, 45.231468200683594, 14.243548393249512]\n",
      "Epoch 173 Loss 60.3214 Current Train,Val,Test Scores [37.09485626220703, 48.31395721435547, 15.103811264038086]\n",
      "Epoch 174 Loss 53.1897 Current Train,Val,Test Scores [40.791419982910156, 51.96916580200195, 16.022794723510742]\n",
      "Epoch 175 Loss 46.2532 Current Train,Val,Test Scores [45.04458236694336, 56.257171630859375, 17.095273971557617]\n",
      "Epoch 176 Loss 56.0189 Current Train,Val,Test Scores [54.86195373535156, 66.20664978027344, 19.725629806518555]\n",
      "Epoch 177 Loss 50.4463 Current Train,Val,Test Scores [59.9015007019043, 71.17440032958984, 21.05059051513672]\n",
      "Epoch 178 Loss 45.4850 Current Train,Val,Test Scores [59.5592041015625, 70.84922790527344, 20.955429077148438]\n",
      "Epoch 179 Loss 53.2072 Current Train,Val,Test Scores [55.12092590332031, 66.48041534423828, 19.779430389404297]\n",
      "Epoch 180 Loss 47.6247 Current Train,Val,Test Scores [47.496307373046875, 58.68721389770508, 17.725936889648438]\n",
      "Epoch 181 Loss 44.5055 Current Train,Val,Test Scores [38.75187301635742, 50.119110107421875, 15.473605155944824]\n",
      "Epoch 182 Loss 50.1464 Current Train,Val,Test Scores [29.2531681060791, 40.53252410888672, 13.099729537963867]\n",
      "Epoch 183 Loss 44.9052 Current Train,Val,Test Scores [23.381122589111328, 34.45695877075195, 11.463443756103516]\n",
      "Epoch 184 Loss 43.4460 Current Train,Val,Test Scores [20.743330001831055, 32.16204833984375, 10.740273475646973]\n",
      "Epoch 185 Loss 47.5362 Current Train,Val,Test Scores [31.59835433959961, 39.138370513916016, 13.444458961486816]\n",
      "Epoch 186 Loss 43.3234 Current Train,Val,Test Scores [85.93787384033203, 67.9407730102539, 26.15155792236328]\n",
      "Epoch 187 Loss 42.5983 Current Train,Val,Test Scores [20.183774948120117, 34.46694564819336, 10.825510025024414]\n",
      "Epoch 188 Loss 45.6145 Current Train,Val,Test Scores [16.328752517700195, 29.868478775024414, 9.726070404052734]\n",
      "Epoch 189 Loss 42.3206 Current Train,Val,Test Scores [20.1084041595459, 31.15575408935547, 10.500808715820312]\n",
      "Epoch 190 Loss 41.6663 Current Train,Val,Test Scores [23.244192123413086, 34.34002685546875, 11.405635833740234]\n",
      "Epoch 191 Loss 43.9580 Current Train,Val,Test Scores [25.325576782226562, 36.454063415527344, 11.993413925170898]\n",
      "Epoch 192 Loss 41.5090 Current Train,Val,Test Scores [28.017953872680664, 39.33110809326172, 12.72907829284668]\n",
      "Epoch 193 Loss 40.7756 Current Train,Val,Test Scores [30.26150131225586, 41.6118278503418, 13.30582046508789]\n",
      "Epoch 194 Loss 42.5667 Current Train,Val,Test Scores [29.277847290039062, 40.62730407714844, 13.05904483795166]\n",
      "Epoch 195 Loss 40.8898 Current Train,Val,Test Scores [28.180177688598633, 39.517425537109375, 12.768178939819336]\n",
      "Epoch 196 Loss 39.9008 Current Train,Val,Test Scores [26.276681900024414, 37.47526931762695, 12.258647918701172]\n",
      "Epoch 197 Loss 41.2890 Current Train,Val,Test Scores [22.175905227661133, 33.36494827270508, 11.086974143981934]\n",
      "Epoch 198 Loss 40.3776 Current Train,Val,Test Scores [19.576330184936523, 30.84898567199707, 10.338242530822754]\n",
      "Epoch 199 Loss 39.1806 Current Train,Val,Test Scores [14.0979585647583, 29.55303192138672, 9.556343078613281]\n",
      "Epoch 200 Loss 40.1546 Current Train,Val,Test Scores [44.117767333984375, 39.273651123046875, 15.393424034118652]\n",
      "Epoch 201 Loss 40.0076 Current Train,Val,Test Scores [49.13209533691406, 41.56497573852539, 16.34404754638672]\n",
      "Epoch 202 Loss 38.7452 Current Train,Val,Test Scores [27.14618492126465, 33.38203811645508, 12.1221342086792]\n",
      "Epoch 203 Loss 39.1280 Current Train,Val,Test Scores [23.40431022644043, 32.78157043457031, 11.451207160949707]\n",
      "Epoch 204 Loss 39.5424 Current Train,Val,Test Scores [15.16636848449707, 30.63018226623535, 9.7830228805542]\n",
      "Epoch 205 Loss 38.5169 Current Train,Val,Test Scores [15.903189659118652, 30.77608871459961, 9.965436935424805]\n",
      "Epoch 206 Loss 38.2396 Current Train,Val,Test Scores [15.923757553100586, 31.428085327148438, 10.082694053649902]\n",
      "Epoch 207 Loss 38.7587 Current Train,Val,Test Scores [15.528136253356934, 31.547183990478516, 9.963062286376953]\n",
      "Epoch 208 Loss 38.3604 Current Train,Val,Test Scores [21.210451126098633, 35.54433822631836, 11.530614852905273]\n",
      "Epoch 209 Loss 37.7132 Current Train,Val,Test Scores [76.4958267211914, 58.38600158691406, 23.76369285583496]\n",
      "Epoch 210 Loss 37.8873 Current Train,Val,Test Scores [180.7898712158203, 147.32957458496094, 49.70829772949219]\n",
      "Epoch 211 Loss 38.0534 Current Train,Val,Test Scores [352.0514831542969, 312.27288818359375, 94.64176940917969]\n",
      "Epoch 212 Loss 37.5894 Current Train,Val,Test Scores [360.41839599609375, 320.6360778808594, 96.59732055664062]\n",
      "Epoch 213 Loss 37.2179 Current Train,Val,Test Scores [248.20831298828125, 211.17474365234375, 67.3499526977539]\n",
      "Epoch 214 Loss 37.3550 Current Train,Val,Test Scores [96.35118865966797, 70.95716094970703, 29.43684959411621]\n",
      "Epoch 215 Loss 37.4169 Current Train,Val,Test Scores [15.245617866516113, 31.491971969604492, 9.896109580993652]\n",
      "Epoch 216 Loss 37.0706 Current Train,Val,Test Scores [21.947891235351562, 33.32545471191406, 10.982694625854492]\n",
      "Epoch 217 Loss 36.7702 Current Train,Val,Test Scores [24.898117065429688, 36.05881881713867, 11.824065208435059]\n",
      "Epoch 218 Loss 36.7953 Current Train,Val,Test Scores [28.005413055419922, 39.30459213256836, 12.669870376586914]\n",
      "Epoch 219 Loss 36.8577 Current Train,Val,Test Scores [29.96484375, 41.29076385498047, 13.169626235961914]\n",
      "Epoch 220 Loss 36.6564 Current Train,Val,Test Scores [31.96832847595215, 43.36648941040039, 13.662009239196777]\n",
      "Epoch 221 Loss 36.3757 Current Train,Val,Test Scores [32.82233428955078, 44.24822998046875, 13.869402885437012]\n",
      "Epoch 222 Loss 36.2973 Current Train,Val,Test Scores [32.231441497802734, 43.641639709472656, 13.720942497253418]\n",
      "Epoch 223 Loss 36.3352 Current Train,Val,Test Scores [32.55674362182617, 43.9727668762207, 13.794605255126953]\n",
      "Epoch 224 Loss 36.2719 Current Train,Val,Test Scores [31.564977645874023, 42.94600296020508, 13.546087265014648]\n",
      "Epoch 225 Loss 36.0498 Current Train,Val,Test Scores [32.078887939453125, 43.48931121826172, 13.667943954467773]\n",
      "Epoch 226 Loss 35.8513 Current Train,Val,Test Scores [32.22991943359375, 43.653526306152344, 13.701287269592285]\n",
      "Epoch 227 Loss 35.7601 Current Train,Val,Test Scores [32.284183502197266, 43.70096206665039, 13.710112571716309]\n",
      "Epoch 228 Loss 35.7351 Current Train,Val,Test Scores [34.20537567138672, 45.65222930908203, 14.186394691467285]\n",
      "Epoch 229 Loss 35.6701 Current Train,Val,Test Scores [35.0377082824707, 46.4663200378418, 14.389291763305664]\n",
      "Epoch 230 Loss 35.5233 Current Train,Val,Test Scores [37.30845260620117, 48.69582748413086, 14.940300941467285]\n",
      "Epoch 231 Loss 35.3432 Current Train,Val,Test Scores [38.62972640991211, 49.993743896484375, 15.261648178100586]\n",
      "Epoch 232 Loss 35.2124 Current Train,Val,Test Scores [39.46000289916992, 50.806114196777344, 15.466521263122559]\n",
      "Epoch 233 Loss 35.1433 Current Train,Val,Test Scores [40.55997848510742, 51.88752746582031, 15.739048957824707]\n",
      "Epoch 234 Loss 35.0879 Current Train,Val,Test Scores [38.99993896484375, 50.34829330444336, 15.33918571472168]\n",
      "Epoch 235 Loss 35.0066 Current Train,Val,Test Scores [39.57069396972656, 50.91634750366211, 15.476909637451172]\n",
      "Epoch 236 Loss 34.8931 Current Train,Val,Test Scores [37.711448669433594, 49.089027404785156, 15.005231857299805]\n",
      "Epoch 237 Loss 34.7709 Current Train,Val,Test Scores [38.041683197021484, 49.41819763183594, 15.077559471130371]\n",
      "Epoch 238 Loss 34.6528 Current Train,Val,Test Scores [37.69415283203125, 49.083404541015625, 14.984039306640625]\n",
      "Epoch 239 Loss 34.5476 Current Train,Val,Test Scores [37.92388916015625, 49.31169891357422, 15.03050708770752]\n",
      "Epoch 240 Loss 34.4614 Current Train,Val,Test Scores [38.54882049560547, 49.927947998046875, 15.174636840820312]\n",
      "Epoch 241 Loss 34.3957 Current Train,Val,Test Scores [37.47554397583008, 48.87791061401367, 14.900957107543945]\n",
      "Epoch 242 Loss 34.3405 Current Train,Val,Test Scores [37.55268478393555, 48.95524215698242, 14.910346031188965]\n",
      "Epoch 243 Loss 34.3012 Current Train,Val,Test Scores [34.335731506347656, 45.78242111206055, 14.112412452697754]\n",
      "Epoch 244 Loss 34.2712 Current Train,Val,Test Scores [33.769927978515625, 45.2131462097168, 13.962752342224121]\n",
      "Epoch 245 Loss 34.2491 Current Train,Val,Test Scores [29.36684226989746, 40.71091079711914, 12.864267349243164]\n",
      "Epoch 246 Loss 34.2629 Current Train,Val,Test Scores [30.043298721313477, 41.419105529785156, 13.035487174987793]\n",
      "Epoch 247 Loss 34.2964 Current Train,Val,Test Scores [26.451669692993164, 37.7161865234375, 12.080078125]\n",
      "Epoch 248 Loss 34.4017 Current Train,Val,Test Scores [28.727130889892578, 40.12736129760742, 12.693737030029297]\n",
      "Epoch 249 Loss 34.5760 Current Train,Val,Test Scores [24.264822006225586, 35.455772399902344, 11.46878433227539]\n",
      "Epoch 250 Loss 34.8980 Current Train,Val,Test Scores [28.315420150756836, 39.697269439697266, 12.566311836242676]\n",
      "Epoch 251 Loss 35.4353 Current Train,Val,Test Scores [23.125244140625, 34.446617126464844, 11.12105655670166]\n",
      "Epoch 252 Loss 36.4852 Current Train,Val,Test Scores [28.766437530517578, 40.15049743652344, 12.656411170959473]\n",
      "Epoch 253 Loss 37.9514 Current Train,Val,Test Scores [22.354116439819336, 33.80457305908203, 10.885108947753906]\n",
      "Epoch 254 Loss 40.8993 Current Train,Val,Test Scores [32.33503723144531, 43.75763702392578, 13.521705627441406]\n",
      "Epoch 255 Loss 43.6433 Current Train,Val,Test Scores [27.233238220214844, 38.50452423095703, 12.212278366088867]\n",
      "Epoch 256 Loss 48.5591 Current Train,Val,Test Scores [40.42387390136719, 51.74342346191406, 15.51905345916748]\n",
      "Epoch 257 Loss 48.0617 Current Train,Val,Test Scores [29.378456115722656, 40.608760833740234, 12.757696151733398]\n",
      "Epoch 258 Loss 47.7451 Current Train,Val,Test Scores [34.74280548095703, 46.152442932128906, 14.11003589630127]\n",
      "Epoch 259 Loss 40.7826 Current Train,Val,Test Scores [26.668466567993164, 37.76371383666992, 12.018916130065918]\n",
      "Epoch 260 Loss 35.2664 Current Train,Val,Test Scores [24.36214256286621, 35.43218231201172, 11.387846946716309]\n",
      "Epoch 261 Loss 33.1920 Current Train,Val,Test Scores [22.76998519897461, 33.878292083740234, 10.921101570129395]\n",
      "Epoch 262 Loss 35.3949 Current Train,Val,Test Scores [18.590349197387695, 29.82635498046875, 9.779026985168457]\n",
      "Epoch 263 Loss 38.5113 Current Train,Val,Test Scores [19.916032791137695, 30.965063095092773, 10.102267265319824]\n",
      "Epoch 264 Loss 37.7333 Current Train,Val,Test Scores [18.10962677001953, 29.606733322143555, 9.608224868774414]\n",
      "Epoch 265 Loss 34.9947 Current Train,Val,Test Scores [18.695837020874023, 29.943090438842773, 9.749536514282227]\n",
      "Epoch 266 Loss 32.8895 Current Train,Val,Test Scores [19.355424880981445, 30.529016494750977, 9.906351089477539]\n",
      "Epoch 267 Loss 33.5936 Current Train,Val,Test Scores [18.43647003173828, 29.853151321411133, 9.653335571289062]\n",
      "Epoch 268 Loss 35.5326 Current Train,Val,Test Scores [22.071094512939453, 33.321144104003906, 10.637895584106445]\n",
      "Epoch 269 Loss 35.6902 Current Train,Val,Test Scores [22.920862197875977, 34.15810012817383, 10.883525848388672]\n",
      "Epoch 270 Loss 34.3003 Current Train,Val,Test Scores [23.947669982910156, 35.14589309692383, 11.179173469543457]\n",
      "Epoch 271 Loss 32.6858 Current Train,Val,Test Scores [20.893451690673828, 32.127010345458984, 10.284270286560059]\n",
      "Epoch 272 Loss 32.6976 Current Train,Val,Test Scores [16.937999725341797, 29.810623168945312, 9.38283920288086]\n",
      "Epoch 273 Loss 33.8133 Current Train,Val,Test Scores [13.76792049407959, 29.638906478881836, 9.063106536865234]\n",
      "Epoch 274 Loss 34.1576 Current Train,Val,Test Scores [401.3705139160156, 374.2992248535156, 106.71305084228516]\n",
      "Epoch 275 Loss 33.5098 Current Train,Val,Test Scores [494.6377258300781, 466.0152282714844, 131.76290893554688]\n",
      "Epoch 276 Loss 32.4101 Current Train,Val,Test Scores [670.05810546875, 641.632080078125, 178.04330444335938]\n",
      "Epoch 277 Loss 32.0912 Current Train,Val,Test Scores [742.531494140625, 711.093505859375, 197.14413452148438]\n",
      "Epoch 278 Loss 32.5706 Current Train,Val,Test Scores [361.4084167480469, 326.38818359375, 96.80155181884766]\n",
      "Epoch 279 Loss 32.9998 Current Train,Val,Test Scores [430.7949523925781, 395.7069396972656, 116.6364974975586]\n",
      "Epoch 280 Loss 32.8371 Current Train,Val,Test Scores [43.13812255859375, 38.531742095947266, 15.208232879638672]\n",
      "Epoch 281 Loss 32.1378 Current Train,Val,Test Scores [19.341331481933594, 31.854829788208008, 10.430336952209473]\n",
      "Epoch 282 Loss 31.6506 Current Train,Val,Test Scores [32.615474700927734, 35.995765686035156, 13.129829406738281]\n",
      "Epoch 283 Loss 31.6475 Current Train,Val,Test Scores [92.4385986328125, 71.91510772705078, 28.06399917602539]\n",
      "Epoch 284 Loss 31.9412 Current Train,Val,Test Scores [383.1746826171875, 354.431640625, 104.80833435058594]\n",
      "Epoch 285 Loss 32.0266 Current Train,Val,Test Scores [537.37060546875, 505.4710693359375, 145.0914764404297]\n",
      "Epoch 286 Loss 31.7445 Current Train,Val,Test Scores [834.56494140625, 805.056396484375, 224.04151916503906]\n",
      "Epoch 287 Loss 31.3109 Current Train,Val,Test Scores [779.4111328125, 749.3453369140625, 209.35581970214844]\n",
      "Epoch 288 Loss 31.0641 Current Train,Val,Test Scores [554.1835327148438, 522.8084106445312, 149.57473754882812]\n",
      "Epoch 289 Loss 31.0908 Current Train,Val,Test Scores [429.11724853515625, 400.8363037109375, 116.9205551147461]\n",
      "Epoch 290 Loss 31.2234 Current Train,Val,Test Scores [106.1510009765625, 86.04265594482422, 31.354061126708984]\n",
      "Epoch 291 Loss 31.3091 Current Train,Val,Test Scores [32.67799758911133, 40.51030731201172, 13.656582832336426]\n",
      "Epoch 292 Loss 31.1858 Current Train,Val,Test Scores [19.018062591552734, 34.58730697631836, 10.657927513122559]\n",
      "Epoch 293 Loss 30.8981 Current Train,Val,Test Scores [20.01262092590332, 34.485416412353516, 10.964034080505371]\n",
      "Epoch 294 Loss 30.6153 Current Train,Val,Test Scores [18.75017738342285, 34.3400993347168, 10.587848663330078]\n",
      "Epoch 295 Loss 30.3688 Current Train,Val,Test Scores [18.66341781616211, 34.25590515136719, 10.567848205566406]\n",
      "Epoch 296 Loss 30.2705 Current Train,Val,Test Scores [19.030614852905273, 34.24467849731445, 10.68899154663086]\n",
      "Epoch 297 Loss 30.2142 Current Train,Val,Test Scores [24.90763282775879, 38.07096481323242, 11.89507007598877]\n",
      "Epoch 298 Loss 30.2639 Current Train,Val,Test Scores [23.737455368041992, 36.8862419128418, 11.580160140991211]\n",
      "Epoch 299 Loss 30.5171 Current Train,Val,Test Scores [32.169681549072266, 43.41115951538086, 13.494243621826172]\n",
      "Epoch 300 Loss 30.6903 Current Train,Val,Test Scores [23.8963680267334, 37.1982421875, 11.723605155944824]\n",
      "Epoch 301 Loss 31.2312 Current Train,Val,Test Scores [31.329410552978516, 42.14313507080078, 13.264068603515625]\n",
      "Epoch 302 Loss 31.9088 Current Train,Val,Test Scores [12.000229835510254, 28.14757537841797, 8.753883361816406]\n",
      "Epoch 303 Loss 34.0310 Current Train,Val,Test Scores [24.496259689331055, 35.5352783203125, 11.275810241699219]\n",
      "Epoch 304 Loss 37.5490 Current Train,Val,Test Scores [27.021202087402344, 34.25075912475586, 11.948161125183105]\n",
      "Epoch 305 Loss 46.2984 Current Train,Val,Test Scores [21.240848541259766, 32.453468322753906, 10.158101081848145]\n",
      "Epoch 306 Loss 51.8514 Current Train,Val,Test Scores [18.90610694885254, 35.5898323059082, 10.300114631652832]\n",
      "Epoch 307 Loss 63.7050 Current Train,Val,Test Scores [23.911544799804688, 38.86227798461914, 11.06645393371582]\n",
      "Epoch 308 Loss 52.5683 Current Train,Val,Test Scores [31.958621978759766, 46.216278076171875, 12.609643936157227]\n",
      "Epoch 309 Loss 41.9682 Current Train,Val,Test Scores [27.252662658691406, 39.67681884765625, 13.190184593200684]\n",
      "Epoch 310 Loss 30.9327 Current Train,Val,Test Scores [274.7548828125, 291.1974182128906, 70.72081756591797]\n",
      "Epoch 311 Loss 33.2497 Current Train,Val,Test Scores [43.80453109741211, 51.640419006347656, 15.9527587890625]\n",
      "Epoch 312 Loss 41.6947 Current Train,Val,Test Scores [1550.7943115234375, 1569.806396484375, 407.92633056640625]\n",
      "Epoch 313 Loss 40.1773 Current Train,Val,Test Scores [425.531005859375, 445.3756103515625, 109.02679443359375]\n",
      "Epoch 314 Loss 36.2955 Current Train,Val,Test Scores [13.692049026489258, 30.7048397064209, 9.109054565429688]\n",
      "Epoch 315 Loss 29.2336 Current Train,Val,Test Scores [18.15781593322754, 29.627103805541992, 9.220328330993652]\n",
      "Epoch 316 Loss 32.6661 Current Train,Val,Test Scores [17.480300903320312, 29.363021850585938, 8.75175666809082]\n",
      "Epoch 317 Loss 37.5660 Current Train,Val,Test Scores [19.89633560180664, 32.2691650390625, 9.335055351257324]\n",
      "Epoch 318 Loss 35.5076 Current Train,Val,Test Scores [20.40675926208496, 31.632526397705078, 10.0480375289917]\n",
      "Epoch 319 Loss 32.5258 Current Train,Val,Test Scores [19.66132354736328, 30.95242691040039, 9.840599060058594]\n",
      "Epoch 320 Loss 30.0353 Current Train,Val,Test Scores [98.73005676269531, 72.73951721191406, 27.944978713989258]\n",
      "Epoch 321 Loss 33.1008 Current Train,Val,Test Scores [684.7034301757812, 644.3717651367188, 182.98240661621094]\n",
      "Epoch 322 Loss 36.7401 Current Train,Val,Test Scores [1592.5455322265625, 1556.152587890625, 421.4183044433594]\n",
      "Epoch 323 Loss 30.3200 Current Train,Val,Test Scores [2399.916259765625, 2374.9462890625, 633.6135864257812]\n",
      "Epoch 324 Loss 31.1467 Current Train,Val,Test Scores [2146.37158203125, 2109.57568359375, 573.7779541015625]\n",
      "Epoch 325 Loss 31.5864 Current Train,Val,Test Scores [1555.4031982421875, 1515.8663330078125, 421.0450134277344]\n",
      "Epoch 326 Loss 32.9075 Current Train,Val,Test Scores [2125.641845703125, 2093.19921875, 572.8267822265625]\n",
      "Epoch 327 Loss 33.3944 Current Train,Val,Test Scores [988.1011962890625, 947.37646484375, 272.9058837890625]\n",
      "Epoch 328 Loss 28.8944 Current Train,Val,Test Scores [131.04823303222656, 102.36470794677734, 44.995155334472656]\n",
      "Epoch 329 Loss 32.4890 Current Train,Val,Test Scores [388.3819885253906, 347.4463806152344, 109.37596893310547]\n",
      "Epoch 330 Loss 36.1239 Current Train,Val,Test Scores [162.39892578125, 174.997314453125, 51.13136291503906]\n",
      "Epoch 331 Loss 31.9483 Current Train,Val,Test Scores [127.83870697021484, 140.3638916015625, 40.615478515625]\n",
      "Epoch 332 Loss 36.2699 Current Train,Val,Test Scores [137.80503845214844, 150.25827026367188, 41.79955291748047]\n",
      "Epoch 333 Loss 36.3198 Current Train,Val,Test Scores [118.27649688720703, 130.6498565673828, 35.303871154785156]\n",
      "Epoch 334 Loss 36.1344 Current Train,Val,Test Scores [84.0073471069336, 96.34382629394531, 26.024898529052734]\n",
      "Epoch 335 Loss 39.8762 Current Train,Val,Test Scores [121.31698608398438, 134.4516143798828, 35.920387268066406]\n",
      "Epoch 336 Loss 28.9139 Current Train,Val,Test Scores [794.912353515625, 754.9016723632812, 209.17860412597656]\n",
      "Epoch 337 Loss 40.9023 Current Train,Val,Test Scores [1098.318359375, 1061.2847900390625, 292.6249084472656]\n",
      "Epoch 338 Loss 35.5407 Current Train,Val,Test Scores [112.83709716796875, 90.58944702148438, 32.48331832885742]\n",
      "Epoch 339 Loss 40.8765 Current Train,Val,Test Scores [1747.32177734375, 1706.9298095703125, 461.0365905761719]\n",
      "Epoch 340 Loss 33.9979 Current Train,Val,Test Scores [975.3028564453125, 935.5630493164062, 256.51885986328125]\n",
      "Epoch 341 Loss 33.6461 Current Train,Val,Test Scores [100.27212524414062, 113.57991027832031, 30.488122940063477]\n",
      "Epoch 342 Loss 33.2777 Current Train,Val,Test Scores [94.06161499023438, 107.22498321533203, 28.736745834350586]\n",
      "Epoch 343 Loss 33.4320 Current Train,Val,Test Scores [92.40238189697266, 104.88404846191406, 28.38722801208496]\n",
      "Epoch 344 Loss 33.2064 Current Train,Val,Test Scores [93.73856353759766, 106.30940246582031, 28.976049423217773]\n",
      "Epoch 345 Loss 28.6574 Current Train,Val,Test Scores [28.260334014892578, 40.6573486328125, 12.922550201416016]\n",
      "Epoch 346 Loss 30.0397 Current Train,Val,Test Scores [26.234174728393555, 37.62809371948242, 13.28441047668457]\n",
      "Epoch 347 Loss 29.3755 Current Train,Val,Test Scores [66.15728759765625, 78.72314453125, 21.598648071289062]\n",
      "Epoch 348 Loss 30.6585 Current Train,Val,Test Scores [63.81378173828125, 76.51535034179688, 20.639171600341797]\n",
      "Epoch 349 Loss 27.6707 Current Train,Val,Test Scores [37.80880355834961, 49.70966339111328, 13.911911964416504]\n",
      "Epoch 350 Loss 28.8217 Current Train,Val,Test Scores [33.72183609008789, 45.6380500793457, 12.896002769470215]\n",
      "Epoch 351 Loss 28.5183 Current Train,Val,Test Scores [40.99110794067383, 53.525875091552734, 14.477828025817871]\n",
      "Epoch 352 Loss 29.1889 Current Train,Val,Test Scores [43.155418395996094, 55.74173355102539, 15.172685623168945]\n",
      "Epoch 353 Loss 27.9484 Current Train,Val,Test Scores [18.015073776245117, 29.51860809326172, 8.900936126708984]\n",
      "Epoch 354 Loss 26.4662 Current Train,Val,Test Scores [19.04139518737793, 31.679786682128906, 9.09268856048584]\n",
      "Epoch 355 Loss 27.4864 Current Train,Val,Test Scores [23.811967849731445, 39.705039978027344, 11.137589454650879]\n",
      "Epoch 356 Loss 26.9684 Current Train,Val,Test Scores [30.32438087463379, 43.145301818847656, 12.128713607788086]\n",
      "Epoch 357 Loss 27.6724 Current Train,Val,Test Scores [17.927703857421875, 30.126462936401367, 9.224270820617676]\n",
      "Epoch 358 Loss 25.9319 Current Train,Val,Test Scores [30.399250030517578, 42.50516128540039, 13.27615737915039]\n",
      "Epoch 359 Loss 26.5338 Current Train,Val,Test Scores [20.801734924316406, 32.72954559326172, 10.401363372802734]\n",
      "Epoch 360 Loss 25.6089 Current Train,Val,Test Scores [20.13909149169922, 32.46944808959961, 9.823505401611328]\n",
      "Epoch 361 Loss 26.3077 Current Train,Val,Test Scores [17.861827850341797, 29.112560272216797, 9.53686809539795]\n",
      "Epoch 362 Loss 25.8998 Current Train,Val,Test Scores [18.611507415771484, 30.168682098388672, 9.922410011291504]\n",
      "Epoch 363 Loss 25.6870 Current Train,Val,Test Scores [22.934505462646484, 35.3426513671875, 10.687666893005371]\n",
      "Epoch 364 Loss 25.1606 Current Train,Val,Test Scores [39.76138687133789, 52.433021545410156, 15.003353118896484]\n",
      "Epoch 365 Loss 25.1958 Current Train,Val,Test Scores [47.80377197265625, 59.85140609741211, 17.0264949798584]\n",
      "Epoch 366 Loss 25.0232 Current Train,Val,Test Scores [45.39358901977539, 57.6688232421875, 16.42873191833496]\n",
      "Epoch 367 Loss 25.2611 Current Train,Val,Test Scores [60.45364761352539, 72.85042572021484, 20.45417022705078]\n",
      "Epoch 368 Loss 24.6790 Current Train,Val,Test Scores [86.7374267578125, 99.67066955566406, 27.55746841430664]\n",
      "Epoch 369 Loss 24.7045 Current Train,Val,Test Scores [109.6234359741211, 122.6314468383789, 33.729286193847656]\n",
      "Epoch 370 Loss 24.1465 Current Train,Val,Test Scores [120.02099609375, 132.87649536132812, 36.55459213256836]\n",
      "Epoch 371 Loss 24.3671 Current Train,Val,Test Scores [119.08609008789062, 132.00201416015625, 36.263671875]\n",
      "Epoch 372 Loss 24.0680 Current Train,Val,Test Scores [98.72151947021484, 111.54360961914062, 30.78820037841797]\n",
      "Epoch 373 Loss 24.1957 Current Train,Val,Test Scores [89.926513671875, 102.68334197998047, 28.50312042236328]\n",
      "Epoch 374 Loss 23.9683 Current Train,Val,Test Scores [84.54913330078125, 97.26174926757812, 27.249975204467773]\n",
      "Epoch 375 Loss 23.8368 Current Train,Val,Test Scores [72.02720642089844, 85.17991638183594, 24.108261108398438]\n",
      "Epoch 376 Loss 23.5519 Current Train,Val,Test Scores [42.777748107910156, 56.011383056640625, 16.40458869934082]\n",
      "Epoch 377 Loss 23.2700 Current Train,Val,Test Scores [20.1065731048584, 31.223331451416016, 10.446317672729492]\n",
      "Epoch 378 Loss 23.4199 Current Train,Val,Test Scores [22.763031005859375, 35.92777633666992, 11.22375774383545]\n",
      "Epoch 379 Loss 23.0369 Current Train,Val,Test Scores [34.59764862060547, 47.85428237915039, 14.395673751831055]\n",
      "Epoch 380 Loss 23.0043 Current Train,Val,Test Scores [21.689407348632812, 34.7526741027832, 11.051539421081543]\n",
      "Epoch 381 Loss 22.9461 Current Train,Val,Test Scores [34.3050651550293, 47.60396194458008, 14.335417747497559]\n",
      "Epoch 382 Loss 22.7506 Current Train,Val,Test Scores [26.558961868286133, 39.80585861206055, 12.137667655944824]\n",
      "Epoch 383 Loss 22.5933 Current Train,Val,Test Scores [16.994949340820312, 29.189571380615234, 9.498993873596191]\n",
      "Epoch 384 Loss 22.6882 Current Train,Val,Test Scores [29.685808181762695, 42.877742767333984, 13.17027473449707]\n",
      "Epoch 385 Loss 22.6460 Current Train,Val,Test Scores [24.68893051147461, 37.77777099609375, 11.871848106384277]\n",
      "Epoch 386 Loss 22.7371 Current Train,Val,Test Scores [31.71554946899414, 44.9463005065918, 13.759438514709473]\n",
      "Epoch 387 Loss 22.8008 Current Train,Val,Test Scores [33.929786682128906, 47.345829010009766, 14.325665473937988]\n",
      "Epoch 388 Loss 23.3777 Current Train,Val,Test Scores [18.292171478271484, 30.89884376525879, 10.078943252563477]\n",
      "Epoch 389 Loss 25.0463 Current Train,Val,Test Scores [46.97187042236328, 60.63020324707031, 17.801774978637695]\n",
      "Epoch 390 Loss 29.1971 Current Train,Val,Test Scores [18.972442626953125, 30.476102828979492, 10.227235794067383]\n",
      "Epoch 391 Loss 33.5151 Current Train,Val,Test Scores [19.27933120727539, 32.56891632080078, 10.498933792114258]\n",
      "Epoch 392 Loss 43.1981 Current Train,Val,Test Scores [49.885040283203125, 63.202396392822266, 17.890445709228516]\n",
      "Epoch 393 Loss 42.8903 Current Train,Val,Test Scores [163.23324584960938, 176.03321838378906, 47.64802932739258]\n",
      "Epoch 394 Loss 43.6714 Current Train,Val,Test Scores [157.3084259033203, 170.18551635742188, 45.879920959472656]\n",
      "Epoch 395 Loss 29.2119 Current Train,Val,Test Scores [201.99026489257812, 214.90982055664062, 57.88969802856445]\n",
      "Epoch 396 Loss 21.9824 Current Train,Val,Test Scores [244.2452850341797, 257.16180419921875, 69.16223907470703]\n",
      "Epoch 397 Loss 24.4516 Current Train,Val,Test Scores [266.9144592285156, 279.7151794433594, 75.16481018066406]\n",
      "Epoch 398 Loss 31.0130 Current Train,Val,Test Scores [320.61126708984375, 333.525146484375, 89.5105972290039]\n",
      "Epoch 399 Loss 33.6067 Current Train,Val,Test Scores [296.44744873046875, 309.20635986328125, 83.06195831298828]\n",
      "Epoch 400 Loss 23.9774 Current Train,Val,Test Scores [307.64385986328125, 320.37615966796875, 86.00386810302734]\n",
      "Epoch 401 Loss 20.4182 Current Train,Val,Test Scores [342.3506164550781, 355.1711120605469, 95.30593872070312]\n",
      "Epoch 402 Loss 23.5979 Current Train,Val,Test Scores [286.8932800292969, 299.5743408203125, 80.85453796386719]\n",
      "Epoch 403 Loss 25.6039 Current Train,Val,Test Scores [288.3497009277344, 300.937255859375, 81.09365844726562]\n",
      "Epoch 404 Loss 23.7778 Current Train,Val,Test Scores [230.29891967773438, 243.0797119140625, 65.85124206542969]\n",
      "Epoch 405 Loss 21.9233 Current Train,Val,Test Scores [237.13804626464844, 250.5073699951172, 67.19498443603516]\n",
      "Epoch 406 Loss 24.5483 Current Train,Val,Test Scores [131.2377471923828, 144.3662109375, 38.648075103759766]\n",
      "Epoch 407 Loss 36.2372 Current Train,Val,Test Scores [975.2230834960938, 990.5294189453125, 360.5208435058594]\n",
      "Epoch 408 Loss 50.4940 Current Train,Val,Test Scores [897.7540283203125, 925.3515014648438, 377.04681396484375]\n",
      "Epoch 409 Loss 59.1087 Current Train,Val,Test Scores [1401.0947265625, 1417.112060546875, 465.3634338378906]\n",
      "Epoch 410 Loss 32.6022 Current Train,Val,Test Scores [1233.9393310546875, 1250.7340087890625, 397.0934143066406]\n",
      "Epoch 411 Loss 29.4515 Current Train,Val,Test Scores [879.8679809570312, 894.28369140625, 279.49298095703125]\n",
      "Epoch 412 Loss 44.1959 Current Train,Val,Test Scores [822.5750732421875, 838.6204223632812, 243.87271118164062]\n",
      "Epoch 413 Loss 37.8280 Current Train,Val,Test Scores [576.7525024414062, 591.7352294921875, 168.22158813476562]\n",
      "Epoch 414 Loss 32.3119 Current Train,Val,Test Scores [327.7574768066406, 342.58294677734375, 100.16417694091797]\n",
      "Epoch 415 Loss 32.8298 Current Train,Val,Test Scores [271.6412658691406, 286.1116943359375, 87.57444763183594]\n",
      "Epoch 416 Loss 30.8623 Current Train,Val,Test Scores [212.8293914794922, 193.8136749267578, 68.55783081054688]\n",
      "Epoch 417 Loss 36.6283 Current Train,Val,Test Scores [470.8431091308594, 452.7460632324219, 136.92007446289062]\n",
      "Epoch 418 Loss 34.3545 Current Train,Val,Test Scores [142.4538116455078, 157.49148559570312, 56.495296478271484]\n",
      "Epoch 419 Loss 29.2141 Current Train,Val,Test Scores [120.031494140625, 136.32537841796875, 52.72074508666992]\n",
      "Epoch 420 Loss 38.5595 Current Train,Val,Test Scores [115.95439147949219, 86.9248046875, 53.27085494995117]\n",
      "Epoch 421 Loss 31.2593 Current Train,Val,Test Scores [145.1943817138672, 160.7355194091797, 62.42214584350586]\n",
      "Epoch 422 Loss 34.3732 Current Train,Val,Test Scores [169.5621337890625, 182.2336883544922, 68.70085906982422]\n",
      "Epoch 423 Loss 36.8465 Current Train,Val,Test Scores [95.5367660522461, 110.57394409179688, 49.9025764465332]\n",
      "Epoch 424 Loss 25.7176 Current Train,Val,Test Scores [284.3810119628906, 256.6440124511719, 98.28936004638672]\n",
      "Epoch 425 Loss 37.2095 Current Train,Val,Test Scores [242.37229919433594, 219.72817993164062, 86.41373443603516]\n",
      "Epoch 426 Loss 27.9042 Current Train,Val,Test Scores [149.41250610351562, 162.0188751220703, 62.819862365722656]\n",
      "Epoch 427 Loss 33.8373 Current Train,Val,Test Scores [135.52479553222656, 148.3603057861328, 57.94235610961914]\n",
      "Epoch 428 Loss 30.8792 Current Train,Val,Test Scores [70.47295379638672, 82.55586242675781, 39.07634353637695]\n",
      "Epoch 429 Loss 27.3962 Current Train,Val,Test Scores [17.33216667175293, 28.876850128173828, 23.45130157470703]\n",
      "Epoch 430 Loss 33.1036 Current Train,Val,Test Scores [48.805572509765625, 60.67586135864258, 29.844745635986328]\n",
      "Epoch 431 Loss 30.6347 Current Train,Val,Test Scores [45.503807067871094, 57.47367858886719, 26.760765075683594]\n",
      "Epoch 432 Loss 34.4984 Current Train,Val,Test Scores [20.015886306762695, 33.28013610839844, 19.06260871887207]\n",
      "Epoch 433 Loss 24.9980 Current Train,Val,Test Scores [24.82487678527832, 38.04720687866211, 19.03485870361328]\n",
      "Epoch 434 Loss 28.1910 Current Train,Val,Test Scores [17.123125076293945, 28.427888870239258, 15.34000301361084]\n",
      "Epoch 435 Loss 25.2181 Current Train,Val,Test Scores [26.81412124633789, 39.04252243041992, 16.5118350982666]\n",
      "Epoch 436 Loss 27.2306 Current Train,Val,Test Scores [22.665842056274414, 34.25477600097656, 14.971364974975586]\n",
      "Epoch 437 Loss 24.9609 Current Train,Val,Test Scores [22.01761245727539, 33.56129455566406, 14.018806457519531]\n",
      "Epoch 438 Loss 23.9161 Current Train,Val,Test Scores [18.714136123657227, 31.144521713256836, 11.443683624267578]\n",
      "Epoch 439 Loss 24.0198 Current Train,Val,Test Scores [20.00972557067871, 31.973989486694336, 10.590362548828125]\n",
      "Epoch 440 Loss 23.3056 Current Train,Val,Test Scores [17.798778533935547, 30.112529754638672, 9.908924102783203]\n",
      "Epoch 441 Loss 24.3611 Current Train,Val,Test Scores [19.189016342163086, 31.58580780029297, 10.037687301635742]\n",
      "Epoch 442 Loss 22.7654 Current Train,Val,Test Scores [34.887664794921875, 47.4354362487793, 14.057172775268555]\n",
      "Epoch 443 Loss 23.5288 Current Train,Val,Test Scores [30.689910888671875, 45.65154266357422, 13.219674110412598]\n",
      "Epoch 444 Loss 22.1214 Current Train,Val,Test Scores [28.82558822631836, 41.11793518066406, 12.0217866897583]\n",
      "Epoch 445 Loss 23.2030 Current Train,Val,Test Scores [21.56641387939453, 33.84377670288086, 9.949997901916504]\n",
      "Epoch 446 Loss 23.2333 Current Train,Val,Test Scores [23.02664566040039, 34.71982192993164, 10.432157516479492]\n",
      "Epoch 447 Loss 23.9052 Current Train,Val,Test Scores [33.24520492553711, 45.27537536621094, 12.739750862121582]\n",
      "Epoch 448 Loss 23.1157 Current Train,Val,Test Scores [54.33884811401367, 66.66954803466797, 18.091083526611328]\n",
      "Epoch 449 Loss 23.2114 Current Train,Val,Test Scores [28.554948806762695, 41.15614318847656, 11.359838485717773]\n",
      "Epoch 450 Loss 22.0649 Current Train,Val,Test Scores [33.8124885559082, 46.00229263305664, 13.01937198638916]\n",
      "Epoch 451 Loss 21.1955 Current Train,Val,Test Scores [47.80801773071289, 59.88025665283203, 17.2110538482666]\n",
      "Epoch 452 Loss 22.9311 Current Train,Val,Test Scores [25.08478355407715, 37.77973175048828, 11.509475708007812]\n",
      "Epoch 453 Loss 21.6287 Current Train,Val,Test Scores [38.04471969604492, 50.150352478027344, 14.710358619689941]\n",
      "Epoch 454 Loss 22.7904 Current Train,Val,Test Scores [55.284305572509766, 67.27941131591797, 19.25837516784668]\n",
      "Epoch 455 Loss 22.6396 Current Train,Val,Test Scores [71.37174987792969, 83.20051574707031, 23.376667022705078]\n",
      "Epoch 456 Loss 21.5087 Current Train,Val,Test Scores [68.53331756591797, 80.4414291381836, 22.401288986206055]\n",
      "Epoch 457 Loss 21.4015 Current Train,Val,Test Scores [82.95271301269531, 94.06340026855469, 26.29792594909668]\n",
      "Epoch 458 Loss 20.3018 Current Train,Val,Test Scores [81.0778579711914, 92.5232162475586, 25.73246192932129]\n",
      "Epoch 459 Loss 21.0569 Current Train,Val,Test Scores [76.85150909423828, 88.38853454589844, 24.88662338256836]\n",
      "Epoch 460 Loss 21.2823 Current Train,Val,Test Scores [73.52408599853516, 85.25080108642578, 23.705516815185547]\n",
      "Epoch 461 Loss 21.0484 Current Train,Val,Test Scores [72.87370300292969, 84.73149108886719, 23.966060638427734]\n",
      "Epoch 462 Loss 21.4264 Current Train,Val,Test Scores [69.7007064819336, 81.5514907836914, 22.994707107543945]\n",
      "Epoch 463 Loss 20.0355 Current Train,Val,Test Scores [56.00377655029297, 68.33428955078125, 19.790376663208008]\n",
      "Epoch 464 Loss 20.0867 Current Train,Val,Test Scores [47.253013610839844, 59.5915641784668, 17.4508113861084]\n",
      "Epoch 465 Loss 20.2261 Current Train,Val,Test Scores [37.08729553222656, 49.90678405761719, 15.12890625]\n",
      "Epoch 466 Loss 20.1438 Current Train,Val,Test Scores [27.557632446289062, 40.18946838378906, 12.773159980773926]\n",
      "Epoch 467 Loss 20.8167 Current Train,Val,Test Scores [30.380813598632812, 43.4001579284668, 13.753591537475586]\n",
      "Epoch 468 Loss 19.7706 Current Train,Val,Test Scores [28.499645233154297, 41.270381927490234, 13.334283828735352]\n",
      "Epoch 469 Loss 19.5199 Current Train,Val,Test Scores [19.613862991333008, 31.0260066986084, 10.80297565460205]\n",
      "Epoch 470 Loss 19.6931 Current Train,Val,Test Scores [25.23046112060547, 37.662418365478516, 12.48300838470459]\n",
      "Epoch 471 Loss 19.4361 Current Train,Val,Test Scores [45.030460357666016, 57.90776062011719, 16.97207260131836]\n",
      "Epoch 472 Loss 19.6045 Current Train,Val,Test Scores [16.87833023071289, 29.13233184814453, 9.91319751739502]\n",
      "Epoch 473 Loss 19.5800 Current Train,Val,Test Scores [21.276611328125, 35.02141571044922, 11.621053695678711]\n",
      "Epoch 474 Loss 19.1258 Current Train,Val,Test Scores [12.199275016784668, 27.961877822875977, 9.364901542663574]\n",
      "Epoch 475 Loss 19.0342 Current Train,Val,Test Scores [12.511198997497559, 28.324949264526367, 9.478906631469727]\n",
      "Epoch 476 Loss 19.0244 Current Train,Val,Test Scores [17.702138900756836, 30.709951400756836, 10.259260177612305]\n",
      "Epoch 477 Loss 18.8417 Current Train,Val,Test Scores [18.787683486938477, 29.56172752380371, 10.156623840332031]\n",
      "Epoch 478 Loss 19.6143 Current Train,Val,Test Scores [33.14976501464844, 45.783199310302734, 14.232579231262207]\n",
      "Epoch 479 Loss 20.4207 Current Train,Val,Test Scores [19.262216567993164, 30.342723846435547, 10.25094985961914]\n",
      "Epoch 480 Loss 21.2935 Current Train,Val,Test Scores [24.86897087097168, 37.63519287109375, 12.153131484985352]\n",
      "Epoch 481 Loss 19.8614 Current Train,Val,Test Scores [18.484920501708984, 29.891992568969727, 10.113571166992188]\n",
      "Epoch 482 Loss 18.7730 Current Train,Val,Test Scores [17.545764923095703, 28.65972137451172, 9.8383207321167]\n",
      "Epoch 483 Loss 18.3837 Current Train,Val,Test Scores [18.979280471801758, 32.124725341796875, 10.422710418701172]\n",
      "Epoch 484 Loss 19.1163 Current Train,Val,Test Scores [33.7938232421875, 44.987548828125, 14.402978897094727]\n",
      "Epoch 485 Loss 20.0289 Current Train,Val,Test Scores [18.469619750976562, 31.382165908813477, 9.858198165893555]\n",
      "Epoch 486 Loss 18.5998 Current Train,Val,Test Scores [20.10060691833496, 30.948606491088867, 10.832310676574707]\n",
      "Epoch 487 Loss 17.9262 Current Train,Val,Test Scores [29.654176712036133, 40.540931701660156, 13.369919776916504]\n",
      "Epoch 488 Loss 17.9233 Current Train,Val,Test Scores [19.048267364501953, 30.31742286682129, 10.520751953125]\n",
      "Epoch 489 Loss 18.3318 Current Train,Val,Test Scores [32.42333984375, 44.535797119140625, 13.922616004943848]\n",
      "Epoch 490 Loss 18.4531 Current Train,Val,Test Scores [24.501056671142578, 36.48983383178711, 11.533415794372559]\n",
      "Epoch 491 Loss 18.4450 Current Train,Val,Test Scores [52.39606857299805, 66.51611328125, 19.100696563720703]\n",
      "Epoch 492 Loss 17.7697 Current Train,Val,Test Scores [50.698917388916016, 64.64293670654297, 18.64232635498047]\n",
      "Epoch 493 Loss 17.3564 Current Train,Val,Test Scores [50.79490280151367, 64.96517181396484, 19.000539779663086]\n",
      "Epoch 494 Loss 17.4124 Current Train,Val,Test Scores [62.51498031616211, 76.81792449951172, 21.641679763793945]\n",
      "Epoch 495 Loss 17.6262 Current Train,Val,Test Scores [57.02411651611328, 72.88542938232422, 20.825489044189453]\n",
      "Epoch 496 Loss 17.8065 Current Train,Val,Test Scores [57.68558120727539, 70.48182678222656, 20.343421936035156]\n",
      "Epoch 497 Loss 17.9629 Current Train,Val,Test Scores [54.769046783447266, 67.66667938232422, 19.51252555847168]\n",
      "Epoch 498 Loss 18.3350 Current Train,Val,Test Scores [72.75202941894531, 85.387451171875, 24.658226013183594]\n",
      "Epoch 499 Loss 18.5779 Current Train,Val,Test Scores [54.4229621887207, 66.39488983154297, 19.637407302856445]\n",
      "Best Train,Val,Test Scores [12.199275016784668, 27.961877822875977, 9.364901542663574]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creates a HeteroGNN model from the previously constructed hetero graph and trains it.\n",
    "\"\"\"\n",
    "\n",
    "best_model = None\n",
    "best_tvt_scores = (float(\"inf\"),float(\"inf\"),float(\"inf\"))\n",
    "\n",
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    # Train\n",
    "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "    # Test for the accuracy of the model\n",
    "    cur_tvt_scores, best_tvt_scores, best_model = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_tvt_scores)\n",
    "    print(f\"Epoch {epoch} Loss {loss:.4f} Current Train,Val,Test Scores {[score.item() for score in cur_tvt_scores]}\")\n",
    "\n",
    "print(\"Best Train,Val,Test Scores\", [score.item() for score in best_tvt_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Score: 28.15311050415039\n",
      "tensor([7.2314], device='cuda:0', grad_fn=<SelectBackward0>) tensor([3.], device='cuda:0')\n",
      "tensor([6.3395], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([88.9189], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([12.7852], device='cuda:0', grad_fn=<SelectBackward0>) tensor([94.], device='cuda:0')\n",
      "tensor([171.6684], device='cuda:0', grad_fn=<SelectBackward0>) tensor([48.], device='cuda:0')\n",
      "tensor([7.8729], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([125.8122], device='cuda:0', grad_fn=<SelectBackward0>) tensor([111.], device='cuda:0')\n",
      "tensor([12.8855], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([11.3711], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([1.9716], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([5.1449], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([11.5472], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([9.8430], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([28.4284], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([7.0100], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.2786], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([37.5543], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([9.6096], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([14.5608], device='cuda:0', grad_fn=<SelectBackward0>) tensor([58.], device='cuda:0')\n",
      "tensor([7.9357], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([3.0662], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.8051], device='cuda:0', grad_fn=<SelectBackward0>) tensor([13.], device='cuda:0')\n",
      "tensor([5.3115], device='cuda:0', grad_fn=<SelectBackward0>) tensor([8.], device='cuda:0')\n",
      "tensor([5.2009], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([3.0324], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.5112], device='cuda:0', grad_fn=<SelectBackward0>) tensor([22.], device='cuda:0')\n",
      "tensor([8.9414], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([9.7915], device='cuda:0', grad_fn=<SelectBackward0>) tensor([20.], device='cuda:0')\n",
      "tensor([142.9066], device='cuda:0', grad_fn=<SelectBackward0>) tensor([32.], device='cuda:0')\n",
      "tensor([14.4702], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n",
      "tensor([6.4822], device='cuda:0', grad_fn=<SelectBackward0>) tensor([19.], device='cuda:0')\n",
      "tensor([10.5913], device='cuda:0', grad_fn=<SelectBackward0>) tensor([17.], device='cuda:0')\n",
      "tensor([7.1220], device='cuda:0', grad_fn=<SelectBackward0>) tensor([144.], device='cuda:0')\n",
      "tensor([128.6206], device='cuda:0', grad_fn=<SelectBackward0>) tensor([79.], device='cuda:0')\n",
      "tensor([3.9129], device='cuda:0', grad_fn=<SelectBackward0>) tensor([6.], device='cuda:0')\n",
      "tensor([5.2605], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([10.5832], device='cuda:0', grad_fn=<SelectBackward0>) tensor([4.], device='cuda:0')\n",
      "tensor([6.2382], device='cuda:0', grad_fn=<SelectBackward0>) tensor([7.], device='cuda:0')\n",
      "tensor([10.8996], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([9.9282], device='cuda:0', grad_fn=<SelectBackward0>) tensor([51.], device='cuda:0')\n",
      "tensor([5.6287], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([6.0929], device='cuda:0', grad_fn=<SelectBackward0>) tensor([1.], device='cuda:0')\n",
      "tensor([82.8827], device='cuda:0', grad_fn=<SelectBackward0>) tensor([40.], device='cuda:0')\n",
      "tensor([6.4058], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([22.3836], device='cuda:0', grad_fn=<SelectBackward0>) tensor([10.], device='cuda:0')\n",
      "tensor([231.5019], device='cuda:0', grad_fn=<SelectBackward0>) tensor([607.], device='cuda:0')\n",
      "tensor([9.8166], device='cuda:0', grad_fn=<SelectBackward0>) tensor([2.], device='cuda:0')\n",
      "tensor([9.2721], device='cuda:0', grad_fn=<SelectBackward0>) tensor([5.], device='cuda:0')\n",
      "tensor([24.3366], device='cuda:0', grad_fn=<SelectBackward0>) tensor([9.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = HeteroGNN(hetero_graph, args, num_layers=2, aggr=\"mean\").to(args['device'])\n",
    "model.load_state_dict(torch.load('./best_model.pkl'))\n",
    "preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "# mask = preds['event'] > 0\n",
    "# preds['event'][preds['event'] > 0].shape\n",
    "\n",
    "# print(preds['event'][0], hetero_graph.node_target['event'][0]) \n",
    "\n",
    "\n",
    "#print(hetero_graph.node_feature['event'])\n",
    "\n",
    "# Assuming val_idx is the dictionary containing the validation indices for 'event' node type\n",
    "\n",
    "# TODO: Best validation score doesn't match the best validation score from the training loop\n",
    "# best_val = torch.sum(torch.abs(preds['event'][val_idx['event']] - hetero_graph.node_target['event'][val_idx['event']])) / val_idx['event'].shape[0]\n",
    "cur_tvt_scores, best_tvt_scores, best_model = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_tvt_scores)\n",
    "\n",
    "\n",
    "# Print the validation loss\n",
    "print(f\"Best Validation Score: {cur_tvt_scores[1].item()}\")\n",
    "\n",
    "# for i in range(3000):\n",
    "#     if hetero_graph.node_target['event'][i] != -1: # concepts have node target -1\n",
    "#         print(preds['event'][i], hetero_graph.node_target['event'][i])\n",
    "        \n",
    "    \n",
    "for i in range(1000):    \n",
    "    if hetero_graph.node_target['event'][test_idx['event']][i] != -1:\n",
    "        print(preds['event'][test_idx['event']][i], hetero_graph.node_target['event'][test_idx['event']][i])\n",
    "\n",
    "\n",
    "# for i in range(1000):\n",
    "#     # Extract the target value and check if it is not equal to -1\n",
    "#     target = hetero_graph.node_target['event'][test_idx['event']][i].item()\n",
    "#     if target != -1:\n",
    "#         # Extract the prediction value\n",
    "#         pred = preds['event'][test_idx['event']][i].item()\n",
    "#         print(f\"Prediction: {pred:.4f}, Target: {target:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions and node IDs saved to 'predictions_with_ids.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming you have already executed the model prediction code\n",
    "# preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
    "\n",
    "# Extract predictions and node IDs for the 'event' node type\n",
    "event_predictions = preds['event'][test_idx['event']].detach().cpu().numpy()\n",
    "event_node_ids = test_idx['event'].cpu().numpy()\n",
    "\n",
    "# Combine predictions and node IDs into a dictionary\n",
    "predictions_with_ids = {\n",
    "    'node_ids': event_node_ids,\n",
    "    'predictions': event_predictions\n",
    "}\n",
    "\n",
    "# Save the dictionary to a pickle file\n",
    "with open('predictions_with_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions_with_ids, f)\n",
    "\n",
    "print(\"Predictions and node IDs saved to 'predictions_with_ids.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Initialize wandb run\n",
    "    wandb.init(project=\"V2_MLG_PredEvents_GNN+LMM\", config={\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True),\n",
    "        \"hidden_size\": trial.suggest_int(\"hidden_size\", 16, 128),\n",
    "        \"attn_size\": 32,  # Fixed value\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 150, 300),\n",
    "        \"num_layers\": 2,  # Fixed value\n",
    "    })\n",
    "\n",
    "    # Use wandb config\n",
    "    config = wandb.config\n",
    "\n",
    "    # Initialize the model with the new hyperparameters\n",
    "    model = HeteroGNN(hetero_graph, {\n",
    "        'hidden_size': config.hidden_size,\n",
    "        'attn_size': config.attn_size,\n",
    "        'device': args['device']\n",
    "    }, num_layers=config.num_layers, aggr=\"mean\").to(args['device'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "    # Initialize best scores with infinity\n",
    "    best_tvt_scores = (float(\"inf\"), float(\"inf\"), float(\"inf\"))\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss = train(model, optimizer, hetero_graph, train_idx)\n",
    "        cur_tvt_scores, best_tvt_scores, _ = test(model, hetero_graph, [train_idx, val_idx, test_idx], None, best_tvt_scores)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_score\": cur_tvt_scores[1],\n",
    "            \"best_val_score\": best_tvt_scores[1],\n",
    "        })\n",
    "\n",
    "        # Update the best validation score\n",
    "        if cur_tvt_scores[1] < best_tvt_scores[1]:\n",
    "            best_tvt_scores = (cur_tvt_scores[0], cur_tvt_scores[1], cur_tvt_scores[2])\n",
    "\n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "    # The objective value is the best validation score\n",
    "    return best_tvt_scores[1]\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=500)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"Value: {trial.value}\")\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
